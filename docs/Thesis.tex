\documentclass[12pt,a4paper,oneside,english]{UPBThesis}
\setcounter{secnumdepth}{3}
\setcounter{tocdepth}{3}
\usepackage{graphicx}
\usepackage[
  bookmarksnumbered,
  bookmarks,
  bookmarksopen=true,
  pdftitle={Dissertation},
  linktocpage,
  dvips]{hyperref}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{inputenc}
\usepackage[T1]{fontenc}
\usepackage{algpseudocode}
\usepackage{algorithm}
\usepackage{tabularx}
\usepackage{array}
\usepackage{subfigure}

\floatstyle{boxed}
\restylefloat{figure}
\algrenewcommand\algorithmicrequire{\textbf{input}}
\algrenewcommand\algorithmicensure{\textbf{output}}

\newcommand{\hctimes}[2]{{#1}\!\times\!{#2}}
\newcommand{\hcrange}[2]{\overline{{#1}\colon\!\!{#2}}}
\newcommand{\hcsignalspace}{\mathbb{R}^d}
\newcommand{\hcweightspace}{\mathbb{R}^w}
\newcommand{\hcdictspace}{\mathbb{R}^{\hctimes{w}{d}}}

\singlespacing

\begin{document}

\author{Horia COMAN}

\title{Text Detection and Deep Neural Networks}

\facultatea{Faculty of Electronics, Telecommunications and Information Technology}
\tiplucrare{Dissertation}
\domeniu{Applied Electronics and Information Engineering}
\catedra{Applied Electronics}
\campus{Leu}
\program{Advanced Curriculum Master Program in Imaging, Bioinformatics and Complex Systems (ITEMS)}
\titlulobtinut{Master of Science}
\director{Prof. Dr. Ing. Erhardt BARTH\\Prof. Dr. Rer. Nat. Thomas MARTINETZ\\As. Dr. Ing. \c{S}erban OPRI\c{S}ESCU}

\submissionmonth{September}
\submissionyear{2012}

\beforepreface
\listoffigures
\listoftables
\abbreviations {
  CNN = Convolutional Neural Network\\
}
%\preface{}
\afterpreface

\unnumberedsection{Stream of thought writing}

[[[Talk about typographic conventions]]]

This work presents several contributions: a general architecture for feature extraction for computer vision classification or regression, with a neural-network like structure, but much more general, a novel way of computing nonlinearities in it which produces state-of-the-art results on two benchmark image recognition tasks, a description of a text detection system, and experimental results with it and the two image recongnition tasks.

Text detection: we are given an input image, which contains several areas of text. The goal is to detect these areas and then perform OCR on them. The task however is harder than OCR as usually natural images are provided and text is written in a variety of fonts, on a variety of backgrounds, and filmed from a variety of positions, such that the classical OCR of top-down view does not hold. Ideally this should be solved in video and online.

The problem has been studied by other authors (cite them here) and in the context of deep learning (cite them here).

We assume throught the usage of the lating alphabet and left-to-right, top-to-bottom writing conventions.

This work shows how I built a system for text detection, using, as the title suggests, Deep Neural Networks.

First we'll consider the problem on a single image. The problem can be broken down into several steps: first, find the regions of the image where text is, then for each image, corresponding to a word, segment it into characters. Then, recognize each character. Finally, build a word from each image, considering the letters as well as spellcheck-type data which is a must.

The first subproblem is arguably the hardest. Suppose we have a general procedure which detects for a small image of size $\hctimes{p}{p}$ if that is a letter or a nonletter. Given such a procedure we run it over all patches found by centering each pixel. A new ``binary'' image is obtained which contains $1$ if the pixel $i,j$ is the center of a letter patch and $0$ otherwise. Several such images are built for varying values of $p$. This image gives a general indication of where the text is. Still, segmentation must be done, and, possibly oriented text segments must be generated from each image. In cases where a rectangle in one image overlaps with a rectangle in another, a method for specifying which has priority should be employed. This is a mostly hand-crafting engineering problem, and as such is solved in an heuristic way.

After segemntation, a series of images are produced from the original image, which contain the words. They still must be processed and understood.

The next step is thus the breaking of words into letters. The images we obtain will generally have a rectangular form, more wide than tall. We consider that a single row of letters is present, therefore segmentation only has to worry about the horizontal axis. If the image is of size $\hctimes{m}{n}$ then we have at most $n$ letters in it, in a very degenerate case, and at least one. This puts an upper bound on what we're about to do. We must also have a classifier here which knows how to separate into letters and the several punctuation marks common in words. This is a tougher classifier though and a probabilistic output would be nice for a Dynamic Programming approach to this.

Much of the actual work for this project has gone into building the classifier for letter/nonletter and then specific letters.

The classifier is based on a Convolutional Neural Network (CNN) (cite CNN here). However, it suffers some generalizations. Finally, we obtain a general coding system, neural network inspired, but not necesarily a classical network.

For the moment assume we are given the image $I$ of size $\hctimes{m}{n}$. This can be a patch from the original image we wish to classify as letter / nonletter or a segment of an extracted image for which we wish to find the actual letter. In any case, the task at hand is image recognition. It is a complex task. The naive approach of simply training a classifier on the raw image data will usually yield poor performance, except in the most simple of cases. Image classification systems therefore consist of two parts: a feature extraction phase and a proper classification phase. The goal of the first is to transform in an input image so as to be more easily handled by the latter phase. As the name suggest, it does so by ``extracting'' certain properties of an image, generically called ``features''. For example, a feature can be the amount of blue in the top of an image, or the variance in a small patch from the image, or the response to a oriented bandpass filter around a certain position etc. Extraction might mean a simple check for the presence or absence of the property, or might be a gradual percentace of presence of the feature in the image, or it might be computed in a whole other way. All the features amployed by a feature extraction layer form the ``feature set''. In general, the extraction process might not consider all features as independent, that is, a high presence of a feature in the set migth trigger a lower resulting presence of another feature. The input of the feature extraction phase is the image, $I$, and the output is a description of the feature set, usually called the ``feature vector'', and denoted by $F$, which contains one element for each feature in the feature set, and which thus codes the extraction result. For the latter stage, only $F$ is used.

Feature engineering, that is, the human-guided process of feature construction, is a big part of most machine learning applications. Usually, for best performance, deep knowledge of the problem domain must be coupled with an understanding of the Statistics and Machine Learning.

In the last decade, however, automatic feature construction, known as unsupervised feature learning (cite people here), has received much attention, producing systems with state of the art performance, and, in the field of Neural Networks, allowing the construction of so called Deep Neural Networks, which were thought impossible to build by standard methods (cite google and microsoft recent results with such networks). The whole field of Deep Learning deals with automatic feature extraction in one way or another.

\chapter{Overview of Feature Extraction}

In our framework, a feature is a small image patch - a filter - of some sort. The full feature set consists of $w$ normalized square images of size $d = \hctimes{p}{p}$ with $p < \min(m,n)$. We denote it by $\textbf{C} = \left[ \textbf{C}^1 \left|\right. \textbf{C}^2 \left|\right. \dots \left|\right. \textbf{C}^w \right] \in \mathbb{R}^{\hctimes{d}{w}}$. Normalization implies $\|\mathbf{C}^i\| = 1$ for all $i \in \hcrange{1}{w}$. For now we consider the feature set as given. Speaking broadly, though, feature sets can be generated randomly \cite{random-weights-feature-learning}, or taken to be a well-known set, such as DCT bases or Gabor Wavelet bases \cite{simple-method-sparse-coding}, or, even learned from a sample of patches extracted from the training set of the classification system \cite{emergence-sparse-coding,sparse-coding-strategy-V1,tiny-images}. More details about feature set construction will be given in the appropriate section.

A feature extractor is composed of a number of recoding modules, linked in a serial manner. The input image is propagated through these modules and transformed by them. The first such module starts with the image $\textbf{I}$ and produces a set of images $\{\textbf{I}_i^1\}_{i=1}^{l_1}$. The next module accepts as input $\{\textbf{I}_i^1\}_{i=1}^{l_1}$ and produces $\{\textbf{I}_i^2\}_{i=1}^{l_2}$. The last module outputs $\{\textbf{I}_i^k\}_{i=1}^{l_k}$, which is linearized to produce the feature vector $\mathbf{F}$. The stacking of these module produces a deep forward path for the input image. As we will see, this transformation can be modelled as a neural network. Hence the name Deep Neural Network. 

A module consists of several standard steps, called \emph{coding}, \emph{nonlinear}, \emph{polarity split} and \emph{reduce}. The coding step depends on an module associated feature set $\textbf{C}^m$. It produces a set of $w_m$ images of the same size as the input ones. In the simplest case, each image is obtained by convoluting the original image with the feature ${\textbf{C}^m}^i$. This is the setup of Convolutional Neural Networks (cite CNNs here). More general, the set of $w_m$ pixels at positions $(\alpha,\beta)$ from all the images are not independent. The next section will describe coding methods, and how all the pixels at the same position can be obtained at once. In any case, using convolution, the local computation is evident. Pixel $I^m_i(\alpha,\beta)$ depends on a patch of pixels, centered at $(\alpha,\beta)$ from the input images. At the limit, all stage output images depend on all the input images (they consider the mean of the input images, for example). However, for better performance (cite stuff from gradient based learning here), each output image depends only on a small subset of the input images. After coding, comes the nonlinear phase. Here, inputs can passed unaffected (the degenerate case), or they can be passed through a sigmoid like nonlinearity (logistic or hyperbolic tangent). Besides the general architecture, our own contribution is the introduction of a global nonlinearity. The experiments section will reveal the advantages of this method. Whereas the identity and sigmoid maps acted on each pixel independently, this method considers the whole set of $w_k$ pixels corresponding to input $(\alpha,\beta)$ and transforms the largest of these into $1$, and the rest in an exponentially decreasing manner from $1$ in a fashion dependent on the rank of that value in the decreasingly sorted list of pixel values. The signs of the pixels is kept, only the magnitude is important. The polarity split step is optional. If it is enabled, it effectivly doubles the number of output images (therefore $l_m = 2w_m$ in this case). One set of images keeps only the pixels with greater than zero values and makes the others zero. The other set keeps only the pixels with lesser than zero values and makes the others zero. The sign can be kept or not, depending on the situation. In any case, this computation is stricly feedforward and pixel-local. This step has been shown to improve classifier performance (cite Ng here), mostly because of a cheap increase in output dimensionality. Finally, the reduce step performs a subsampling of the output of the polarity split layer. This also works on each image independently. Each image is split into non-overlapping patches, as best as possible (several rows and columns at the end might be removed this way). From each patch a single value is computed. Several methods exist for this as well: a single fixed value can be used each time (a subsampling), the maximum absolute value can be used, the maximum with sign kept can be used, the sum of absolute values can be used and the sum of squares can be used. In any case, this is a patch-local feedforward computation as well.

Several options for building feature extractors thus become apparent. The best for a certain dataset should be determined by cross-validation as well as computational concerns - for example the global order nonlinearity produces very good results, but it requires many sorts, whereas the sigmoid nonlinearity produces slightly worse results, but is simpler. In any case, the experiments section presents guidelines for all these methods.

The feature extractor is therefore a mostly feed-forward and patch-local or pixel-local affair. The only parameters it requires are the feature sets for each recoder module. These are determined in an unsupervised fashion externally from the recoder and just plugged in at run time. This is unsupervised pre-training. The whole stack is built in a greedy fashion. The first set of features is obtained from the original images. Then these are coded with the first recoder. From the new set, features for the second layer are learned. The process continues as you would expect. 

The extractor can be modelled as a more general neural network, with specialized nodes and more complex activation functions (functions which depend on the results of other units). For extractors with a classical feedforward structure or at least one for which gradients can be computed for all parameters, an extra step can be done after unsupervised pre-training. The network can be ``fine-tuned''. That is, back-propagation is applied to the network and parameters are once-more adapted to the classification task at hand as with shallow networks. The initial weights are given by the values of the learned feature sets. Extra parameters can be added to the network in this case. Weights for the reduce layer and bias terms for the code and reduce layers. A very sharp logistic or hyperbolic tangent can be used to implement a polarity split layer in this case as well.

Fine-tuning wasn't the approach I took however. Just the simple extractor with one layer proved sufficient for the needs of the studied applications. 

\chapter{Coding and Different Coding Methods}

This section describes in more detail how to do the coding. For simplicity, we will assume we work with $d$-dimensional signals. Thus, the $\hctimes{p}{p}$ patches previously discussed must be linearized such that $d = p^2$. Assume also that we are given a set of features $\textbf{C}$, like in the previous section, and a signal $\textbf{x} \in \hcsignalspace$. The goal of coding is to find the signal $\textbf{a}$, which is the representation or ``code'' of $\textbf{x}$ in terms of $\textbf{C}$, such that the reconstruction $\mathcal{R}[\textbf{C}](\textbf{a})$ is optimal with regard to a reconstruction error.

More often than not, the generative model is a linear one. That is, the code $\textbf{a}$ is a vector in $\hcsignalspace$ and the reconstruction is:

\begin{equation*}
\mathcal{R}[\textbf{C}](\textbf{a}) = \sum_{i=1}^{w}{a_i \textbf{C}^i} = \textbf{C}\textbf{a}
\end{equation*}

If we consider the reconstruction error to be the square euclidean norm, then the optimal code is the one which satisfies:

\begin{equation}
\textbf{a}^{\star} = \arg\min_{\textbf{a}} \| \textbf{x} - \textbf{C}\textbf{a} \|_2^2
\end{equation}

For the case of $w \leq d$, ($\textbf{C}$ is a tall matrix), we have the classical least-squares problem. If the features are linearly independent, then an analytic solution is provided by the Normal equation, in the form of:

\begin{equation}
\textbf{a}^{\star} = (\textbf{C}^T\textbf{C})^{-1}\textbf{C}^T\textbf{x}
\end{equation}

Alternatively, for computational considerations, a gradient descent can be employed to find the optimal solution. The problem is convex, therefore a single global minimum exists. The reconstructed signal, $\mathcal{R}[\textbf{C}](a) = \textbf{C}\textbf{a}^{\star} = \textbf{C}(\textbf{C}^T\textbf{C})^{-1}\textbf{C}^T\textbf{x}$, is the orthogonal projection of $\textbf{x}$ onto the subspace spanned by the columns of $\textbf{C}$ (our features) and is the closest point in $\overline{\text{span}(\textbf{C})}$ to $\textbf{x}$, wrt the Euclidean norm. Notice that when employing ``classical'' bases such as DCT, with $w = d$, the feature matrix $\textbf{C}$ is orthogonal, therefore the reconstructed signal is $\mathcal{R}[\textbf{C}](\textbf{a}) = \textbf{C}^T\textbf{a}$.

For $w \ge d$, this formulation does not work however. In this case, $\textbf{C}$ is a wide matrix. Also, it does not admit a single inverse, and the interpretation of the reconstruction as the projection on the column space of $\textbf{C}$, does not hold, as, most likely, the column space is equal to the signal space $\hcsignalspace$.

A different approach is needed here. Technically we are dealing with an inverse problem. Extra constraints must be put on the reconstruction error measure, in order to guarantee a good solution. One important class of constraints is that of sparseness. This means that we wish the code $\textbf{a}$ to contain a small number of different from zero components. In figure [[[insert figure here]]] we can see the difference between a sparse and a normal vector, as well as the distribution of values. The reason for desiring sparseness has to do with mathematical niceness (??? - cite Donovo and LASSO here here) but also with biological plausability (cite Olshausen here). The two directions produce different forms of algorithms for sparsity induction.

All methods ultimately come down to solving an augumented least squares problem, or approximating the solution through different means. The extra regularization term added induces sparsity by controlling the values allowed for $\mathbf{a}$. In probabilitic terms, we place a prior on the code components $\mathbf{a_i}$, considered independently, of the form presented in the figure (???).

The first approach ammends the cost function as:

\begin{equation}
\textbf{a}^{\star} = \arg\min_{\textbf{a}} \| \textbf{x} - \textbf{C}\textbf{a}\|_2^2 + \lambda \|a\|_0
\label{eq:SparseProblem}
\end{equation}

The $\lambda$ term controls the trade-off between reconstruction power and sparseness. However, solving this problem is NP-Complete (cite who showed this) and gennerally cannot be attempted except combinatorially. However, it has been shown (cite Donovo again), that a different formulation produces the same result under mild conditions and is easier to solve. This is the LASSO formulation of regression and it has the form of :

\begin{equation}
\textbf{a}^{\star} = \arg\min_{\textbf{a}} \| \textbf{x} - \textbf{C}\textbf{a}\|_2^2 + \lambda \|a\|_1
\end{equation}

That is, it replaces the $\mathcal{L}_0$ norm with the $\mathcal{L}_1$ norm.

[[Discuss algorithms for solving the LASSO here]]

Many other methods for sparse coding have been proposed \cite{undetermined-minimal-L1,sparse-coding-strategy-V1}. We will focus on a group of iterative methods for computing $\textbf{a}$, known as pursuits, which originate in the signal processing community. All assume $\textbf{C}$ and $\textbf{x}$ are given and run for a number of $k \leq d$ iterations. The general problem they try to solve is:

\begin{equation}
\arg\min_\textbf{a} \|\textbf{x} - \textbf{C}\textbf{a}\|_2^2 \text{~~subject to~~} \|\textbf{a}\|_0 \leq k
\end{equation}

This an NP-complete problem (cite the work that says this). It can be seen as the Lagrange dual of the original sparse formulation of Equation \ref{eq:SparseProblem}. The problem is thus of selecting the subset of $k$ features which have the minimum reconstruction error. This is a classical combinatorial problem (each state has a certain cost, and we must select the one of smallest energy, constrained by some structure of the cost function).

The pursuit methods are greedy approximations for this problem. They do not solve the exact problem, but provide a good enough solution. The code has a common form for all methods. Let the initial residual $\mathcal{R}^0\textbf{x} = \textbf{x}$. At iteration $t$, let $\textbf{C}^\omega$ be the most similar feature in $\textbf{C}$, relative to $\mathcal{R}^t\textbf{x}$. The updated code and residual, $\textbf{a}^{t+1}$ and $\mathcal{R}^{t+1}\textbf{x}$, are produced by decomposing $\mathcal{R}^t\textbf{x}$ in terms of $\textbf{C}^\omega$. After $k$ iterations, $\textbf{a}^k$ is returned as the code associated to $\textbf{x}$, and $\mathcal{R}^k\textbf{x}$ is returned as a measure of the ability of the algorithm to reconstruct the signal in terms of $\textbf{C}$. The difference between the several methods consists in how they find $\textbf{C}^\omega$ and how they update $\textbf{a}^{t+1}$. The general procedure is illustrated in \textbf{Algorithm \ref{algo:GeneralPursuitMethod}}. At the end of this algorithm we obtain $\textbf{x} = \textbf{C}\textbf{a}^k + \mathcal{R}^k\textbf{x}$. Also, the norm of the final residual $\mathcal{R}^k\textbf{x}$ tends to $0$ as $k \rightarrow +\infty$ for sensible choices of $\text{\textbf{sim}}$ and $\text{\textbf{next}}$ functions. In the limit, the equality becomes $\textbf{x} = \textbf{C}\textbf{a}^{+\infty}$.

\begin{algorithm}
\caption{The General Pursuit Method}
\label{algo:GeneralPursuitMethod}
\begin{algorithmic}
\Require $\textbf{C},\textbf{x},k$
\Ensure $\textbf{a}^k,\mathcal{R}^k\textbf{x}$
\State $\Lambda^0 \gets \phi$
\State $\textbf{a}^0 \gets \textbf{0}$
\State $\mathcal{R}^0\textbf{x} \gets \textbf{x}$
\For {$t = \hcrange{1}{k}$}
\State $\omega \gets \arg \max_{i \in \textbf{dom}} \text{\textbf{sim}}(\mathcal{R}^{t-1}\textbf{x},\textbf{C},\Lambda^{t-1},i)$
\State $\Lambda^t \gets \Lambda^{t-1} \cup \omega$
\State $\textbf{a}^t \gets \text{\textbf{next}}(\textbf{a}^{t-1},\mathcal{R}^{t-1}\textbf{x},\textbf{C},\Lambda^t,\omega)$
\State $\mathcal{R}^t\textbf{x} \gets \mathcal{R}^{t-1}\textbf{x} - a_\omega^t\textbf{C}^\omega$
\EndFor
\end{algorithmic}
\end{algorithm}

\renewcommand{\arraystretch}{1.5}
\begin{table}
  \caption{The different parametrization for pursuit methods}
  \label{table:PursuitParametrization}
  \begin{tabularx}{\textwidth}{|l|>{\centering}X|>{\centering}X|c|}
       \hline
        Method & $\text{\textbf{sim}}$ function & $\text{\textbf{next}}$ function & $\text{\textbf{dom}}$ domain\\ \hline \hline
        \textbf{MP} & $\left| \langle \mathcal{R}^{t-1}\textbf{x} , \textbf{C}^i \rangle \right|$ & $\textbf{a}^{t-1} + \langle \mathcal{R}^{t-1}\textbf{x} , \textbf{C}^\omega \rangle \delta_\omega$ & $\hcrange{1}{w}$ \\  \hline
        \textbf{OMP} & $\left| \langle \mathcal{R}^{t-1}\textbf{x} , \textbf{C}^i \rangle \right|$ & $\arg\min_{\textbf{a}} {\| \textbf{x} - \textbf{C}^{\Lambda^t}\textbf{a} \|_2^2}$ & $\hcrange{1}{w} \setminus \Lambda^{t-1}$ \\ \hline
        \textbf{OOMP} & $\min_{\textbf{a}} {\| \textbf{x} - \textbf{C}^{\Lambda^{t-1} \cup i}\textbf{a} \|_2^2}$ & $\arg\min_{\textbf{a}} {\| \textbf{x} - \textbf{C}^{\Lambda^t}\textbf{a} \|_2^2}$ & $\hcrange{1}{w} \setminus \Lambda^{t-1}$ \\
       \hline
    \end{tabularx}
\end{table}
\renewcommand{\arraystretch}{1.0}

The simplest pursuit method, introduced in \cite{matchingpursuit1}, is Matching Pursuit (\textbf{MP}). \textbf{Table \ref{table:PursuitParametrization}} shows what form the $\text{\textbf{sim}}$ and $\text{\textbf{next}}$ functions take in this case. An important property of this algorithm is that for every $t$, $\|\mathcal{R}^t\textbf{x}\|_2^2 \geq \|\mathcal{R}^{t+1}\textbf{x}\|_2^2$ and, furthermore, with a decay that is exponential. The two major drawbacks of this method are that the approximation at time $t$, $\textbf{C}\textbf{a}^t$, is not optimal with respect to the selection of features $\Lambda^t$; and that for the residual norm to actually reach small enough values, a $k > w$ could be necessary. However, these drawbacks are not critical for classification purposes, and, because of its simplicity and speed, we use it in our experiments. $\mathcal{R}^t$ is orthogonal with respect to $\textbf{C}^\omega$, but not necesarily every feature in $\textbf{C}^{\Lambda^t \setminus \omega}$.

The specialization of the General Pursuit Method to Matching Pursuit can be seen in \textbf{Algorithm \ref{algo:MatchingPursuitMethodV1}}.

\begin{algorithm}
\caption{The Matching Pursuit Method (Version 1)}
\label{algo:MatchingPursuitMethodV1}
\begin{algorithmic}
\Require $\textbf{C},\textbf{x},k$
\Ensure $\textbf{a}^k,\mathcal{R}^k\textbf{x}$
\State $\Lambda^0 \gets \phi$
\State $\textbf{a}^0 \gets \textbf{0}$
\State $\mathcal{R}^0\textbf{x} \gets \textbf{x}$
\For {$t = \hcrange{1}{k}$}
\State $\omega \gets \arg \max_{i \in \hcrange{1}{w}} \left| \langle \mathcal{R}^{t-1}\textbf{x}, \textbf{C}^i \rangle \right|$
\State $\Lambda^t \gets \Lambda^{t-1} \cup \omega$
\State $\textbf{a}_\omega^t \gets \textbf{a}_\omega^{t-1} + \langle \mathcal{R}^{t-1}\textbf{x} , \textbf{C}^\omega \rangle$
\State $\mathcal{R}^t\textbf{x} \gets \mathcal{R}^{t-1}\textbf{x} - a_\omega^t\textbf{C}^\omega$
\EndFor
\end{algorithmic}
\end{algorithm}

The runtime of this variant is $T_1^{MP} = \Theta(kwd)$. This is somewhat suboptimal, as, in most cases, it will be quadratic in $d$ ($d$ and $w$ tend to have the same order magnitude), or even cubic in $d$ for cases where $k$ has the same order of magnitude with $d$. The source of this complexity is the small optimization problem we have to solve in the inner loop. This just requires a walk through all the features and computing of a ``similarity'' of cost $\Theta(d)$. The per iteration time is therefore $\Theta(dw)$. Notice, however the following decomposition of $\mathcal{R}^t\textbf{x}$:

\begin{align*}
\mathcal{R}^t\textbf{x} & \gets \mathcal{R}^{t-1}\textbf{x} - a_\omega^t\textbf{C}^\omega \\
\mathcal{R}^t\textbf{x} & \gets \mathcal{R}^{t-1}\textbf{x} - \langle \mathcal{R}^{t-1}\textbf{x}, \mathcal{C}^\omega \rangle \textbf{C}^\omega
\end{align*}

Now, if we want to compute the inner product between $\mathcal{R}^t\textbf{x}$ and a $\textbf{C}^i$, we have:

\begin{align*}
\langle \mathcal{R}^t\textbf{x}, \textbf{C}^i \rangle & \gets \langle \mathcal{R}^{t-1}\textbf{x} - \langle \mathcal{R}^{t-1}\textbf{x}, \mathcal{C}^\omega \rangle \textbf{C}^\omega , \textbf{C}^i \rangle \\
& \gets \langle \mathcal{R}^{t-1}, \textbf{C}^i \rangle - \langle \mathcal{R}^{t-1}\textbf{x}, \textbf{C}^\omega \rangle \langle \textbf{C}^\omega, \textbf{C}^i \rangle
\end{align*}

Thus, we have expressed the computation of the inner product between the current residual and a feature in terms of the inner product between the previous residual and the same feature and an ``adjustement term'', which is a product of the maximum inner product of the previous iteration and the inner product of the best previous feature and the current feature under consideration. The new algorithm can be seen in \textbf{Algorithm \ref{algo:MatchingPursuitMethodV2}}.

\begin{algorithm}
\caption{The Matching Pursuit Method (Version 2)}
\label{algo:MatchingPursuitMethodV2}
\begin{algorithmic}
\Require $\textbf{C},\textbf{x},k$
\Ensure $\textbf{a}^k,\mathcal{R}^k\textbf{x}$
\State $\textbf{a}^0 \gets \textbf{0}$
\State $\mathcal{S}^f \gets [ \langle \textbf{C}^i, \textbf{C}^j \rangle ~ \mbox{\textbf{for} $(i,j) \in \hctimes{w}{w}$}]$
\State $\mathcal{S}^s \gets [ \langle \textbf{x}, \textbf{C}^i \rangle ~ \mbox{\textbf{for} $i \in \hcrange{1}{w}$} ]$
\For {$t = \hcrange{1}{k}$}
\State $\omega \gets \arg \max_{i \in \hcrange{1}{w}} \left| \mathcal{S}_i^s \right|$
\State $\textbf{a}_\omega^t \gets \textbf{a}_\omega^{t-1} + \mathcal{S}_\omega^s$
\State $\mathcal{S}^s \gets [ \mathcal{S}_i^s - \mathcal{S}_\omega^s \mathcal{S}_{\omega i}^f ~ \mbox{\textbf{for} $i \in \hcrange{1}{w}$} ]$
\EndFor
\end{algorithmic}
\end{algorithm}

As it is, the runtime of this algorithm is $T_2^{MP} = \Theta(wd + kw)$, which does not seem like much of an improvement. However, in the broad picture, many patches are coded with the same feature set. Therefore, we need to compute only once the dictionary, and then use it for all patches. The amortized cost of this computation tends to $0$ as the number of patches increases. Therefore, the actual time complexity is $T_2^{MP} = \Theta(kw)$, which is an order of magnitude lower than the original time $T_1^{MP}$.

An improvement to \textbf{MP} is Orthogonal Matching Pursuit (\textbf{OMP}) \cite{matchingpursuit2,orthopursuit,pursuitdifferences}, which addresses the two issues discussed above. Again, \textbf{Table \ref{table:PursuitParametrization}} shows the forms the $\text{\textbf{sim}}$ and $\text{\textbf{next}}$ functions take. Also, notice that at each iteration, only the features not considered before are processed. All the properties of \textbf{MP} hold here as well. At iteration $t$, the approximation computed is the closest point in $\overline{\text{span}(\textbf{C}^{\Lambda^t})}$ to $\textbf{x}$, according to the Euclidean norm. The algorithm is presented in \textbf{Algorithm \ref{algo:OrthogonalMatchingPursuitMethodV1}}.

\begin{algorithm}
\caption{Orthogonal Matching Pursuit (Version 1)}
\label{algo:OrthogonalMatchingPursuitMethodV1}
\begin{algorithmic}
\Require $\textbf{C},\textbf{x},k$
\Ensure $\textbf{a}^k,\mathcal{R}^k\textbf{x}$
\State $\Lambda^0 \gets \phi$
\State $\textbf{a}^0 \gets \textbf{0}$
\State $\mathcal{R}^0\textbf{x} \gets \textbf{x}$
\For {$t = \hcrange{1}{k}$}
\State $\omega \gets \arg \max_{i \in \hcrange{1}{w} \setminus \Lambda^{t-1}} \left| \langle \mathcal{R}^{t-1}\textbf{x} , \textbf{C}^i \rangle \right|$
\State $\Lambda^t \gets \Lambda^{t-1} \cup \omega$
\State $\textbf{a}^t \gets \arg\min_{\textbf{a}} {\| \textbf{x} - \textbf{C}^{\Lambda^t}\textbf{a} \|_2^2}$
\State $\mathcal{R}^t\textbf{x} \gets \mathcal{R}^{t-1}\textbf{x} - a_\omega^t\textbf{C}^\omega$
\EndFor
\end{algorithmic}
\end{algorithm}

Therefore, at iteration $t$, we chose the feature $\textbf{C}^\omega$ of maximum ``similarity'' according to the inner product measure. We add this to the current feature set $\textbf{C}^{\Lambda^t}$. In order to compute $a^t$ we find the point $a$ in the column space of $\textbf{C}^{\Lambda^t}$ closest to $x$. This is, of course, a least squares problem. It seems counterintuitive to solve a problem we originally posed to move away from a simple least squares formulation, in terms of least squares. However, first, this problem is solvable in any case, as we have, by necessity $k \leq d$ (for $k = d$ the residual becomes $\textbf{0}$), therefore, except for some mild conditions, we always have a solution, given by the Normal Equation, usually (or gradient descent on the error surface, but that's a little too involved for our purposes here). Second, we use this as a guide in selecting a $k$-subset of the feature set. The previous formulation was needlessly inefficient from this point of view. The $a^k$ found was not optimal with respect to the final group of selected features.

The major problem of the \textbf{OMP} algorithm, as it is, is the extra complexity introduced by the need to solve the least squares problem. We have $T_1^{OMP} = \Theta(k(wd + T^{LS}))$. Assuming the usage of the Normal Method and computation of the inverse via a LU decomposition, $T^{LS}$ becomes: $T^{LS} = O(t^2d + t^3)$, therefore $T_1^{OMP} = \Theta(kwd + k^4)$. This is somewhat prohibitive. However, an approach suggested by (cite guy here) combines a QR factorization with the ideas from the second version of the Matching Pursuit, from before. Suppose, at time step $t$ we compute the QR decomposition of $C^{\Lambda^t}$. Let $\Delta_t = C^{\Lambda^t}$. We have:

\begin{equation}
\Delta_t = \textbf{Q}_t\textbf{R}_t
\end{equation}

Where $\textbf{Q}_t$ is an orthogonal matrix and $\textbf{R}_t$ is an upper triangular matrix. We can then find $\textbf{a}^t$ as:

\begin{align*}
\textbf{a}^t & \gets (\Delta_t^T\Delta_t)^{-1}\Delta_t^T\textbf{x} \\
             & \gets ((\textbf{Q}_t\textbf{R}_t)^T(\textbf{Q}_t\textbf{R}_t))^{-1}(\textbf{Q}_t\textbf{R}_t)^T\textbf{x} \\
             & \gets (\textbf{R}_t^T\textbf{Q}_t^T\textbf{Q}_t\textbf{R}_t)^{-1}\textbf{R}_t^T\textbf{Q}_t^T\textbf{x} \\
             & \gets (\textbf{R}_t^T\textbf{R}_t)^{-1}\textbf{R}_t^T\textbf{Q}_t^T\textbf{x} \\
             & \gets \textbf{R}_t^{-1}{\textbf{R}_t^T}^{(-1)}\textbf{R}_t^T\textbf{Q}_t^T\textbf{x} \\
             & \gets \textbf{R}_t^{-1}\textbf{Q}_t^T\textbf{x}
\end{align*}

Computing $\textbf{a}^t$ is then a $\Theta(t^2)$ operation. This is relatively expensive, but an order of magnitude lower than the original implementation cost. However, the bulk of the computing time is for the decomposition. The QR decomposition can be computed using the Gram-Schmidt process at a time complexity of $T^{QR} = \Theta(t^3)$. This process builts $\textbf{Q}_t$ one column at a time and $\textbf{R}_t$ one column at a time. Furthermore, the building of column $i \in \hcrange{1}{t}$ depends only on columns $\hcrange{1}{i}$ from $\Delta_t$ and columns $\hcrange{1}{i-1}$ from $Q_t$. Also, the matrices $\Delta_t$ and $\Delta_{t+1}$ are identical, except $\Delta_{t+1}$ has one extra column. Using our notation, this column, corresponding to position $\omega^{t+1} \in \hcrange{1}{w}$, is inserted ``between'' two other columns from $\Delta_t$. However, for our purposes, the order of the columns is irrelevant. Therefore we can consider $\Delta_{t+1} = \left[\Delta_t \left|\right. \textbf{C}^{\omega^{t+1}} \right]$. The two matrices have the same first $t$ columns, therefore, the first $t$ columns of the $Q$ matrices of the QR decomposition will be identical. The same goes for the first $t$ rows of the $R$ matrices. This naturally leads us to a version of the algorithm which accumulates the $\textbf{Q}_t$ and $\textbf{R}_t$ matrices, building them one column at at time.

Another optimization follows. Consider how the residual is updated. An alternative form is $\mathcal{R}^t\textbf{x} \gets \textbf{x} - \textbf{C}^{\Lambda^t}\textbf{a}^t$. Replacing both $\textbf{C}^{\Lambda^t}$ and $\textbf{a}^t$ with the forms we computed at the current iteraton, we obtain:

\begin{align*}
\mathcal{R}^t\textbf{x} & \gets \textbf{x} - \textbf{C}^{\Lambda^t}\textbf{a}^t \\
& \gets \textbf{x} - (\textbf{Q}_t\textbf{R}_t)(\textbf{R}_t^{-1}\textbf{Q}_t^T)\textbf{x} \\
& \gets \textbf{x} - \textbf{Q}_t\textbf{Q}_t^T\textbf{x}
\end{align*}

The product $\textbf{Q}_t\textbf{Q}_t^T$ is not the identity matrix, except for $t = d$, because $\textbf{Q}_t$ is not yet an square matrix (a necessary condition for orthogonality). It is only partway there. Computing $\textbf{Q}_t^T\textbf{x}$ turns out to be easy as well. We have:

\begin{align*}
\textbf{Q}_t^T\textbf{x} & = [ \langle \textbf{Q}_t^1, \textbf{x} \rangle \cdots \langle \textbf{Q}_t^t, \textbf{x} \rangle ] \\
\textbf{Q}_{t+1}^T\textbf{x} & = [ \langle \textbf{Q}_{t+1}^1, \textbf{x} \rangle \cdots \langle \textbf{Q}_{t+1}^t, \textbf{x} \rangle \langle \textbf{Q}_{t+1}^{t+1}, \textbf{x} \rangle ] \\
& = [ \langle \textbf{Q}_t^1, \textbf{x} \rangle \cdots \langle \textbf{Q}_t^t, \textbf{x} \rangle \langle \textbf{Q}_{t+1}^{t+1}, \textbf{x} \rangle] \\
\end{align*}

Therefore, computing $\textbf{Q}_{t+1}^T\textbf{x}$ needs just the $\Theta(d)$ time needed for $\langle \textbf{Q}_{t+1}^{t+1}, \textbf{x} \rangle$. Then, we have that:

\begin{align*}
\textbf{Q}_t\textbf{Q}_t^T\textbf{x} & = \sum_{i=1}^t {\langle \textbf{Q}_t^i, \textbf{x} \rangle \textbf{Q}_t^i} \\
\textbf{Q}_{t+1}\textbf{Q}_{t+1}^T\textbf{x} & = \sum_{i=1}^{t+1} {\langle \textbf{Q}_{t+1}^i, \textbf{x} \rangle \textbf{Q}_{t+1}^i} \\
& = \textbf{Q}_t\textbf{Q}_t^T\textbf{x} + \langle \textbf{Q}_{t+1}^{t+1}, \textbf{x} \rangle \textbf{Q}_{t+1}^{t+1}
\end{align*}

Thus, we can come up with a new way of computing the residual:

\begin{equation*}
\mathcal{R}^t\textbf{x} \gets \mathcal{R}^{t-1}\textbf{x} - \langle \textbf{Q}_{t}^{\omega}, \textbf{x} \rangle \textbf{Q}_{t}^{\omega}
\end{equation*}

The algorithm is presented now in \textbf{Algorithm \ref{algo:OrthogonalMatchingPursuitMethodV2}}. Notice we don't need to compute $\textbf{a}^t$ at all during the inner-loop. Only at the end. We don't have a similar computation for the residual.

\begin{algorithm}
\caption{Orthogonal Matching Pursuit (Version 2)}
\label{algo:OrthogonalMatchingPursuitMethodV2}
\begin{algorithmic}
\Require $\textbf{C},\textbf{x},k$
\Ensure $\textbf{a}^k,\mathcal{R}^k\textbf{x}$
\State $\Lambda^0 \gets \phi$
\State $\textbf{a}^0 \gets \textbf{0}$
\State $\mathcal{R}^0\textbf{x} \gets \textbf{x}$
\State $\textbf{Q} \gets \textbf{0}$
\State $\textbf{R} \gets \textbf{0}$
\State $(\textbf{Q}^T\textbf{x}) \gets \textbf{0}$
\For {$t = \hcrange{1}{k}$}
\State $\omega \gets \arg \max_{i \in \hcrange{1}{w} \setminus \Lambda^{t-1}} \left| \langle \mathcal{R}^{t-1}\textbf{x} , \textbf{C}^i \rangle \right|$
\State $\Lambda^t \gets \Lambda^{t-1} \cup \omega$
\State $\textbf{R}^t \gets [ \langle \textbf{C}^\omega, \textbf{Q}^i \rangle~\mbox{\textbf{for} $i \in \hcrange{1}{d}$ \textbf{if} $i < t$ \textbf{else} $0$} ]$ \Comment{Fill column $t$ of $\textbf{R}$}
\State $\textbf{Q}^t \gets \textbf{C}^\omega - \sum_{i=1}^{t-1}\textbf{R}^t_i \textbf{Q}^i$ \Comment{Substract projection of $\textbf{C}^\omega$ on all $\textbf{Q}^i$ from $\textbf{C}^\omega$}
\State $\textbf{Q}^t \gets \textbf{Q}^t / \| \textbf{Q}^t \|_2$ \Comment{Normalize column $t$ of $\textbf{Q}$}
\State $(\textbf{Q}^T\textbf{x})_t \gets \langle \textbf{Q}^t, \textbf{x} \rangle$
\State $\mathcal{R}^t\textbf{x} \gets \mathcal{R}^{t-1}\textbf{x} -  (\textbf{Q}^T\textbf{x})_t \textbf{Q}^t$
\EndFor
\State $\textbf{a}^k \gets \textbf{R}_k^{-1}(\textbf{Q}^T\textbf{x})$
\end{algorithmic}
\end{algorithm}

The running time of this algorithm is $T_2^{OMP} = \Theta(k(wd + dk + dk + d + d + d) + k^23) = \Theta(kwd + dk^2)$, which is much better. A property of \textbf{OMP} is that $\mathcal{R}^t\textbf{x}$ is orthogonal to the span of $\textbf{Q}^1,\dots,\textbf{Q}^{t-1}$. Also, given the way it works, when $k = d$, then the residual $\mathcal{R}^d\textbf{x}$ becomes $\textbf{0}$, therefore the algorithm is guaranteed to stop. Notice however, that this does not imply an exact solving of our problem as seen in (reference posing of problem subject to hard constraint). If the algorithm is formulated with a $\delta$ instead of a fixed $k$, it will indeed stop when $t = d$, and the approximation will be perfect, barring any case of the features being linearly dependent, but this shouldn't be a problem in practice, but this does not guarantee that this is the smallest such set with $0$ recunstruction error. Or maybe there is a smaller set, but with bigger reconstruction error, which would be more useful in applications.

A final pursuit method studied is Optimized Orthogonal Matching Pursuit (\textbf{OOMP}) also known as Orthogonal Least Squares (\textbf{OLS}). It is similar to \textbf{OMP}. The major difference is the way in which the decision for which $\textbf{C}^\omega$ to use is taken. We select the one which gives the smallest least squares difference between $\textbf{x}$ and its reconstruction. Once this decision has been made, however, the $\textbf{a}$ produced is such that the difference between signal and reconstruction is minimal. The algorithm is presented in \textbf{Algorithm \ref{algo:OptimizedOrthogonalMatchingPursuitMethodV1}}.

\begin{algorithm}
\caption{Optimized Orthogonal Matching Pursuit (Version 1)}
\label{algo:OptimizedOrthogonalMatchingPursuitMethodV1}
\begin{algorithmic}
\Require $\textbf{C},\textbf{x},k$
\Ensure $\textbf{a}^k,\mathcal{R}^k\textbf{x}$
\State $\Lambda^0 \gets \phi$
\State $\textbf{a}^0 \gets \textbf{0}$
\State $\mathcal{R}^0\textbf{x} \gets \textbf{x}$
\For {$t = \hcrange{1}{k}$}
\State $\omega \gets \arg \min_{i \in \hcrange{1}{w} \setminus \Lambda^{t-1}} \min_{\textbf{a}} {\| \textbf{x} - \textbf{C}^{\Lambda^{t-1} \cup i}\textbf{a} \|_2^2}$
\State $\Lambda^t \gets \Lambda^{t-1} \cup \omega$
\State $\textbf{a}^t \gets \arg\min_{\textbf{a}} {\| \textbf{x} - \textbf{C}^{\Lambda^t}\textbf{a} \|_2^2}$
\State $\mathcal{R}^t\textbf{x} \gets \mathcal{R}^{t-1}\textbf{x} - a_\omega^t\textbf{C}^\omega$
\EndFor
\end{algorithmic}
\end{algorithm}

All the nice properties of \textbf{OMP} carry over to this method as well. Again, the biggest problem for this method is the prohibitive running cost. We have to solve several least squares problems for each iteration of the algorithm. The running time is $T_1^{OOMP} = \Theta(k^2T^{LS})$. The ideas which led to the second version of \textbf{OMP} can be employed here as well. We again iteratively build the $\textbf{Q}$ and $\textbf{R}$ matrices. First, notice that whenever we are asked to compute $\arg\min_\textbf{a} \| \textbf{x} - \textbf{C}\textbf{a} \|_2^2$ we want to basically minimize the residual $\textbf{R}\textbf{x}$. If we have a partial QR decomposition of $\textbf{Q}$, then, we can replace in the hat matrix and we obtain:

\begin{align*}
\textbf{x} - \textbf{C}\textbf{a}^\star & = \textbf{x} - \textbf{C}(\textbf{C}^T\textbf{C})^{-1}\textbf{C}^T\textbf{x} \\
& = \textbf{x} - \textbf{Q}\textbf{R}\textbf{R}^{-1}\textbf{Q}^T\textbf{x} \\
& = \textbf{x} - \textbf{Q}\textbf{Q}^T\textbf{x}
\end{align*}

This is exactly the result we obtained a few paragraphs above and we should unify this. Note that if we actually built a orthogonal $\textbf{Q}$, that is with $k = d$ and all features linearly independent, then the residual would be $0$, since $\textbf{Q}\textbf{Q}^T = \textbf{Q}^T\textbf{Q} = \mathbb{I}_d$. For our purposes, it is enough to notice that for computing the actual residual, only $\textbf{Q}$ is needed. Combing this with our previous knowledge, we can reformulate the optimization problem as such:

\begin{align*}
\omega & \gets \arg\min_{i \in \hcrange{1}{w} \setminus \Lambda^{t-1}} \arg\min_\textbf{a} \| \textbf{x} - \textbf{C}^{\Lambda^{t-1} \cup i}\textbf{a} \|_2^2 \\
& \gets \arg\min_{i \in \hcrange{1}{w} \setminus \Lambda^{t-1}} \| \textbf{x} - \textbf{Q}^{\Lambda^{t-1} \cup i}{\textbf{Q}^{\Lambda^{t-1} \cup i}}^T\textbf{x} \|_2^2 \\
& \gets \arg\min_{i \in \hcrange{1}{w} \setminus \Lambda^{t-1}} \| \mathcal{R}^{t-1}\textbf{x} - \langle \textbf{Q}^i, \textbf x \rangle \textbf{Q}^i \|_2^2
\end{align*}

Therefore we have reduced the complex $\Theta((w - t)T^{LS}) = \Theta((w - t)t^3)$ runtime of the first loop step to $\Theta((w - k)d)$, for a given $\textbf{Q}^i$. Quite an improvement! However, as it stands now, the $\textbf{Q}^i$ must be computed at each iteration for each one of the $w - t$ remaining features. We introduce the $\tilde{\textbf{C}}$ matrix of dimension equal to $\textbf{C}$. This is set initially as:

\begin{equation*}
\tilde{\textbf{C}} = \left[ \frac{\textbf{C}^1}{\|\textbf{C}^1\|_2} \left|\right. \dots \left|\right. \frac{\textbf{C}^w}{\|\textbf{C}^w\|_2} \right]
\end{equation*}

It contains the $\textbf{Q}^i$ at the first iteration. In general, at iteration $t$, after $\omega$ is found, each column of $\tilde{\textbf{C}}$ is updated as:

\begin{align*}
\tilde{\textbf{C}}^i & \gets \tilde{\textbf{C}}^i - \langle \tilde{\textbf{C}}^i, \tilde{\textbf{C}}^\omega \rangle \tilde{\textbf{C}}^\omega \\
\tilde{\textbf{C}}^i & \gets \tilde{\textbf{C}}^i / \| \tilde{\textbf{C}}^i \|_2
\end{align*}

At the end of the $k$ iterations, $k$ columns will be zero. Furthermore, at the start of each iteration, $\tilde{\textbf{C}}$ contains the features in $\textbf{C}$, orthogonalized with respect to $\textbf{Q}^1, \dots, \textbf{Q}^{t-1}$ and normalized. \textbf{Algorithm \ref{algo:OptimizedOrthogonalMatchingPursuitV2}} shows this second form of the \textbf{OOMP} algorithm.

\begin{algorithm}
\caption{Optimized Orthogonal Matching Pursuit (Version 2)}
\label{algo:OptimizedOrthogonalMatchingPursuitV2}
\begin{algorithmic}
\Require $\textbf{C},\textbf{x},k$
\Ensure $\textbf{a}^k,\mathcal{R}^k\textbf{x}$
\State $\Lambda^0 \gets \phi$
\State $\textbf{a}^0 \gets \textbf{0}$
\State $\mathcal{R}^0\textbf{x} \gets \textbf{x}$
\State $\textbf{R} \gets \textbf{0}$
\State $(\textbf{Q}^T\textbf{x}) \gets \textbf{0}$
\State $\tilde{\textbf{C}} \gets [ \textbf{C}^i / \|\textbf{C}^i\|_2 ~ \mbox{\textbf{for} $i \in \hcrange{1}{w}$} ]$
\For {$t = \hcrange{1}{k}$}
\State $\omega \gets \arg\min_{i \in \hcrange{1}{w} \setminus \Lambda^{t-1}} \| \mathcal{R}^{t-1}\textbf{x} - \langle \tilde{\textbf{C}}^i, \textbf x \rangle \tilde{\textbf{C}}^i \|_2^2$
\State $\Lambda^t \gets \Lambda^{t-1} \cup \omega$
\State $\textbf{R}^t \gets [ \langle \textbf{C}^\omega, \textbf{Q}^i \rangle~\mbox{\textbf{for} $i \in \hcrange{1}{d}$ \textbf{if} $i < t$ \textbf{else} $0$} ]$ \Comment{Fill column $t$ of $\textbf{R}$}
\State $\tilde{\textbf{C}} \gets [ \tilde{\textbf{C}}^i - \langle \tilde{\textbf{C}}^i, \tilde{\textbf{C}}^\omega \rangle \tilde{\textbf{C}}^\omega ~ \mbox{\textbf{for} $i \in \hcrange{1}{w}$} ]$
\State $\tilde{\textbf{C}} \gets [ \tilde{\textbf{C}}^i / \| \tilde{\textbf{C}}^i \|_2 ~ \mbox{\textbf{for} $i \in \hcrange{1}{w}$} ]$
\State $(\textbf{Q}^T\textbf{x})_t \gets \langle \tilde{\textbf{C}}^\omega, \textbf{x} \rangle$
\State $\mathcal{R}^t\textbf{x} \gets \mathcal{R}^{t-1}\textbf{x} -  (\textbf{Q}^T\textbf{x})_t \tilde{\textbf{C}}^t$
\EndFor
\State $\textbf{a}^k \gets \textbf{R}_k^{-1}(\textbf{Q}^T\textbf{x})$
\end{algorithmic}
\end{algorithm}

The major implementation difference between the two algorithms is basically that \textbf{OOMP} holds the full orthogonalized feature set, while \textbf{OMP} keeps only the features it actually uses to encode a feature. The runtime of this algorithm is $T_2^{OOMP} = \Theta(wd + k((w-k)d + kd + wd + wd + d + d)) = \Theta(kwd + dk^2)$, which is asymptotically the same with the time for \textbf{OMP}, but in practice more.

\chapter{Obtaining a Feature Set}

We now turn to the problem of learning the feature set $\textbf{C}$, given a coding method $\hat{\mathcal{C}}_\textbf{C}$ and a sample $\textbf{X} = \left[ \textbf{X}^1 \left|\right. \textbf{X}^2 \left|\right. \dots \left|\right. \textbf{X}^N \right] \in \mathbb{R}^{\hctimes{d}{N}}$ of linearized image patches of size $\hctimes{p}{p}$, usually extracted from either the whole training set or from a larger ``natural scenes'' dataset \cite{self-taught-learning}. The literature on feature set learning is vaster than that on coding. In particular, many methods can agnostic of a coding method and can work with whatever the user provides. More importantly, recent results show that the order of importance of the three components is: extractor architecture, coding method, feature set learning method (cite Ng). The other two can almost make up for a bad feature set (say, a completely random one).

The simplest solution is to use a set of randomly generated features. Each element from $\textbf{C}^i$ is generated independently. The only constraint is that the distribution allow positive and negative values and be unimodal. Other shape constraints do not influence performance (cite Ng and co. here). Although un-intuitive, random features work surprisingly well (cite Ng and co. here), hinting that the performance of this approach comes more from the architecture described in a previous section than the particular choice of features. Nevertheless, for best performance, a more structured feature set seems to be required. A correlation between feature extractor performance (as measured in classification tasks) with fixed architecture and, in turn, random features and trained features exists, and it can be employed for faster architecture selection.

Figure \ref{fig:RandomAndHandmadeFeatures}\subref{fig:RandomFeatures} shows an example of 16 randomly chosen features. We have for a particular feature $i$ that $C^i_{\alpha\beta} \leftarrow U[-1,1]$. A normalization also occurs. As expected, the filters do not show specific structure. However, they can be seen as selecting for sinusoids components of the inputs corresponding to the highest amplitude spectral component of the filter (cite Ng again here).

\begin{figure}
\centering
\subfigure[Random features.]{
  \includegraphics[width=0.35\textwidth]{ThesisData/RandomFeatures.png}
  \label{fig:RandomFeatures}
}
~~~~~~
\subfigure[Handmade features.]{
  \includegraphics[width=0.35\textwidth]{ThesisData/HandmadeFeatures.png}
  \label{fig:HandmadeFeatures}
}
\caption{Examples of simple features. Features are arranged in a $\hctimes{4}{4}$ array and scaled for presentation such that the highest intensity component corresponds to $1$ while the lowest intensity component corresponds to $0$.}
\label{fig:RandomAndHandmadeFeatures}
\end{figure}


A feature set can be, of course, crafted by hand. Figure \ref{fig:RandomAndHandmadeFeatures}\subref{fig:HandmadeFeatures} shows an example of 10 designed features, similar to Haar Wavelets. Needless to say, this is not a scalable strategy for the common case of thousands of features currently employed in state of the art methods. A well known set can be emplyed then. Figure \ref{fig:DCTFeatures} shows several DCT features, while (figure here) shows several Gabor Wavelet features. The former are more computationally easy to work with while the latter allow overcomplete representations, thanks to their parametrization.

\begin{figure}
\includegraphics[width=0.35\textwidth]{ThesisData/DCTFeatures.png}
\caption{DCT Features.}
\label{fig:DCTFeatures}
\end{figure}

The focus of this chapter is, however, on learned feature sets. We select as a training set for this purpose a set of $100000$ patches of size $d = \hctimes{11}{11}$ randomly extracted from the MNIST and SmallNORB datasets, respectively. A sample of these can be seen in image (cite here). Two pre-processing steps are applied. First, from each image the DC component is subtracted. Second, the mean of all the patches is extracted from each patch, such that $E[final_patch] = \textbf{O}_{\hctimes{11}{11}}$. The final training set can be seen in figure (fig here). See the experiments section for how the patches were extracted. Another dataset will be employed as well, for didactic purposes. It can be seen in Figure \ref{fig:ThreeComponentPointCloud}. It consists of three slightly overlapping subsets generated from a different gaussian each. The set has a relatively complicated topology and is useful for illustrating the differences in learning algorithms.

\begin{figure}
\centering
\subfigure[Original MNIST patches]{
  \includegraphics[width=0.35\textwidth]{ThesisData/MNISTPatches.png}
  \label{fig:MNISTPatches}
}
~~~~~~
\subfigure[Original SmallNORB patches]{
  \includegraphics[width=0.35\textwidth]{ThesisData/NORBSmallPatches.png}
  \label{fig:NORBSmallPatches}
}
\subfigure[MNIST patches with no DC component and mean substracted]{
  \includegraphics[width=0.35\textwidth]{ThesisData/MNISTPatchesDcMean.png}
  \label{fig:MNISTPatchesDcMean}
}
~~~~~~
\subfigure[SmallNORB patches with no DC component and mean substracted]{
  \includegraphics[width=0.35\textwidth]{ThesisData/NORBSmallPatchesDcMean.png}
  \label{fig:NORBSmallPatchesDCMean}
}
\subfigure[MNIST patches with no DC component and ZCA applied]{
  \includegraphics[width=0.35\textwidth]{ThesisData/MNISTPatchesDcMeanZCA.png}
  \label{fig:MNISTPatchesDcMeanZCA}
}
~~~~~~
\subfigure[SmallNORB patches with no DC component and ZCA applied]{
  \includegraphics[width=0.35\textwidth]{ThesisData/NORBSmallPatchesDcMeanZCA.png}
  \label{fig:NORBSmallPatchesDcMeanZCA}
}
\caption{Patches from the MNIST and SmallNORB datasets, in different stages of pre-processing.}
\label{fig:Patches}
\end{figure}

\begin{figure}
\centering
\includegraphics[width=0.8\textwidth]{ThesisData/ThreeComponentPointCloud.png}
\caption{The three component dataset.}
\label{fig:ThreeComponentPointColud}
\end{figure}

The simplest learning method is PCA. It finds the directions of maximum variance in the dataset. The results of applying PCA to learn a feature set can be seen in Figure (alal). PCA has several problems however. First, it cannot ``learn'' more than $d$ features. Second, it assumes an unrealistic model for image data: a Gaussian point cloud. The distribution of natural images is assumed to lie on a lower dimensional manifold embedded in the $d$-dimensional space and the Gaussian approximation is very crude. Comparing the features from PCA and Gabor Wavelets we can see clear distinctions. The latter perform better.

Maybe talk about ICA a little bit here.

There are at least two other general directions of learnings: autoencoders (with sparse and noisy variants) and Restricted Boltzmann Machines. We will not cover these here.

The main impulse for searching for a better algorithm came from the work of Olshausen and Field (cite them here), who, building on the work of (cite here), which showed that the receptive fields of cells in the V1 area of the brain, responsible for early visual processing, had response properties similar to bandpass selective and oriented filters, not unlike certain Wavelet families, devised an algorithm for learning such filters / feature sets and replicating the behaviour of the brain. Their central insight was that sparseness was the requried constraint on the coding phase.

[[[Show mathematical derivation of SparseNet algorithm]]]

Figure (there) shows examples of bases learned from these. The Zero Component Analysis transform was applied to the dataset before learning to decorelate the input components and speed up convergence.

Generalizing this, we can perform gradient descent or stochastic gradient descent with any coding method, in this double loop. For example, bases learned with inner product coding (bad) and MP, OMP and OOMP coding can be seen in figure (...).

Other methods in this direction are MOD and K-SVD (cite them here).

An alternative I've worked with and slightly improved (in the sense of generalizing it) is the Sparse Coding Neural Gas of (cite here). This is adaptation of the Neural Gas algorithm introduced in the context of vector quantization. Vector quantization can be considered as a stricter version of feature learning, where the codes are $1$-sparse and only a boolean ``indictator'' of the feature most similar to the input $\textbf{x}$, as measured by the Euclidean distance, is stored.

The Neural Gas algorithm is an iterative one. It begins by initializing $\textbf{C}$ to $w$ random observations from the training sample $\textbf{X}$. Then, for a number of $T_{max}$ iterations, an adaptation process takes place, which slowly changes $\textbf{C}$ in order to best represent the distribution over the input space. More precisely, at each iteration $t$ an observation is randomly selected from $\textbf{X}$ and distances to each element of $\textbf{C}$ are computed. Each feature is then modified in a manner proportional to the distorsion between it and the signal $\textbf{x}$, on the one hand, and the ranking of this distorsion in the list of all distorsions, on the other hand. Therefore, the update process includes a local and a global component. \textbf{Algorithm \ref{algo:NeuralGas}} gives the whole picture. Note that both a time decreasing learning factor is used as well as a time decreasing neighbourhood control. 

\begin{algorithm}
\caption{Neural Gas}
\label{algo:NeuralGas}
\begin{algorithmic}
\Require $\textbf{X},w,T_{max},\mu^0,\mu^{T_{max}},\lambda^0,\lambda^{T_{max}}$
\Ensure $\textbf{C}$
\State $\textbf{C} \gets \mbox{randomly select $w$ observations from $\textbf{X}$}$
\For {$t = \hcrange{1}{T_{max}}$}
\State $\mu^t \gets \mu^0 (\mu^{T_max} / \mu^0)^{t / T_{max}}$ \Comment {Current learning rate}
\State $\lambda^t \gets \lambda^0 (\lambda^{T_{max}} / \lambda^0)^{t / T_{max}}$ \Comment {Current neighbourhood control}
\State $\textbf{x} \gets \text{an observation from $\textbf{X}$}$
\State $\textbf{a} \gets [ ~ \|\textbf{x} - \textbf{C}^i\|_2^2 ~ \mbox{\textbf{for} $i \in \hcrange{1}{w}$} ~ ]$
\State $\textbf{C} \gets \textbf{C} + [ ~ \mu^t e^{-rank_{\textbf{a}}(a_i) / \lambda^t} (\textbf{x} - \textbf{C}^i) ~ \mbox{\textbf{for} $i \in \hcrange{1}{w}$} ~ ]$
\EndFor
\end{algorithmic}
\end{algorithm}

The Neural Gas algorithm works in the input space rather than feature set space. Adapting the algorithm to work with features and accept any coding method gives rise to a first version of the Sparse Coding Neural Gas. The major modification is the fact that each update is done according to Oja's Rule \cite{oja-rule} instead of the simple error term of the Neural Gas. The full algorithm is described in \textbf{Algorithm \ref{algo:SparseCodingNeuralGasV1}}. Notice that the $rank_{\textbf{a}}$ function considers absolute values, so that features are updated proportional to the magnitude of the associated response.

\begin{algorithm}
\caption{Sparse Coding Neural Gas V1}
\label{algo:SparseCodingNeuralGasV1}
\begin{algorithmic}
\Require $\textbf{X},w,\mathcal{C},T_{max},\lambda^0,\lambda^{T_{max}},\mu^0,\mu^{T_{max}}$
\Ensure $\textbf{C}$
\State $\textbf{C} \gets \mbox{randomly initialize $w$ normalized features}$
\For {$t = \hcrange{1}{T_{max}}$}
\State $\mu^t \gets \mu^0 (\mu^{T_{max}} / \mu^0)^{t / T_{max}}$  \Comment {Current learning rate}
\State $\lambda^t \gets \lambda^0 (\lambda^{T_{max}} / \lambda^0)^{t / T_{max}}$ \Comment {Current neighbourhood control}
\State $\textbf{x} \gets \text{an observation from $\textbf{X}$}$
\State $\textbf{a} \gets \mathcal{C}_{\textbf{C}}\{ \textbf{x} \}$
\State $\textbf{C} \gets \textbf{C} + [ ~ \mu^t e^{-rank_{\textbf{a}}(a_i) / \lambda^t} a_i (\textbf{x} - a_i \textbf{C}^i) ~ \mbox{\textbf{for} $i \in \hcrange{1}{w}$} ~ ]$
\State $\textbf{C} \gets \mbox{normalize each feature in $\textbf{C}$}$
\EndFor
\end{algorithmic}
\end{algorithm}

A further improvement is possible considering the fact that many coding methods are iterative and produce orderings of a subset $\hcrange{1}{w} \setminus \Lambda^t$ of the feature elements at each iteration. \textbf{MP} and \textbf{OMP} are such methods. A second version of the Sparse Coding Neural Gas is presented as \textbf{Algorithm \ref{algo:SparseCodingNeuralGasV2}}. Notice that at each iteration only the subset of previously unselected features is updated, instead of the whole set. Also, the variable $S^i$, which is a substitute for all the abstracted coding method specific information, must contain a copy of the original feature set $\textbf{C}$ at iteration $t$, before the inner-loop coding procedure. The reason for this is that $\textbf{C}$ is updated in the inner-loop and it can cause problems for the coder to change the features as time progresses.

\begin{algorithm}
\caption{Sparse Coding Neural Gas V2}
\label{algo:SparseCodingNeuralGasV2}
\begin{algorithmic}
\Require $\textbf{X},w,\mathcal{C},T_{max},\lambda^0,\lambda^{T_{max}},\mu^0,\mu^{T_{max}}$
\Ensure $\textbf{C}$
\State $C \gets \mbox{randomly initialize $w$ normalized features}$
\For {$t = \hcrange{1}{T_{max}}$}
\State $\mu^t \gets \mu^0 (\mu^{T_{max}} / \mu^0)^{t / T_{max}}$  \Comment {Current learning rate}
\State $\lambda^t \gets \lambda^0 (\lambda^{T_{max}} / \lambda^0)^{t / T_{max}}$ \Comment {Current neighbourhood control}
\State $\textbf{x} \gets \text{an observation from $\textbf{X}$}$
\State $S^0 \gets \text{initialize coding method specific state}$
\For {$i = \hcrange{0}{k}$}
\State $[\alpha^i~\Lambda^i~S^{i+1}] \gets \mathcal{C}_{\textbf{C}} \{S^i,\textbf{x}\}$ \Comment {$\alpha^i$ stores similarities for features in $\hcrange{1}{w} \setminus \Lambda^i$}
\State $\textbf{C} \gets \textbf{C} + [ ~ \mu^t e^{-rank_{\alpha^i}(\alpha_j^i) / \lambda^t} \alpha_j^i (\textbf{x} - \alpha_j^i \textbf{C}^j) ~ \mbox{\textbf{for} $j \in \hcrange{1}{w}\setminus\Lambda^i$} ~ ]$
\State $\textbf{C} \gets \mbox{normalize each feature in $\textbf{C}$}$
\EndFor
\EndFor
\end{algorithmic}
\end{algorithm}

\chapter{Experiments}

Talk about how to extract patches here. Minimum required variance and all.

\section{Coding and Sparseness}

\section{Learning Feature Sets}

\section{MNIST}

The MNIST database (cite source here) is used for handwritten digit recognition. It consists of $\hctimes{28}{28}$ grayscale images of the digits from $0$ to $9$. Examples for each class can be seen in Figure (figure). There are $60000$ training images and $10000$ test images. It is frequently used as a benchmark for classifier systems, although, as the score reference in (cite site with scores) will attest, the practical limits have been reached. Nevertheless, the system we built, especially using the Global Order nonlinearity, were able to break the record on this database, for the case of a non-modified dataset.

Here is an overview of what we did.

\section{NORB and SmallNORB}

\section{Text Detection Application}

\chapter{Conclusions}

\bibliographystyle{unsrt}
\bibliography{Bibliography}

\appendix

\end{document}
