\documentclass[12pt,a4paper,oneside,english]{UPBThesis}
\setcounter{secnumdepth}{3}
\setcounter{tocdepth}{3}
\usepackage{graphicx}
\usepackage[
  bookmarksnumbered,
  bookmarks,
  bookmarksopen=true,
  pdftitle={Dissertation},
  linktocpage,
  dvips]{hyperref}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{inputenc}
\usepackage[T1]{fontenc}
\usepackage{algpseudocode}
\usepackage{algorithm}
\usepackage{tabularx}
\usepackage{array}
\usepackage{subfigure}
\usepackage{multirow}
\usepackage{wrapfig}
\usepackage{afterpage}

\floatstyle{boxed}
\restylefloat{figure}
\algrenewcommand\algorithmicrequire{\textbf{input}}
\algrenewcommand\algorithmicensure{\textbf{output}}

\newcommand{\hctimes}[2]{{#1}\!\times\!{#2}}
\newcommand{\hcrange}[2]{\overline{{#1}\colon\!\!{#2}}}
\newcommand{\hcsignalspace}{\mathbb{R}^d}
\newcommand{\hcweightspace}{\mathbb{R}^w}
\newcommand{\hcdictspace}{\mathbb{R}^{\hctimes{w}{d}}}

\singlespacing

\begin{document}

\author{Horia COMAN}

\title{Deep Learning with Applications to Computer Vision}

\facultatea{Faculty of Electronics, Telecommunications and Information Technology}
\tiplucrare{Dissertation}
\domeniu{Applied Electronics and Information Engineering}
\catedra{Applied Electronics}
\campus{Leu}
\program{Advanced Curriculum Master Program in Imaging, Bioinformatics and Complex Systems (ITEMS)}
\titlulobtinut{Master of Science}
\director{Prof. Dr. Ing. Erhardt BARTH\\Prof. Dr. Rer. Nat. Thomas MARTINETZ\\As. Dr. Ing. \c{S}erban OPRI\c{S}ESCU}
\submissionmonth{September}
\submissionyear{2012}
\copyrightpage
\figurespage
\tablespage
\acknowledgements{The author would like to thank, for supporting this work, The German Academic Exchange Service Programme ``Ostpartnerschaften'', the University of L\"{u}beck,  and The Sectoral Operational Programme Human Resources Development 2007-2013 of the Romanian Ministry of Labour, Family and Social Protection through the Financial Agreement POSDRU/86/1.2/S/61756.}

\beforepreface


\abbreviations {
  CNN = Convolutional Neural Network\\
  MP = Matching Pursuit\\
  OMP = Orthogonal Matching Pursuit\\
  OOMP = Optimized Orhtogonal Matching Pursuit\\
  PCA = Principal Component Analysis\\
  ZCA = Zero Component Analysis\\
  SVM = Support Vector Machine\\
  OLS = Ordinary Least Squares\\
  LASSO = What\\
}
%\preface{}
\afterpreface

\unnumberedsection{Stream of thought writing}

We focus more on coding methods. According to (cite Ng and guys here), this is the most important part in such a system. Dictionaries could well be random.

Some conventions: Vectors are written in boldface with lower case letters, like this $\textbf{v}$. Addressing elment $i$ is done with a subscript, like this: $\textbf{v}_i$. Accessing a set of element $\Lambda$ is done with a subscript, but using the whole set, like this: $\textbf{v}_\Lambda$. It is understood that the elements are produced in increasing index order. Matrices are written in boldface with capital letters, like this $\textbf{A}$. Accessing a certain column $i$ is done with a superscript, like this: $\textbf{A}^i$. Accessing a certain element $j$ in column $i$ is done with both a superscript and a subscript, like this: $\textbf{A}^i_j$. Accessing a certain subset of columns $\Lambda$ is done with a superscript, but using the whole set, like this: $\textbf{A}^\Lambda$. It is understood that the columns are produced in increasing index order. Accessing a certain subset of indices $\Lambda$ of column $i$ is done with both a superscript and a subscript, like this: $\textbf{A}^i_\Lambda$. It is understood that the elements are produced in increasing column order. Accessing a certain subset of indices $\Theta$ of a certain subset of columns $\Lambda$ is done using both a superscript and a subscript, like this: $\textbf{A}^\Lambda_\Theta$. It is understood that the columns are produced in increasing index order and the elements from each column are produced in increasing index order as well.

In all our experiments, images were represented by matrices stored in column-major order. Linearization respects this convention. Therefore, the first $p$ coefficients of a linearized patch correspond to the first column, the next $p$ correspond to the second column and so on. This convention will be used wherever we do a linearization, regardeless of dimension. Talk about tensor linearization here.

This work presents several contributions: a general architecture for feature extraction for computer vision classification or regression, with a neural-network like structure, but much more general, a novel way of computing nonlinearities in it which produces state-of-the-art results on two benchmark image recognition tasks, a description of a text detection system, and experimental results with it and the two image recongnition tasks.

Text detection: we are given an input image, which contains several areas of text. The goal is to detect these areas and then perform OCR on them. The task however is harder than OCR as usually natural images are provided and text is written in a variety of fonts, on a variety of backgrounds, and filmed from a variety of positions, such that the classical OCR of top-down view does not hold. Ideally this should be solved in video and online.

The problem has been studied by other authors (cite them here) and in the context of deep learning (cite them here).

We assume throught the usage of the lating alphabet and left-to-right, top-to-bottom writing conventions.

This work shows how I built a system for text detection, using, as the title suggests, Deep Neural Networks.

First we'll consider the problem on a single image. The problem can be broken down into several steps: first, find the regions of the image where text is, then for each image, corresponding to a word, segment it into characters. Then, recognize each character. Finally, build a word from each image, considering the letters as well as spellcheck-type data which is a must.

The first subproblem is arguably the hardest. Suppose we have a general procedure which detects for a small image of size $\hctimes{p}{p}$ if that is a letter or a nonletter. Given such a procedure we run it over all patches found by centering each pixel. A new ``binary'' image is obtained which contains $1$ if the pixel $i,j$ is the center of a letter patch and $0$ otherwise. Several such images are built for varying values of $p$. This image gives a general indication of where the text is. Still, segmentation must be done, and, possibly oriented text segments must be generated from each image. In cases where a rectangle in one image overlaps with a rectangle in another, a method for specifying which has priority should be employed. This is a mostly hand-crafting engineering problem, and as such is solved in an heuristic way.

After segemntation, a series of images are produced from the original image, which contain the words. They still must be processed and understood.

The next step is thus the breaking of words into letters. The images we obtain will generally have a rectangular form, more wide than tall. We consider that a single row of letters is present, therefore segmentation only has to worry about the horizontal axis. If the image is of size $\hctimes{m}{n}$ then we have at most $n$ letters in it, in a very degenerate case, and at least one. This puts an upper bound on what we're about to do. We must also have a classifier here which knows how to separate into letters and the several punctuation marks common in words. This is a tougher classifier though and a probabilistic output would be nice for a Dynamic Programming approach to this.

Much of the actual work for this project has gone into building the classifier for letter/nonletter and then specific letters.

The classifier is based on a Convolutional Neural Network (CNN) (cite CNN here). However, it suffers some generalizations. Finally, we obtain a general coding system, neural network inspired, but not necesarily a classical network.

For the moment assume we are given the image $I$ of size $\hctimes{m}{n}$. This can be a patch from the original image we wish to classify as letter / nonletter or a segment of an extracted image for which we wish to find the actual letter. In any case, the task at hand is image recognition. It is a complex task. The naive approach of simply training a classifier on the raw image data will usually yield poor performance, except in the most simple of cases. Image classification systems therefore consist of two parts: a feature extraction phase and a proper classification phase. The goal of the first is to transform in an input image so as to be more easily handled by the latter phase. As the name suggest, it does so by ``extracting'' certain properties of an image, generically called ``features''. For example, a feature can be the amount of blue in the top of an image, or the variance in a small patch from the image, or the response to a oriented bandpass filter around a certain position etc. Extraction might mean a simple check for the presence or absence of the property, or might be a gradual percentace of presence of the feature in the image, or it might be computed in a whole other way. All the features amployed by a feature extraction layer form the ``feature set''. In general, the extraction process might not consider all features as independent, that is, a high presence of a feature in the set migth trigger a lower resulting presence of another feature. The input of the feature extraction phase is the image, $I$, and the output is a description of the feature set, usually called the ``feature vector'', and denoted by $F$, which contains one element for each feature in the feature set, and which thus codes the extraction result. For the latter stage, only $F$ is used.

Feature engineering, that is, the human-guided process of feature construction, is a big part of most machine learning applications. Usually, for best performance, deep knowledge of the problem domain must be coupled with an understanding of the Statistics and Machine Learning.

In the last decade, however, automatic feature construction, known as unsupervised feature learning (cite people here), has received much attention, producing systems with state of the art performance, and, in the field of Neural Networks, allowing the construction of so called Deep Neural Networks, which were thought impossible to build by standard methods (cite google and microsoft recent results with such networks). The whole field of Deep Learning deals with automatic feature extraction in one way or another.

\chapter{Overview of Feature Extraction}
\label{chap:OverviewFeatureExtraction}

In our framework, a feature is a small image patch - a filter - of some sort. The full feature set consists of $w$ normalized square images of size $d = \hctimes{p}{p}$ with $p < \min(m,n)$. We denote it by $\textbf{C} = \left[ \textbf{C}^1 \left|\right. \textbf{C}^2 \left|\right. \dots \left|\right. \textbf{C}^w \right] \in \mathbb{R}^{\hctimes{d}{w}}$. Normalization implies $\|\mathbf{C}^i\| = 1$ for all $i \in \hcrange{1}{w}$. For now we consider the feature set as given. Speaking broadly, though, feature sets can be generated randomly \cite{random-weights-feature-learning,beyond-simple-featureshow}, or taken to be a well-known set, such as DCT bases or Gabor Wavelet bases \cite{simple-method-sparse-coding}, or, even learned from a sample of patches extracted from the training set of the classification system \cite{emergence-sparse-coding,sparse-coding-strategy-V1,tiny-images}. More details about feature set construction will be given in the appropriate section.

A feature extractor is composed of a number of recoding modules, linked in a serial manner. The input image is propagated through these modules and transformed by them. The first such module starts with the image $\textbf{I}$ and produces a set of images $\{\textbf{I}_i^1\}_{i=1}^{l_1}$. The next module accepts as input $\{\textbf{I}_i^1\}_{i=1}^{l_1}$ and produces $\{\textbf{I}_i^2\}_{i=1}^{l_2}$. The last module outputs $\{\textbf{I}_i^k\}_{i=1}^{l_k}$, which is linearized to produce the feature vector $\mathbf{F}$. The stacking of these module produces a deep forward path for the input image. As we will see, this transformation can be modelled as a neural network. Hence the name Deep Neural Network. 

A module consists of several standard steps, called \emph{coding}, \emph{nonlinear}, \emph{polarity split} and \emph{reduce}. The coding step depends on an module associated feature set $\textbf{C}^m$. It produces a set of $w_m$ images of the same size as the input ones. In the simplest case, each image is obtained by convoluting the original image with the feature ${\textbf{C}^m}^i$. This is the setup of Convolutional Neural Networks \cite{gradient-based-learning}. More general, the set of $w_m$ pixels at positions $(\alpha,\beta)$ from all the images are not independent. The next section will describe coding methods, and how all the pixels at the same position can be obtained at once. In any case, using convolution, the local computation is evident. Pixel $I^m_i(\alpha,\beta)$ depends on a patch of pixels, centered at $(\alpha,\beta)$ from the input images. At the limit, all stage output images depend on all the input images (they consider the mean of the input images, for example). However, for better performance \cite{gradient-based-learning}, each output image depends only on a small subset of the input images. After coding, comes the nonlinear phase. Here, inputs can passed unaffected (the degenerate case), or they can be passed through a sigmoid like nonlinearity (logistic or hyperbolic tangent). Besides the general architecture, our own contribution is the introduction of a global nonlinearity. The experiments section will reveal the advantages of this method. Whereas the identity and sigmoid maps acted on each pixel independently, this method considers the whole set of $w_k$ pixels corresponding to input $(\alpha,\beta)$ and transforms the largest of these into $1$, and the rest in an exponentially decreasing manner from $1$ in a fashion dependent on the rank of that value in the decreasingly sorted list of pixel values. The signs of the pixels is kept, only the magnitude is important. The polarity split step is optional. If it is enabled, it effectivly doubles the number of output images (therefore $l_m = 2w_m$ in this case). One set of images keeps only the pixels with greater than zero values and makes the others zero. The other set keeps only the pixels with lesser than zero values and makes the others zero. The sign can be kept or not, depending on the situation. In any case, this computation is stricly feedforward and pixel-local. This step has been shown to improve classifier performance \cite{importance-encoding-sparse-coding-vq}, mostly because of a cheap increase in output dimensionality. Finally, the reduce step performs a subsampling of the output of the polarity split layer. This also works on each image independently. Each image is split into non-overlapping patches, as best as possible (several rows and columns at the end might be removed this way). From each patch a single value is computed. Several methods exist for this as well: a single fixed value can be used each time (a subsampling), the maximum absolute value can be used, the maximum with sign kept can be used, the sum of absolute values can be used and the sum of squares can be used. In any case, this is a patch-local feedforward computation as well.

Several options for building feature extractors thus become apparent. The best for a certain dataset should be determined by cross-validation as well as computational concerns - for example the global order nonlinearity produces very good results, but it requires many sorts, whereas the sigmoid nonlinearity produces slightly worse results, but is simpler. In any case, the experiments section presents guidelines for all these methods.

The feature extractor is therefore a mostly feed-forward and patch-local or pixel-local affair. The only parameters it requires are the feature sets for each recoder module. These are determined in an unsupervised fashion externally from the recoder and just plugged in at run time. This is unsupervised pre-training. The whole stack is built in a greedy fashion. The first set of features is obtained from the original images. Then these are coded with the first recoder. From the new set, features for the second layer are learned. The process continues as you would expect. 

The extractor can be modelled as a more general neural network, with specialized nodes and more complex activation functions (functions which depend on the results of other units). For extractors with a classical feedforward structure or at least one for which gradients can be computed for all parameters, an extra step can be done after unsupervised pre-training. The network can be ``fine-tuned''. That is, back-propagation is applied to the network and parameters are once-more adapted to the classification task at hand as with shallow networks. The initial weights are given by the values of the learned feature sets. Extra parameters can be added to the network in this case. Weights for the reduce layer and bias terms for the code and reduce layers. A very sharp logistic or hyperbolic tangent can be used to implement a polarity split layer in this case as well.

Fine-tuning wasn't the approach I took however. Just the simple extractor with one layer proved sufficient for the needs of the studied applications. 

[[[Maybe talk about HoG and SWIFT here as well.]]]

\chapter{Coding and Different Coding Methods}
\label{chap:CodingAndCodingMethods}

\textbf{Chapter \ref{chap:OverviewFeatureExtraction}} described the general model of a feature extractor for image processing tasks, as well as the simpler extractor we employ in this work, which uses just one recoder module. Central to this model is the coding stage which makes explicit use of the feature set \textbf{C} and is the the only step of the four defined which does so. More precisely, it transformes the input image $\textbf{I}$ into a set of $w$ output images $\{\textbf{I}_i\}_{i=1}^w$. Image $\textbf{I}_i$ roughly corresponds to a particular feature $\textbf{C}^i$ from the feature set. Coefficient $\textbf{I}_i[\alpha,\beta]$ is the ``response'' of $\textbf{I}$ at position $(\alpha,\beta)$ to feature $\textbf{C}^i$. In general, this response can depend on the whole of $\textbf{C}$, not just onf $\textbf{C}^i$.

The basic task of the coding stage is to obtain the set of $w$ coefficients corresponding to the patch of size $\hctimes{p}{p}$ centered at position $(\alpha,\beta)$ from $\textbf{I}$. Such a procedure is called a \emph{coding method}. In order to keep things simple, we will assume the inputs are $d$-dimensional signals. Thus, the $\hctimes{p}{p}$ patches previously discussed must be linearized such that $d = p^2$. We reffer to the input of a coding method as the \emph{signal} and to the output as the \emph{code}. The signal is then $\textbf{x} \in \hcsignalspace$ while the code is $\textbf{a} \in \hcweightspace$. We understand the code as a recipe for reconstructing the signal from the feature set. The most common such interpretation is that each coefficient $\textbf{a}_i$ determines in what proportion feature $\textbf{C}^i$ contributes to the reconstruction $\mathcal{R}_\textbf{C}(\textbf{a})$ of $\textbf{x}$. In other words, $\mathcal{R}_\textbf{C}$ is a linear combination of $\textbf{C}$ with coefficients provided by $\textbf{a}$. Formally, we have:

\begin{equation*}
\mathcal{R}_\textbf{C}(\textbf{a}) = \sum_{i=1}^{w}{a_i \textbf{C}^i} = \textbf{C}\textbf{a}
\end{equation*}

The simplest coding method builds $\textbf{a}$ as the inner-product between the signal and each feature. The inner-product is a natural similarity measure in the Euclidean spaces we usually work with and is directly proportional to the ``angle'' between the signal and feature as well as the length of the signal. The feature is normalize and thus not enter the picture. In $\hcsignalspace$ we find $\textbf{a}^\star$ and $\textbf{C}\textbf{a}^\star$ as:

\begin{align*}
\textbf{a}^\star & = \textbf{C}^T\textbf{x} \\
\textbf{C}\textbf{a}^\star & = \textbf{C}\textbf{C}^T\textbf{a}^\star
\end{align*}

When the feature set is critically-complete and the vectors are orthonormal, then we have a perfect reconstruction because $\textbf{C}\textbf{C}^T = \mathbb{I}$. This is the case when using DCT features or certain wavelet feature sets. For critically-complete and non-orthonormal, under-complete or especially for over-complete feature sets, the reconstruction is no longer perfect. While for under-complete feature sets this is expressed as a bounded (?) difference between the original and the reconstructed, in the over-complete case, things are worse - the error actually increases with $w$, as more features are added to the reconstruction, with no strategy for diminuishing them in order to improve approximation accuracy.

Because of this, simply using the inner-product coding method will yield poor results for most tasks, without extra limiting factors in the architecture. More results and intuitions for this can be seen in the \textbf{Section \ref{sec:CodingReconstructionSparseness}}.

Coding methods try to build an $\textbf{a}^\star$ that will produce the best approximation of $\textbf{x}$, usually subject to some constraints. We commonly define ``best approximation'' as the one which minimizes the Euclidean distance between the signal and its reconstruction. In other words, we formulate our search for $\textbf{a}^\star$ as a least squares problem.

\begin{equation}
\textbf{a}^{\star} = \arg\min_{\textbf{a}} \frac{1}{2} \| \textbf{x} - \textbf{C}\textbf{a} \|_2^2
\end{equation}

The cost function has many nice properties which make it ammendable to analysis: it is continuos, differentiable and convex \cite{elements-statistical-learning}. The gradient with respect to $\textbf{a}$ is:

\begin{align*}
\frac{d \left( \frac{1}{2}\|\textbf{x} - \textbf{C}\textbf{a}\|_2^2 \right)}{d\textbf{a}} & = \frac{1}{2} \frac{d {\left( \textbf{x} - \textbf{C}\textbf{a} \right)}^T\left( \textbf{x} - \textbf{C}\textbf{a} \right)}{d\textbf{a}} \\
& = {\left( \textbf{x} - \textbf{C}\textbf{a} \right)}^T \frac{d \left( \textbf{x} - \textbf{C}\textbf{a} \right)}{d\textbf{a}} \\
& = - {\left( \textbf{x} - \textbf{C}\textbf{a} \right)}^T \textbf{C} \\
& = - {\left( \textbf{C}^T \left( \textbf{x} - \textbf{C}\textbf{a} \right) \right)}^T \\
& = - \textbf{C}^T \textbf{x} + \textbf{C}^T\textbf{C}\textbf{a}
\end{align*}

There are two major cases to consider: $w \leq d$ and $w > d$. The first one corresponds to $\textbf{C}$ being a tall matrix while the second one corresponds to $\textbf{C}$ being a wide matrix. It is the second case that interests us, as, most of the times, our feature sets will be overcomplete. We will study the first case however, as methods for the second are extensions of those for the first.

The first case admits two approaches for finding the optimum: an analytical one and an iterative one. Whichever is best depends on the situation. The analytical solution is derived from the nice properties of the cost function. More precisely, the optimum $\textbf{a}^\star$ is found as the point in $\hcweightspace$ where the gradient becomes $\textbf{0}$. We have:

\begin{align*}
- \textbf{C}^T \textbf{x} + \textbf{C}^T\textbf{C}\textbf{a}^\star & = \textbf{0} \\
\textbf{C}^T\textbf{C}\textbf{a}^\star & = \textbf{C}^T\textbf{x} \\
\textbf{a}^\star & = {\left( \textbf{C}^T\textbf{C} \right)}^{-1} \textbf{C}^T\textbf{x}
\end{align*}

To compute the reconstruction of $\textbf{x}$ we replace $\textbf{a}^\star$ in $\textbf{C}\textbf{a}^\star$ and obtain:

\begin{align*}
\textbf{C}\textbf{x} & = \textbf{C}{\left( \textbf{C}^T\textbf{C} \right)}^{-1} \textbf{C}^T\textbf{x} \\
& = \textbf{H}\textbf{x}
\end{align*}

The matrix $\textbf{H}$ is known as the hat matrix, and, as we will see, it is a projection matrix. Since $\textbf{C}$ is tall, the matrix $\textbf{C}^T\textbf{C}$ is positive definite and it is thus invertible (check these). A first geometric interpretatios sees $\textbf{a}^\star$ as describing the $w$-dimensional hyperplane which best approximates a function $f:\hcweightspace \rightarrow \mathbb{R}$ which is known only at $d$ points $\{(\textbf{C}[i,:],x_i)\}_{i=1}^d$. This is the setup of the linear regression problem. A second geometric interpretation sees $\textbf{a}^\star$ as the vector in the span of the columns of $\textbf{C}$ which is closest to $\textbf{x}$. It is this latter interpretation which interests us because it tells us that the least squares solution will produce a reconstruction which is the best we can hope for with regards to the most commonly used error metric - the Euclidean distance between signal and reconstruction. The hat matrix is then the projector from $\hcsignalspace$ into the columnspace of $\textbf{C}$.

Actually computing the inverse of $\textbf{C}^T\textbf{C}$ is not recommended however, as it is both computationally expensive and numerically unstable. It turns out it is not required either. Consider the QR decomposition $\textbf{C} = \textbf{Q}\textbf{R}$, where $\textbf{Q}$ is the same size as $\textbf{C}$ and its columns form an orthonormal basis for $\hcsignalspace$ and $\textbf{R}$ is an $\hctimes{w}{w}$ matrix which is upper triangular. Computing $\textbf{a}^\star$ and $\textbf{C}\textbf{a}$ becomes, by a simple substitution:

\begin{align*}
\textbf{a}^\star & = \textbf{R}^{-1}\textbf{Q}^T\textbf{x} \\
\textbf{C}\textbf{x} & = \textbf{Q}\textbf{Q}^T\textbf{x}
\end{align*}

We have reduced the problem of finding the optimal code and reconstruction to transpose operations, matrix-vector multiplications and solving upper triangular systems of equations, all of which can be efficiently performed in $\Theta(wd)$ time. Finding $\textbf{Q}$ and $\textbf{R}$ is another story however, but $\Theta(w^2d)$ algorithms based on the Gram-Schmidt decomposition exist for this purpose. The overall time complexity is therefore $\Theta(w^2d)$. Notice also that, when $w = d$, $\textbf{Q}$ actually is an orthonormal matrix, therefore $\textbf{C}\textbf{x} = \textbf{Q}\textbf{Q}^T\textbf{x} = \textbf{x}$. We obtain perfect reconstruction, which is what we would have expected since the columnspace of $\textbf{C}$ is equal to $\hcsignalspace$.

For very large values of $d$ and $w$, performing the computations required for the analytic solution can be prohibitive. Since the minimization problem is convex, techniques from classical optimization can be used to find a suitable $\textbf{a}^\star$ without actually performing any inversion of $\textbf{C}$. This is the interative solution to the least squares problem. Most algorithms require the ability to compute the cost function as well as the gradient of this function with respect to $\textbf{a}$. Both operations can be implemented in terms of transpose operations, matrix-vector multiplications and vector additions. Actually computing them has time complexity $\Theta(wd)$ for the value of the cost function at a certain $\textbf{a}$ and $\Theta(wd)$ for the gradient at that point. The overall time complexity is therefore $\Theta(Twd)$, where $T$ is the number of steps required for convergence and is, hopefully, much smaller than $d$ or $w$.

In the second major case, for $w > d$, the methods we've developed so far do not produce good results. The cost function retains its nice properties, with the exception that there isn't a single unique global minima, but rather a flat region of equal lowest cost. The iterative approach will produce one of these global minima, but the analytical solution via QR decomposition will flat out not work, as it can compute at most $d$ columns of $\textbf{Q}$, leaving the other $w - d$ equal to $\textbf{0}$. The core problem is how to restrict the space of global minima, to ones which better interest us. This is an example of an ill-posed problem and it is usually solved by augmenting the cost function in some way, in order to restrict the space of possible options. What this means basically is that using only the reconstruction error as a guide for finding $\textbf{a}^\star$ is not a precise enough constraint and we need more information about the structure of a good code in order to pick the best one.

One approach which has shown good results in practice for feature learning \cite{emergence-sparse-coding,sparse-coding-strategy-V1} and classification purposes \cite{sparse-features-audio-classification,importance-encoding-sparse-coding-vq,simple-method-sparse-coding} is the restriction that the code be \emph{sparse}, that is, the code should have a small number of large magnitude coefficients and a large number of near zero coefficients. The optimal code $\textbf{a}^\star$ is then the code which best approximates $\textbf{x}$ and is sparse.

One way to measure sparseness for any signal is with the sparseness coefficient, $\rho(\textbf{a}) \in [0,1]$, which counts how many non-zero coefficients relative to the signal size there are. Large values of $\rho(\textbf{a})$ indicate very dense signals, with almost no zero values, while small values indicate sparse signals. In practice, we care that a coefficient is ``large'enogh'', say, greater in absolute value than a s mall positive real $\delta$, rather than simply different from $0$. For code words we have:

\begin{equation*}
\rho(\textbf{a}) = \frac{\left| \{ a_i \left|\right. i \in \hcrange{1}{w} \wedge \left|a_i\right| > \delta \} \right|}{w}
\end{equation*}

Another way to measure sparseness is to look at the distribution of the coefficients in a signal. \textbf{Figure \ref{fig:CodingMethodsCoeffHists}\subref{fig:CodingMethodsCoeffSparse}} shows the histogram for a sparse signal while \textbf{Figure \ref{fig:CodingMethodsCoeffHists}\subref{fig:CodingMethodsCoeffFull}} shows the histogram for a normal signal. As you can see, sparse signals have very narrow and peaker around $0$ histograms while normal signals have spread histograms.

\begin{figure}
\centering
\subfigure[Histogram for sparse signal]{
  \includegraphics[width=0.40\textwidth]{thesis_data/coding_methods/coeff_sparse.png}
  \label{fig:CodingMethodsCoeffSparse}
}
\subfigure[Histogram for normal signal]{
  \includegraphics[width=0.40\textwidth]{thesis_data/coding_methods/coeff_full.png}
  \label{fig:CodingMethodsCoeffFull}
} 
\label{fig:CodingMethodsCoeffHists}
\caption{Histograms for sparse and normal code words. There are $61$ bins spread evenly in $[-1.5,1.5]$. All horizontal axes are between $[-1.5,1.5]$ all vertical one are between $[0,250]$.}
\end{figure}

The methods we are going to study will directly constrain either $\rho(\textbf{a})$ or the shape of the distribution in order to achieve the desired level of sparseness. The augumented optimization problem becomes:

\begin{equation*}
\textbf{a}^{\star} = \arg\min_{\textbf{a}} \frac{1}{2} \| \textbf{x} - \textbf{C}\textbf{a} \|_2^2 + \lambda \mathbb{S}(\textbf{a})
\end{equation*}

Here, $\mathbb{S}$ is the sparseness constraint on the cost function while $\lambda$ is a regularization parameter which controls the tradeoff between reconstruction accuracy and sparseness.

A first method, introduced by \cite{emergence-sparse-coding} uses an $\mathbb{S}$ of the form $\sum_{i=1}^w{S(a_i / \sigma_{\textbf{a}})}$, where $\sigma$ is the standard deviation of the code word coefficients and $S$ is chosen from a set of sparsity inducing functions such as $\log(1 + x^2)$ or $-e^{-x^2}$ etc. This is one half of the SparseNet method for feature learning, adapted as a coding method. The $S$ function must be chosen such that is has small values for small absolute value inputs and large values for large absolute value inputs. The $\mathbb{S}$ constraint then will have small values when all $a_i$ are small. The only large coefficients will be forced by the reconstructing part of the problem. There is a probabilistic interpretation and derivation of SparseNet, which will be discussed in more detail in the feature learning section. Suffice to say that the different forms of $S$ place a different prior or the coefficients of $\textbf{a}$, when we consider them as independently drawn from a sparse-like distribution. This method, therefore, can be considered one that tries to shape the distribution in order to achieve sparseness.

A solution to the new problem can be found only iteratively, through a modified version of the gradient based algorithm for ordinary least squares. The new formulation of the optimization problem is convex as well, though it is more difficult (from pratical experience) to minimize than the simpler formulation. The new gradient of the cost function with respect to $\textbf{a}$ becomes:

\begin{align*}
\frac{d \left( \left( \frac{1}{2}\|\textbf{x} - \textbf{C}\textbf{a}\|_2^2 \right) + \lambda\sum_{i=1}^{w}{S(\frac{a_i}{\sigma_{\textbf{a}}})} \right)}{d\textbf{a}} & = \frac{d \left( \frac{1}{2}\|\textbf{x} - \textbf{C}\textbf{a}\|_2^2 \right)}{d\textbf{a}} + \lambda\sum_{i=1}^{w}\frac{\partial S(\frac{a_i}{\sigma_{\textbf{a}}})}{\partial a_i} \\
& = -\textbf{C}^T\textbf{x} + \textbf{C}^T\textbf{C}\textbf{a} + \frac{\lambda}{\sigma_{\textbf{a}}} \sum_{i=1}^{w} {\left( \frac{\partial S}{\partial a_i} \right) \left( \frac{a_i}{\sigma_{\textbf{a}}} \right)}
\end{align*}

Computing the cost function is $\Theta(wd)$, that is, the same, in the limit, with the ordinary least squares cost function time complexity, though, in practice, it will be a little bit slower thanks to the $\Theta(w)$ cost of computing $\mathbb{S}$, which also involves more complex operations than the addition and multiplication necessary for the approximation part. Computing the new gradient is, again, equal in the limit with the ordinary least squares gradient, that is $\Theta(wd)$, but the same remarks are valid here as for the cost function. The total time complexity will be $\Theta(Twd)$, with the mention that $T$ here might be bigger than the $T$ for the non-sparse case.

Methods that try to control $\rho(\textbf{a})$ directly all work by somehow selecting the most important features in $\textbf{C}$ and computing only the coefficients with regards to them. Examples include subset selection, forward selection and backward selection \cite{elements-statistical-learning}, which are part of a general class of feature selection algorithms, much used in Machine Learning. The canonical form for such methods is $\mathcal{L}_0$ regularized least squares, which tries to solve the following problem:

\begin{equation*}
\textbf{a}^{\star} = \arg\min_{\textbf{a}} \frac{1}{2} \| \textbf{x} - \textbf{C}\textbf{a} \|_2^2 + \lambda \| \textbf{a} \|_0
\end{equation*}

Again, $\lambda$ is the regularization parameter which controls the tradeoff between reconstruction error and sparseness, while $\|\textbf{a}\|_0$ is the $0$-norm of the code word, which is defined as $\sum_{i=1}^{w}a_i^0$ and which is equal to the number of non-null entries in $\textbf{a}$. Directly solving this problem is hard though [how hard is it?]. Luckily, it has been shown that the $\mathcal{L}_1$ regularized least squares problem, known as the \textbf{LASSO} \cite{lasso}, is a good approximation to this, and produces the good solution most of the time \cite{undetermined-minimal-L1}. There is even an efficient algorithm for solving this problem \cite{least-angle-regression}, known as Least Angle Regression. In general we can talk about the $\mathcal{L}_p$ regularized least squares problem, which solves the problem:

\begin{equation*}
\textbf{a}^{\star} = \arg\min_{\textbf{a}} \frac{1}{2} \| \textbf{x} - \textbf{C}\textbf{a} \|_2^2 + \lambda \| \textbf{a} \|_p
\end{equation*}

The $\mathcal{L}_2$ norm problem also has a good behaviour and is known from inverse problem theory as Ridge Regression.

We will focus on a group of iterative methods for computing $\textbf{a}^\star$, known as matching pursuits, which originate in the signal processing community. The general problem they try to solve is the Lagrangian dual of the $\mathcal{L}_0$ norm least squares problem from before:

\begin{align*}
\textbf{a}^\star = \arg & \min_\textbf{a} \|\textbf{x} - \textbf{C}\textbf{a}\|_2^2 \\
& \text{~~\textbf{st}~~} \|\textbf{a}\|_0 \leq k
\end{align*}

Here $k$ is a user controlled sparseness control parameter. An optimum $\textbf{a}^\star$ will have $\rho(\textbf{a}^\star) \leq k$, thereby powerfully controlling the desired sparseness. The problem is not easier to solve in this form, naturally, but it gives us a new avenue for attack, namely, we try to select the subset of $k$ features which have the minimum reconstruction error. Subset selection and forward and backward selection work on this same principle as well and are somewhat related.

The pursuit methods are greedy approximations methods. They do not solve the exact problem, but provide a good enough solution. The algorithm has a common form for all methods. In broad terms, it decomposes the original signal into smaller and smaller residual signals, which are matched with the features in $\textbf{C}$. At each step $\textbf{a}$ is filled with one new coefficient, corresponding to the feature $\omega$ which best matches the current residual. More specifically, let the initial residual $\mathcal{R}^0\textbf{x} = \textbf{x}$. At iteration $t$, let $\textbf{C}^\omega$ be the most similar feature in $\textbf{C}$, relative to $\mathcal{R}^t\textbf{x}$. The updated code and residual, $\textbf{a}^{t+1}$ and $\mathcal{R}^{t+1}\textbf{x}$, are produced by decomposing $\mathcal{R}^t\textbf{x}$ in terms of $\textbf{C}^\omega$. After $k$ iterations, $\textbf{a}^k$ is returned as the code associated to $\textbf{x}$, and $\mathcal{R}^k\textbf{x}$ is returned as a measure of the ability of the algorithm to reconstruct the signal in terms of $\textbf{C}$. The difference between the several methods consists in how they find $\textbf{C}^\omega$ and how they update $\textbf{a}^{t+1}$. The general procedure is illustrated in \textbf{Algorithm \ref{algo:GeneralPursuitMethod}}. At the end of this algorithm we obtain $\textbf{x} = \textbf{C}\textbf{a}^k + \mathcal{R}^k\textbf{x}$.

Also, the norm of the final residual $\mathcal{R}^k\textbf{x}$ tends to $0$ as $k \rightarrow +\infty$ for sensible choices of $\text{\textbf{sim}}$ and $\text{\textbf{next}}$ functions. In the limit, the equality becomes $\textbf{x} = \textbf{C}\textbf{a}^{+\infty}$, that is, perfect approximation is obtained.

\begin{algorithm}
\caption{The General Pursuit Method}
\label{algo:GeneralPursuitMethod}
\begin{algorithmic}
\Require $\textbf{C},\textbf{x},k$
\Ensure $\textbf{a}^k,\mathcal{R}^k\textbf{x}$
\State $\Lambda^0 \gets \phi$
\State $\textbf{a}^0 \gets \textbf{0}$
\State $\mathcal{R}^0\textbf{x} \gets \textbf{x}$
\For {$t = \hcrange{1}{k}$}
\State $\omega \gets \arg \max_{i \in \textbf{dom}} \text{\textbf{sim}}(\mathcal{R}^{t-1}\textbf{x},\textbf{C},\Lambda^{t-1},i)$
\State $\Lambda^t \gets \Lambda^{t-1} \cup \omega$
\State $\textbf{a}^t \gets \text{\textbf{next}}(\textbf{a}^{t-1},\mathcal{R}^{t-1}\textbf{x},\textbf{C},\Lambda^t,\omega)$
\State $\mathcal{R}^t\textbf{x} \gets \mathcal{R}^{t-1}\textbf{x} - a_\omega^t\textbf{C}^\omega$
\EndFor
\end{algorithmic}
\end{algorithm}

\renewcommand{\arraystretch}{1.5}
\begin{table}
  \caption{The different parametrization for pursuit methods}
  \label{table:PursuitParametrization}
  \begin{tabularx}{\textwidth}{|l|>{\centering}X|>{\centering}X|c|}
       \hline
        Method & $\text{\textbf{sim}}$ function & $\text{\textbf{next}}$ function & $\text{\textbf{dom}}$ domain\\ \hline \hline
        \textbf{MP} & $\left| \langle \mathcal{R}^{t-1}\textbf{x} , \textbf{C}^i \rangle \right|$ & $\textbf{a}^{t-1} + \langle \mathcal{R}^{t-1}\textbf{x} , \textbf{C}^\omega \rangle \delta_\omega$ & $\hcrange{1}{w}$ \\  \hline
        \textbf{OMP} & $\left| \langle \mathcal{R}^{t-1}\textbf{x} , \textbf{C}^i \rangle \right|$ & $\arg\min_{\textbf{a}} {\| \textbf{x} - \textbf{C}^{\Lambda^t}\textbf{a} \|_2^2}$ & $\hcrange{1}{w} \setminus \Lambda^{t-1}$ \\ \hline
        \textbf{OOMP} & $\min_{\textbf{a}} {\| \textbf{x} - \textbf{C}^{\Lambda^{t-1} \cup i}\textbf{a} \|_2^2}$ & $\arg\min_{\textbf{a}} {\| \textbf{x} - \textbf{C}^{\Lambda^t}\textbf{a} \|_2^2}$ & $\hcrange{1}{w} \setminus \Lambda^{t-1}$ \\
       \hline
    \end{tabularx}
\end{table}
\renewcommand{\arraystretch}{1.0}

The simplest pursuit method, introduced in \cite{matchingpursuit1}, is Matching Pursuit (\textbf{MP}). \textbf{Table \ref{table:PursuitParametrization}} shows what form the $\text{\textbf{sim}}$ and $\text{\textbf{next}}$ functions take in this case. An important property of this algorithm is that for every $t$, $\|\mathcal{R}^t\textbf{x}\|_2^2 \geq \|\mathcal{R}^{t+1}\textbf{x}\|_2^2$ and, furthermore, with a decay that is exponential. The two major drawbacks of this method are that the approximation at time $t$, $\textbf{C}\textbf{a}^t$, is not optimal with respect to the selection of features $\Lambda^t$; and that for the residual norm to actually reach small enough values, a $k > w$ could be necessary. However, these drawbacks are not critical for classification purposes, and, because of its simplicity and speed, we use it in our experiments. $\mathcal{R}^t\textbf{x}$ is orthogonal with respect to $\textbf{C}^\omega$, but not necesarily every feature in $\textbf{C}^{\Lambda^t \setminus \omega}$.

The specialization of the General Pursuit Method to Matching Pursuit can be seen in \textbf{Algorithm \ref{algo:MatchingPursuitMethodV1}}.

\begin{algorithm}
\caption{The Matching Pursuit Method (Version 1)}
\label{algo:MatchingPursuitMethodV1}
\begin{algorithmic}
\Require $\textbf{C},\textbf{x},k$
\Ensure $\textbf{a}^k,\mathcal{R}^k\textbf{x}$
\State $\Lambda^0 \gets \phi$
\State $\textbf{a}^0 \gets \textbf{0}$
\State $\mathcal{R}^0\textbf{x} \gets \textbf{x}$
\For {$t = \hcrange{1}{k}$}
\State $\omega \gets \arg \max_{i \in \hcrange{1}{w}} \left| \langle \mathcal{R}^{t-1}\textbf{x}, \textbf{C}^i \rangle \right|$
\State $\Lambda^t \gets \Lambda^{t-1} \cup \omega$
\State $\textbf{a}_\omega^t \gets \textbf{a}_\omega^{t-1} + \langle \mathcal{R}^{t-1}\textbf{x} , \textbf{C}^\omega \rangle$
\State $\mathcal{R}^t\textbf{x} \gets \mathcal{R}^{t-1}\textbf{x} - a_\omega^t\textbf{C}^\omega$
\EndFor
\end{algorithmic}
\end{algorithm}

The runtime of this variant is $T_1^{MP} = \Theta(kwd)$. This is somewhat suboptimal, as, in most cases, it will be quadratic in $d$ ($d$ and $w$ tend to have the same order of magnitude), or even cubic in $d$ for cases where $k$ has the same order of magnitude with $d$. The source of this complexity is the small optimization problem we have to solve in the inner loop. This just requires a walk through all the features and computing of a ``similarity'' of cost $\Theta(d)$. The per iteration time is therefore $\Theta(dw)$. Notice, however the following decomposition of $\mathcal{R}^t\textbf{x}$:

\begin{align*}
\mathcal{R}^t\textbf{x} & = \mathcal{R}^{t-1}\textbf{x} - a_\omega^t\textbf{C}^\omega \\
\mathcal{R}^t\textbf{x} & = \mathcal{R}^{t-1}\textbf{x} - \langle \mathcal{R}^{t-1}\textbf{x}, \mathcal{C}^\omega \rangle \textbf{C}^\omega
\end{align*}

Now, if we want to compute the inner product between $\mathcal{R}^t\textbf{x}$ and a $\textbf{C}^i$, we have:

\begin{align*}
\langle \mathcal{R}^t\textbf{x}, \textbf{C}^i \rangle & = \langle \mathcal{R}^{t-1}\textbf{x} - \langle \mathcal{R}^{t-1}\textbf{x}, \mathcal{C}^\omega \rangle \textbf{C}^\omega , \textbf{C}^i \rangle \\
& = \langle \mathcal{R}^{t-1}\textbf{x}, \textbf{C}^i \rangle - \langle \mathcal{R}^{t-1}\textbf{x}, \textbf{C}^\omega \rangle \langle \textbf{C}^\omega, \textbf{C}^i \rangle
\end{align*}

Thus, we have expressed the computation of the inner product between the current residual and a feature in terms of the inner product between the previous residual and the same feature and an ``adjustement term'', which is a product of the maximum inner product of the previous iteration and the inner product of the best previous feature and the current feature under consideration. The new algorithm can be seen in \textbf{Algorithm \ref{algo:MatchingPursuitMethodV2}}.

\begin{algorithm}
\caption{The Matching Pursuit Method (Version 2)}
\label{algo:MatchingPursuitMethodV2}
\begin{algorithmic}
\Require $\textbf{C},\textbf{x},k$
\Ensure $\textbf{a}^k,\mathcal{R}^k\textbf{x}$
\State $\textbf{a}^0 \gets \textbf{0}$
\State $\mathcal{S}^f \gets \textbf{C}^T\textbf{C}$
\State $\mathcal{S}^s \gets \textbf{C}\textbf{x}$
\For {$t = \hcrange{1}{k}$}
\State $\omega \gets \arg \max_{i \in \hcrange{1}{w}} \left| \mathcal{S}_i^s \right|$
\State $\textbf{a}_\omega^t \gets \textbf{a}_\omega^{t-1} + \mathcal{S}_\omega^s$
\State $\mathcal{S}^s \gets [ \mathcal{S}_i^s - \mathcal{S}_\omega^s \mathcal{S}_{\omega i}^f ~ \mbox{\textbf{for} $i \in \hcrange{1}{w}$} ]$
\EndFor
\end{algorithmic}
\end{algorithm}

As it is, the runtime of this algorithm is $T_2^{MP} = \Theta(w^2d + kw)$, which does not seem like much of an improvement. However, in the broad picture, many patches are coded with the same feature set. Therefore, we need to compute only once $\mathcal{S}^f$, and then use it for all patches. The average time complexity for coding $N$ patches if $\Theta(w^2d/N + kw)$ which tends to $\Theta(kw)$ for large enough $N$. Therefore, the actual time complexity is $T_2^{MP} = \Theta(kw)$, which is an order of magnitude lower than the original time $T_1^{MP}$.

An improvement to \textbf{MP} is Orthogonal Matching Pursuit (\textbf{OMP}) \cite{matchingpursuit2,orthopursuit}, which addresses the two issues discussed above. Again, \textbf{Table \ref{table:PursuitParametrization}} shows the forms the $\text{\textbf{sim}}$ and $\text{\textbf{next}}$ functions take. Also, notice that at each iteration, only the features not considered before are processed. All the properties of \textbf{MP} hold here as well. At iteration $t$ we solve a classical least squares problem with the subset of features in $\Lambda^t$. This makes sense, because the matrix will always be tall - $\left| \Lambda^t \right| \leq d$ - otherwise we produce a $0$ norm residual which effectively stops the algorithm. The algorithm is presented in \textbf{Algorithm \ref{algo:OrthogonalMatchingPursuitMethodV1}}.

\begin{algorithm}
\caption{Orthogonal Matching Pursuit (Version 1)}
\label{algo:OrthogonalMatchingPursuitMethodV1}
\begin{algorithmic}
\Require $\textbf{C},\textbf{x},k$
\Ensure $\textbf{a}^k,\mathcal{R}^k\textbf{x}$
\State $\Lambda^0 \gets \phi$
\State $\textbf{a}^0 \gets \textbf{0}$
\State $\mathcal{R}^0\textbf{x} \gets \textbf{x}$
\For {$t = \hcrange{1}{k}$}
\State $\omega \gets \arg \max_{i \in \hcrange{1}{w} \setminus \Lambda^{t-1}} \left| \langle \mathcal{R}^{t-1}\textbf{x} , \textbf{C}^i \rangle \right|$
\State $\Lambda^t \gets \Lambda^{t-1} \cup \omega$
\State $\textbf{a}^t \gets \arg\min_{\textbf{a}} {\| \textbf{x} - \textbf{C}^{\Lambda^t}\textbf{a} \|_2^2}$
\State $\mathcal{R}^t\textbf{x} \gets \mathcal{R}^{t-1}\textbf{x} - a_\omega^t\textbf{C}^\omega$
\EndFor
\end{algorithmic}
\end{algorithm}

Therefore, at iteration $t$, we chose the feature $\textbf{C}^\omega$ of maximum ``similarity'' according to the inner product measure. We add this to the current feature set $\textbf{C}^{\Lambda^t}$. In order to compute $\textbf{a}^t$ we find the point $\textbf{a}$ in the column space of $\textbf{C}^{\Lambda^t}$ closest to $\textbf{x}$ as the solution of the least squares problem. This solves the major problem of \textbf{MP}, that is, the found $\textbf{a}^k$ was not even optimal in the approximation sense to the input signal. \textbf{OMP} produces such a solution, guaranteed, and, more importantly, uses the information continuously to drive the search for new features, instead of just at the end, as a naive modification of $\textbf{MP}$ might suggest. Notice however, that, even though the solution found has minimum approximation error, it is only in respect to the subset of features selected. There is no guarantee that there isn't a better solution with regards to another subset, or a solution of the same cost but using a smaller number of features. We are not solving an $NP$-hard problem in polynomial time with this algorithm.

The major problem of the \textbf{OMP} algorithm, as it is, is the extra complexity introduced by the need to solve the least squares problem. We have $T_1^{OMP} = \Theta(kwd + kT^{LS}))$. We know that $T^{LS} = \Theta(w^2d)$ if we use the analytical solution, therefore we have $T_1^{OMP} = \Theta(kwd + kw^2d) = \Theta(kw^2d)$, which is an order of magnitude larger than the naive algorithm time complexity for \textbf{MP}. This is somewhat prohibitive. However, an approach suggested by \cite{matchingpursuit2,pursuitdifferences} combines the computation of the QR factorization with the gradual update ideas from the second version of the Matching Pursuit.

The QR decomposition can be computed using the Gram-Schmidt process. This builds $\textbf{Q}$ one column at a time and $\textbf{R}$ one column at a time. Furthermore, the building of column $i \in \hcrange{1}{t}$ depends only on columns $\hcrange{1}{i}$ from $\textbf{C}^{\Lambda^t}$ and columns $\hcrange{1}{i-1}$ from $\textbf{Q}$. Also, the matrices $\textbf{C}^{\Lambda^t}$ and $\textbf{C}^{\Lambda^{t+1}}$ are identical, except the latter has one extra column. Using our notation, this column, corresponding to position $\omega^{t+1} \in \hcrange{1}{w}$, is inserted ``between'' two other columns from $\textbf{C}^{\Lambda^t}$. However, for our purposes, the order of the columns is irrelevant. Therefore we can consider $\textbf{C}^{\Lambda^{t+1}} = \left[\textbf{C}^{\Lambda^t} \left|\right. \textbf{C}^{\omega^{t+1}} \right]$. The two matrices have the same first $t$ columns, therefore, the first $t$ columns of the associated $\textbf{Q}$ matrices of the QR decomposition will be identical. The same goes for the first $t$ rows of the $\textbf{R}$ matrices. This naturally leads us to the version of the algorithm which accumulates the $\textbf{Q}$ and $\textbf{R}$ matrices.

Another optimization follows, when computing the residual. Consider the current residual $\mathcal{R}^t\textbf{x}$ and the next residual $\mathcal{R}^t\textbf{x}$. We have:

\begin{align*}
\mathcal{R}^t\textbf{x} & = \textbf{x} - \textbf{C}^{\Lambda^t}\textbf{a}^t \\
& = \textbf{x} - \textbf{Q}\textbf{Q}^T\textbf{x}
\end{align*}

Making use of the fact that the $\textbf{Q}$ matrix used in the computation of $\mathcal{R}^{t+1}\textbf{x}$ contains just one extra column besides the one used in the computation of $\mathcal{R}^t\textbf{x}$, we have:

\begin{align*}
\textbf{Q}_t^T\textbf{x} & = [ \langle \textbf{Q}_t^1, \textbf{x} \rangle \cdots \langle \textbf{Q}_t^t, \textbf{x} \rangle ] \\
\textbf{Q}_{t+1}^T\textbf{x} & = [ \langle \textbf{Q}_{t+1}^1, \textbf{x} \rangle \cdots \langle \textbf{Q}_{t+1}^t, \textbf{x} \rangle \langle \textbf{Q}_{t+1}^{t+1}, \textbf{x} \rangle ] \\
& = [ \langle \textbf{Q}_t^1, \textbf{x} \rangle \cdots \langle \textbf{Q}_t^t, \textbf{x} \rangle \langle \textbf{Q}_{t+1}^{t+1}, \textbf{x} \rangle] \\
\end{align*}

Therefore, computing $\textbf{Q}_{t+1}^T\textbf{x}$ needs just the $\Theta(d)$ time needed for $\langle \textbf{Q}_{t+1}^{t+1}, \textbf{x} \rangle$. Then, we have that:

\begin{align*}
\textbf{Q}_t\textbf{Q}_t^T\textbf{x} & = \sum_{i=1}^t {\langle \textbf{Q}_t^i, \textbf{x} \rangle \textbf{Q}_t^i} \\
\textbf{Q}_{t+1}\textbf{Q}_{t+1}^T\textbf{x} & = \sum_{i=1}^{t+1} {\langle \textbf{Q}_{t+1}^i, \textbf{x} \rangle \textbf{Q}_{t+1}^i} \\
& = \textbf{Q}_t\textbf{Q}_t^T\textbf{x} + \langle \textbf{Q}_{t+1}^{t+1}, \textbf{x} \rangle \textbf{Q}_{t+1}^{t+1}
\end{align*}

Thus, we can come up with a new way of computing the residual:

\begin{equation*}
\mathcal{R}^t\textbf{x} \gets \mathcal{R}^{t-1}\textbf{x} - \langle \textbf{Q}_{t}^{\omega}, \textbf{x} \rangle \textbf{Q}_{t}^{\omega}
\end{equation*}

The algorithm is presented now in \textbf{Algorithm \ref{algo:OrthogonalMatchingPursuitMethodV2}}. Notice we don't need to compute $\textbf{a}^t$ at all during the inner-loop. Only at the end. We don't have a similar computation for the residual. In any case, having such an optimization wouldn't modify the order of the algorithm, but it would just do a speedup in practice.

\begin{algorithm}
\caption{Orthogonal Matching Pursuit (Version 2)}
\label{algo:OrthogonalMatchingPursuitMethodV2}
\begin{algorithmic}
\Require $\textbf{C},\textbf{x},k$
\Ensure $\textbf{a}^k,\mathcal{R}^k\textbf{x}$
\State $\Lambda^0 \gets \phi$
\State $\textbf{a}^0 \gets \textbf{0}$
\State $\mathcal{R}^0\textbf{x} \gets \textbf{x}$
\State $\textbf{Q} \gets \textbf{0}$
\State $\textbf{R} \gets \textbf{0}$
\State $(\textbf{Q}^T\textbf{x}) \gets \textbf{0}$
\For {$t = \hcrange{1}{k}$}
\State $\omega \gets \arg \max_{i \in \hcrange{1}{w} \setminus \Lambda^{t-1}} \left| \langle \mathcal{R}^{t-1}\textbf{x} , \textbf{C}^i \rangle \right|$
\State $\Lambda^t \gets \Lambda^{t-1} \cup \omega$
\State $\textbf{R}^t \gets [ \langle \textbf{C}^\omega, \textbf{Q}^i \rangle~\mbox{\textbf{for} $i \in \hcrange{1}{d}$ \textbf{if} $i < t$ \textbf{else} $0$} ]$ \Comment{Fill column $t$ of $\textbf{R}$}
\State $\textbf{Q}^t \gets \textbf{C}^\omega - \textbf{Q}^{\hcrange{1}{t-1}}\textbf{R}^t_{\hcrange{1}{t-1}}$ \Comment{Substract projection of $\textbf{C}^\omega$ on all $\textbf{Q}^i$ from $\textbf{C}^\omega$}
\State $\textbf{Q}^t \gets \textbf{Q}^t / \| \textbf{Q}^t \|_2$ \Comment{Normalize column $t$ of $\textbf{Q}$}
\State $\textbf{R}^t_t \gets \langle \textbf{C}^\omega, \textbf{Q}^t \rangle$
\State $(\textbf{Q}^T\textbf{x})_t \gets \langle \textbf{x}, \textbf{Q}^t \rangle$
\State $\mathcal{R}^t\textbf{x} \gets \mathcal{R}^{t-1}\textbf{x} -  (\textbf{Q}^T\textbf{x})_t \textbf{Q}^t$
\EndFor
\State $\textbf{a}^k \gets \textbf{R}^{-1}(\textbf{Q}^T\textbf{x})$
\end{algorithmic}
\end{algorithm}

The running time of this algorithm is $T_2^{OMP} = \Theta(k(wd + dk + dk + d + d + d) + k^23) = \Theta(kwd + dk^2)$, which is much better. A property of \textbf{OMP} is that $\mathcal{R}^t\textbf{x}$ is orthogonal to the span of $\textbf{Q}^1,\dots,\textbf{Q}^{t-1}$. Also, given the way it works, when $k = d$, then the residual $\mathcal{R}^d\textbf{x}$ becomes $\textbf{0}$, therefore the algorithm is guaranteed to stop.

[[[Recompute running times, being 1.more careful and 2.taking into account that $\textbf{R}$ is just of size $\hctimes{k}{k}$]]]

A final pursuit method studied is Optimized Orthogonal Matching Pursuit (\textbf{OOMP}) \cite{optimizedorthopursuit,orthogonalls} also known as Orthogonal Least Squares (\textbf{OLS}). It is similar to \textbf{OMP}. The major difference is the way in which the decision for which $\textbf{C}^\omega$ to use is taken. We select the one which gives the smallest least squares difference between $\textbf{x}$ and its reconstruction. Once this decision has been made, however, the $\textbf{a}$ produced is such that the difference between signal and reconstruction is minimal. The algorithm is presented in \textbf{Algorithm \ref{algo:OptimizedOrthogonalMatchingPursuitMethodV1}}.

\begin{algorithm}
\caption{Optimized Orthogonal Matching Pursuit (Version 1)}
\label{algo:OptimizedOrthogonalMatchingPursuitMethodV1}
\begin{algorithmic}
\Require $\textbf{C},\textbf{x},k$
\Ensure $\textbf{a}^k,\mathcal{R}^k\textbf{x}$
\State $\Lambda^0 \gets \phi$
\State $\textbf{a}^0 \gets \textbf{0}$
\State $\mathcal{R}^0\textbf{x} \gets \textbf{x}$
\For {$t = \hcrange{1}{k}$}
\State $\omega \gets \arg \min_{i \in \hcrange{1}{w} \setminus \Lambda^{t-1}} \min_{\textbf{a}} {\| \textbf{x} - \textbf{C}^{\Lambda^{t-1} \cup i}\textbf{a} \|_2^2}$
\State $\Lambda^t \gets \Lambda^{t-1} \cup \omega$
\State $\textbf{a}^t \gets \arg\min_{\textbf{a}} {\| \textbf{x} - \textbf{C}^{\Lambda^t}\textbf{a} \|_2^2}$
\State $\mathcal{R}^t\textbf{x} \gets \mathcal{R}^{t-1}\textbf{x} - a_\omega^t\textbf{C}^\omega$
\EndFor
\end{algorithmic}
\end{algorithm}

All the nice properties of \textbf{OMP} carry over to this method as well. Again, the biggest problem for this method is the prohibitive running cost. We have to solve several least squares problems for each iteration of the algorithm. The running time is $T_1^{OOMP} = \Theta(k^2T^{LS})$. The ideas which led to the second version of \textbf{OMP} can be employed here as well. We again iteratively build the $\textbf{Q}$ and $\textbf{R}$ matrices. For our purposes, it is enough to notice that for computing the actual residual, only $\textbf{Q}$ is needed. Combing this with our previous knowledge, we can reformulate the optimization problem as such:

\begin{align*}
\omega & \gets \arg\min_{i \in \hcrange{1}{w} \setminus \Lambda^{t-1}} \arg\min_\textbf{a} \| \textbf{x} - \textbf{C}^{\Lambda^{t-1} \cup i}\textbf{a} \|_2^2 \\
& \gets \arg\min_{i \in \hcrange{1}{w} \setminus \Lambda^{t-1}} \| \textbf{x} - \textbf{Q}^{\Lambda^{t-1} \cup i}{\textbf{Q}^{\Lambda^{t-1} \cup i}}^T\textbf{x} \|_2^2 \\
& \gets \arg\min_{i \in \hcrange{1}{w} \setminus \Lambda^{t-1}} \| \mathcal{R}^{t-1}\textbf{x} - \langle \textbf{Q}^i, \textbf x \rangle \textbf{Q}^i \|_2^2
\end{align*}

Therefore we have reduced the complex $\Theta((w - t)T^{LS}) = \Theta((w - t)t^3)$ runtime of the first loop step to $\Theta((w - k)d)$, for a given $\textbf{Q}^i$. Quite an improvement! However, as it stands now, the $\textbf{Q}^i$ must be computed at each iteration for each one of the $w - t$ remaining features. We introduce the $\tilde{\textbf{C}}$ matrix of dimension equal to $\textbf{C}$. This is set initially as:

\begin{equation*}
\tilde{\textbf{C}} = \left[ \frac{\textbf{C}^1}{\|\textbf{C}^1\|_2} \left|\right. \dots \left|\right. \frac{\textbf{C}^w}{\|\textbf{C}^w\|_2} \right]
\end{equation*}

It contains the $\textbf{Q}^i$ at the first iteration. In general, at iteration $t$, after $\omega$ is found, each column of $\tilde{\textbf{C}}$ is updated as:

\begin{align*}
\tilde{\textbf{C}}^i & \gets \tilde{\textbf{C}}^i - \langle \tilde{\textbf{C}}^i, \tilde{\textbf{C}}^\omega \rangle \tilde{\textbf{C}}^\omega \\
\tilde{\textbf{C}}^i & \gets \tilde{\textbf{C}}^i / \| \tilde{\textbf{C}}^i \|_2
\end{align*}

At the end of the $k$ iterations, $k$ columns will be zero. Furthermore, at the start of each iteration, $\tilde{\textbf{C}}$ contains the features in $\textbf{C}$, orthogonalized with respect to $\textbf{Q}^1, \dots, \textbf{Q}^{t-1}$ and normalized. \textbf{Algorithm \ref{algo:OptimizedOrthogonalMatchingPursuitV2}} shows this second form of the \textbf{OOMP} algorithm.

\begin{algorithm}
\caption{Optimized Orthogonal Matching Pursuit (Version 2)}
\label{algo:OptimizedOrthogonalMatchingPursuitV2}
\begin{algorithmic}
\Require $\textbf{C},\textbf{x},k$
\Ensure $\textbf{a}^k,\mathcal{R}^k\textbf{x}$
\State $\Lambda^0 \gets \phi$
\State $\textbf{a}^0 \gets \textbf{0}$
\State $\mathcal{R}^0\textbf{x} \gets \textbf{x}$
\State $\textbf{R} \gets \textbf{0}$
\State $(\textbf{Q}^T\textbf{x}) \gets \textbf{0}$
\State $\tilde{\textbf{C}} \gets [ \textbf{C}^i / \|\textbf{C}^i\|_2 ~ \mbox{\textbf{for} $i \in \hcrange{1}{w}$} ]$
\For {$t = \hcrange{1}{k}$}
\State $\omega \gets \arg\min_{i \in \hcrange{1}{w} \setminus \Lambda^{t-1}} \| \mathcal{R}^{t-1}\textbf{x} - \langle \textbf{x}, \tilde{\textbf{C}}^i \rangle \tilde{\textbf{C}}^i \|_2^2$  \Comment{Choose $i$ such that residual is minimized}
\State $\Lambda^t \gets \Lambda^{t-1} \cup \omega$
\State $\textbf{R}^t \gets [ \langle \textbf{C}^\omega, \textbf{Q}^i \rangle~\mbox{\textbf{for} $i \in \hcrange{1}{d}$ \textbf{if} $i < t$ \textbf{else} $0$} ]$ \Comment{Fill column $t$ of $\textbf{R}$}
\State $\textbf{Q}^t \gets \tilde{\textbf{C}}^\omega$
\State $\textbf{R}^t_t \gets \langle \textbf{C}^\omega, \textbf{Q}^t \rangle$
\State $(\textbf{Q}^T\textbf{x})_t \gets \langle \textbf{x}, \textbf{Q}^t \rangle$
\State $\mathcal{R}^t\textbf{x} \gets \mathcal{R}^{t-1}\textbf{x} -  (\textbf{Q}^T\textbf{x})_t \textbf{Q}^t$
\State $\tilde{\textbf{C}} \gets [ \tilde{\textbf{C}}^i - \langle \tilde{\textbf{C}}^i, \textbf{Q}^t \rangle \textbf{Q}^t ~ \mbox{\textbf{for} $i \in \hcrange{1}{w}$} ]$
\State $\tilde{\textbf{C}} \gets [ \tilde{\textbf{C}}^i / \| \tilde{\textbf{C}}^i \|_2 ~ \mbox{\textbf{for} $i \in \hcrange{1}{w}$} ]$
\EndFor
\State $\textbf{a}^k \gets \textbf{R}^{-1}(\textbf{Q}^T\textbf{x})$
\end{algorithmic}
\end{algorithm}

The major implementation difference between the two algorithms is basically that \textbf{OOMP} holds the full orthogonalized feature set, while \textbf{OMP} keeps only the features it actually uses to encode a feature. The runtime of this algorithm is $T_2^{OOMP} = \Theta(wd + k((w-k)d + kd + wd + wd + d + d)) = \Theta(kwd + dk^2)$, which is asymptotically the same with the time for \textbf{OMP}, but in practice more.

A special case occurs when $k=1$. In this case all the methods are the same and identical to just using correlation.

\chapter{Obtaining a Feature Set}
\label{chap:ObtainingFeatureSet}

We now turn to the problem of learning the feature set $\textbf{C}$, given a coding method $\hat{\mathcal{C}}_\textbf{C}$ and a sample $\textbf{X} = \left[ \textbf{X}^1 \left|\right. \textbf{X}^2 \left|\right. \dots \left|\right. \textbf{X}^N \right] \in \mathbb{R}^{\hctimes{d}{N}}$ of linearized image patches of size $\hctimes{p}{p}$, usually extracted from either the whole training set or from a larger ``natural scenes'' dataset \cite{self-taught-learning}. The literature on feature set learning is vaster than that on coding. In particular, many methods can agnostic of a coding method and can work with whatever the user provides. More importantly, recent results show that the order of importance of the three components is: extractor architecture, coding method, feature set learning method \cite{importance-encoding-sparse-coding-vq,random-weights-feature-learning,beyond-simple-features}. The other two can almost make up for a bad feature set (say, a completely random one).

The simplest solution is to use a set of randomly generated features. Each element from $\textbf{C}^i$ is generated independently. The only constraint is that the distribution allow positive and negative values and be unimodal. Other shape constraints do not influence performance \cite{random-weights-feature-learning}. Although un-intuitive, random features work surprisingly well, hinting that the performance of this approach comes more from the architecture described in a previous section than the particular choice of features. Nevertheless, for best performance, a more structured feature set seems to be required. A correlation between feature extractor performance (as measured in classification tasks) with fixed architecture and, in turn, random features and trained features exists, and it can be employed for faster architecture selection.

Figure \ref{fig:RandomAndHandmadeFeatures}shows an example of 16 randomly chosen features. We have for a particular feature $i$ that $C^i_{\alpha\beta} \leftarrow U[-1,1]$. A normalization also occurs. As expected, the filters do not show specific structure. However, they can be seen as selecting for sinusoids components of the inputs corresponding to the highest amplitude spectral component of the filter \cite{random-weights-feature-learning}.

A feature set can be, of course, crafted by hand. Figure \ref{fig:RandomAndHandmadeFeatures}\subref{fig:HandmadeFeatures} shows an example of 10 designed features, similar to Haar Wavelets. Needless to say, this is not a scalable strategy for the common case of thousands of features currently employed in state of the art methods. A well known set can be emplyed then. Figure \ref{fig:DCTFeatures} shows several DCT features, while (figure here) shows several Gabor Wavelet features. The former are more computationally easy to work with while the latter allow overcomplete representations, thanks to their parametrization.

The focus of this chapter is, however, on learned feature sets. We select as a training set for this purpose a set of $100000$ patches of size $d = \hctimes{11}{11}$ randomly extracted from the MNIST and SmallNORB datasets, respectively. A sample of these can be seen in image (cite here). Two pre-processing steps are applied. First, from each image the DC component is subtracted. Second, the mean of all the patches is extracted from each patch, such that $E[final_patch] = \textbf{O}_{\hctimes{11}{11}}$. The final training set can be seen in figure (fig here). See the experiments section for how the patches were extracted. Another dataset will be employed as well, for didactic purposes. It can be seen in Figure \ref{fig:ThreeComponentPointCloud}. It consists of three slightly overlapping subsets generated from a different gaussian each. The set has a relatively complicated topology and is useful for illustrating the differences in learning algorithms.

\begin{figure}
\centering
\includegraphics[width=0.8\textwidth]{thesis_data/three_component_point_cloud.png}
\caption{The three component dataset.}
\label{fig:ThreeComponentPointColud}
\end{figure}

The simplest learning method is PCA. It finds the directions of maximum variance in the dataset. The results of applying PCA to learn a feature set can be seen in Figure (alal). PCA has several problems however. First, it cannot ``learn'' more than $d$ features. Second, it assumes an unrealistic model for image data: a Gaussian point cloud. The distribution of natural images is assumed to lie on a lower dimensional manifold embedded in the $d$-dimensional space and the Gaussian approximation is very crude. Comparing the features from PCA and Gabor Wavelets we can see clear distinctions. The latter perform better.

Maybe talk about ICA a little bit here.

There are at least two other general directions of learnings: autoencoders (with sparse and noisy variants) and Restricted Boltzmann Machines. We will not cover these here.

The main impulse for searching for a better algorithm came from the work of Olshausen and Field \cite{emergence-sparse-coding,sparse-coding-strategy-V1}, who, building on the work of \cite{macaque-cortex}, which showed that the receptive fields of cells in the V1 area of the brain, responsible for early visual processing, had response properties similar to bandpass selective and oriented filters, not unlike certain Wavelet families, devised an algorithm for learning such filters / feature sets and replicating the behaviour of the brain. Their central insight was that sparseness was the requried constraint on the coding phase.

[[[Show mathematical derivation of SparseNet algorithm]]]

Figure (there) shows examples of bases learned from these. The Zero Component Analysis transform was applied to the dataset before learning to decorelate the input components and speed up convergence.

Generalizing this, we can perform gradient descent or stochastic gradient descent with any coding method, in this double loop. For example, bases learned with inner product coding (bad) and MP, OMP and OOMP coding can be seen in figure (...).

[[[Other methods in this direction are MOD and K-SVD and others (build reference here)]]]

An alternative I've worked with and slightly improved (in the sense of generalizing it) is the Sparse Coding Neural Gas of \cite{sparse-coding-neural-gas-1,sparse-coding-neural-gas-2,sparse-coding-neural-gas-3, sparse-coding-neural-gas-4}. This is adaptation of the Neural Gas \cite{neural-gas-1,neural-gas-2} algorithm introduced in the context of vector quantization. Vector quantization can be considered as a stricter version of feature learning, where the codes are $1$-sparse and only a boolean ``indictator'' of the feature most similar to the input $\textbf{x}$, as measured by the Euclidean distance, is stored.

The Neural Gas algorithm is an iterative one. It begins by initializing $\textbf{C}$ to $w$ random observations from the training sample $\textbf{X}$. Then, for a number of $T_{max}$ iterations, an adaptation process takes place, which slowly changes $\textbf{C}$ in order to best represent the distribution over the input space. More precisely, at each iteration $t$ an observation is randomly selected from $\textbf{X}$ and distances to each element of $\textbf{C}$ are computed. Each feature is then modified in a manner proportional to the distorsion between it and the signal $\textbf{x}$, on the one hand, and the ranking of this distorsion in the list of all distorsions, on the other hand. Therefore, the update process includes a local and a global component. \textbf{Algorithm \ref{algo:NeuralGas}} gives the whole picture. Note that both a time decreasing learning factor is used as well as a time decreasing neighbourhood control. 

\begin{algorithm}
\caption{Neural Gas}
\label{algo:NeuralGas}
\begin{algorithmic}
\Require $\textbf{X},w,T_{max},\mu^0,\mu^{T_{max}},\lambda^0,\lambda^{T_{max}}$
\Ensure $\textbf{C}$
\State $\textbf{C} \gets \mbox{randomly select $w$ observations from $\textbf{X}$}$
\For {$t = \hcrange{1}{T_{max}}$}
\State $\mu^t \gets \mu^0 (\mu^{T_max} / \mu^0)^{t / T_{max}}$ \Comment {Current learning rate}
\State $\lambda^t \gets \lambda^0 (\lambda^{T_{max}} / \lambda^0)^{t / T_{max}}$ \Comment {Current neighbourhood control}
\State $\textbf{x} \gets \text{an observation from $\textbf{X}$}$
\State $\textbf{a} \gets [ ~ \|\textbf{x} - \textbf{C}^i\|_2^2 ~ \mbox{\textbf{for} $i \in \hcrange{1}{w}$} ~ ]$
\State $\textbf{C} \gets \textbf{C} + [ ~ \mu^t e^{-rank_{\textbf{a}}(a_i) / \lambda^t} (\textbf{x} - \textbf{C}^i) ~ \mbox{\textbf{for} $i \in \hcrange{1}{w}$} ~ ]$
\EndFor
\end{algorithmic}
\end{algorithm}

The Neural Gas algorithm works in the input space rather than feature set space. Adapting the algorithm to work with features and accept any coding method gives rise to a first version of the Sparse Coding Neural Gas. The major modification is the fact that each update is done according to Oja's Rule \cite{oja-rule} instead of the simple error term of the Neural Gas. The full algorithm is described in \textbf{Algorithm \ref{algo:SparseCodingNeuralGasV1}}. Notice that the $rank_{\textbf{a}}$ function considers absolute values, so that features are updated proportional to the magnitude of the associated response.

\begin{algorithm}
\caption{Sparse Coding Neural Gas V1}
\label{algo:SparseCodingNeuralGasV1}
\begin{algorithmic}
\Require $\textbf{X},w,\mathcal{C},T_{max},\lambda^0,\lambda^{T_{max}},\mu^0,\mu^{T_{max}}$
\Ensure $\textbf{C}$
\State $\textbf{C} \gets \mbox{randomly initialize $w$ normalized features}$
\For {$t = \hcrange{1}{T_{max}}$}
\State $\mu^t \gets \mu^0 (\mu^{T_{max}} / \mu^0)^{t / T_{max}}$  \Comment {Current learning rate}
\State $\lambda^t \gets \lambda^0 (\lambda^{T_{max}} / \lambda^0)^{t / T_{max}}$ \Comment {Current neighbourhood control}
\State $\textbf{x} \gets \text{an observation from $\textbf{X}$}$
\State $\textbf{a} \gets \mathcal{C}_{\textbf{C}}\{ \textbf{x} \}$
\State $\textbf{C} \gets \textbf{C} + [ ~ \mu^t e^{-rank_{\textbf{a}}(a_i) / \lambda^t} a_i (\textbf{x} - a_i \textbf{C}^i) ~ \mbox{\textbf{for} $i \in \hcrange{1}{w}$} ~ ]$
\State $\textbf{C} \gets \mbox{normalize each feature in $\textbf{C}$}$
\EndFor
\end{algorithmic}
\end{algorithm}

A further improvement is possible considering the fact that many coding methods are iterative and produce orderings of a subset $\hcrange{1}{w} \setminus \Lambda^t$ of the feature elements at each iteration. \textbf{MP} and \textbf{OMP} are such methods. A second version of the Sparse Coding Neural Gas is presented as \textbf{Algorithm \ref{algo:SparseCodingNeuralGasV2}}. Notice that at each iteration only the subset of previously unselected features is updated, instead of the whole set. Also, the variable $S^i$, which is a substitute for all the abstracted coding method specific information, must contain a copy of the original feature set $\textbf{C}$ at iteration $t$, before the inner-loop coding procedure. The reason for this is that $\textbf{C}$ is updated in the inner-loop and it can cause problems for the coder to change the features as time progresses.

\begin{algorithm}
\caption{Sparse Coding Neural Gas V2}
\label{algo:SparseCodingNeuralGasV2}
\begin{algorithmic}
\Require $\textbf{X},w,\mathcal{C},T_{max},\lambda^0,\lambda^{T_{max}},\mu^0,\mu^{T_{max}}$
\Ensure $\textbf{C}$
\State $C \gets \mbox{randomly initialize $w$ normalized features}$
\For {$t = \hcrange{1}{T_{max}}$}
\State $\mu^t \gets \mu^0 (\mu^{T_{max}} / \mu^0)^{t / T_{max}}$  \Comment {Current learning rate}
\State $\lambda^t \gets \lambda^0 (\lambda^{T_{max}} / \lambda^0)^{t / T_{max}}$ \Comment {Current neighbourhood control}
\State $\textbf{x} \gets \text{an observation from $\textbf{X}$}$
\State $S^0 \gets \text{initialize coding method specific state}$
\For {$i = \hcrange{0}{k}$}
\State $[\alpha^i~\Lambda^i~S^{i+1}] \gets \mathcal{C}_{\textbf{C}} \{S^i,\textbf{x}\}$ \Comment {$\alpha^i$ stores similarities for features in $\hcrange{1}{w} \setminus \Lambda^i$}
\State $\textbf{C} \gets \textbf{C} + [ ~ \mu^t e^{-rank_{\alpha^i}(\alpha_j^i) / \lambda^t} \alpha_j^i (\textbf{x} - \alpha_j^i \textbf{C}^j) ~ \mbox{\textbf{for} $j \in \hcrange{1}{w}\setminus\Lambda^i$} ~ ]$
\State $\textbf{C} \gets \mbox{normalize each feature in $\textbf{C}$}$
\EndFor
\EndFor
\end{algorithmic}
\end{algorithm}

\chapter{Experiments}
\label{chap:Experiments}

The following suite of experiments are meant to illustrate the strengths and weaknesses of the different coding methods, feature learning algorithms and recoding architectures. We will make reference to two datasets in this section. The first one is the MNIST handwritten digit dataset, which is available from \cite{mnist-website} and is described in more detail in \cite{gradient-based-learning}. Briefly, it consists of $60000$ training and $10000$ test images of size $\hctimes{28}{28}$. Each image represents a single arabic digit, placed such that the center of mass of the non-zero pixels corresponds to the center of the image. A small sample of these images can be seen in \textbf{Figure \ref{fig:ViewsOfMNISTAndNORBSmall}\subref{fig:MNISTExample}}. The second one is the NORB object recognition dataset, or, more specifically, the SmallNORB variant. This set is available from \cite{norbsmall-website} and is described in more detail in \cite{learning-methods-invariance-pose-lighting}. Briefly, it consists of $23400$ training and $23400$ test stereo images of size $\hctimes{32}{32}$. Each image represents one shot of a plastic toy, using a certain camera rotation and elevation. There are five categories of toys: human, animal, plane, truck, car. A small sample of these images can be seen in \textbf{Figure \ref{fig:ViewsOfMNISTAndNORBSmall}\subref{fig:NORBSmallExample}}. We discard the right channel and use only the left one.

Most of the experiments will use patches extracted from one or the other of the two datasets. For MNIST, these patches will have size $\hctimes{9}{9}$, while for SmallNORB, they will have size $\hctimes{13}{13}$. To extract one patch, we choose one image at random from the training dataset. Then, we choose one pixel from this image, again, at random, but far enough from the borders. Finally, if the patch centered at this pixel has a variance greater then a treshhold, $\sigma^2_\tau$, we keep it, otherwise we repeat the whole procedure. For our experiments, we used a threshold of $\sigma^2_\tau = 0.01$. The reason for variance filtering is to exclude the constant or almost-constant patches which provide no useful information for the feature learning algorithms \cite{simple-method-sparse-coding,tiny-images}, but which take up computational resources. Naturally, to extract $N$ patches, the procedure is repeated $N$ times. A small sample of MNIST patches can be seen in \textbf{Figure \ref{fig:ViewsOfMNISTAndNORBSmall}\subref{fig:MNISTPatches}}. A similar sample of SmallNORB patches can be seen in \textbf{Figure \ref{fig:ViewsOfMNISTAndNORBSmall}\subref{fig:NORBSmallPatches}}.

All patches also have their DC component substracted. It is common practice in the literature \cite{tiny-images} to perform this pre-processing step, mainly because it leads to improved results in classification. Intuitively, the reason for doing this is that, when coding, resources are not spent, to a certain extent, trying to match the actual ``magnitude'' of a signal, but rather just the ``features'' in it.

We further transform only those patches which will be used as training data for the feature learning algorithms by applying the Zero Component Analysis (\textbf{ZCA}) transform \cite{tiny-images}, which is a close relative of \textbf{PCA} and which follows the projection of a point into the principal component space with a standardization. The covariance matrix in the new space is therefore $\mathbb{I}_d$. The effect is that of removing linear dependencies between the pixels of a patch. Using this transform speeds up convergence for the feature learning methods. It is not always needed, however, as features for simpler datasets, like MNIST, can be learned without it, although at a slower pace than when using it. A sample of MNIST \textbf{ZCA} transformed patches can be seen in \textbf{Figure \ref{fig:ViewsOfMNISTAndNORBSmall}\subref{fig:MNISTPatchesZCA}}, while a similar sample from SmallNORB can be seen in \textbf{Figure \ref{fig:ViewsOfMNISTAndNORBSmall}\subref{fig:NORBSmallPatchesZCA}}.

\begin{figure}
\centering
\subfigure[A subsample from the MNIST dataset]{
  \includegraphics[width=0.40\textwidth]{thesis_data/mnist_example.png}
  \label{fig:MNISTExample}
}
\subfigure[A subsample from the SmallNORB dataset]{
  \includegraphics[width=0.40\textwidth]{thesis_data/norbsmall_example.png}
  \label{fig:NORBSmallExample}
}
\subfigure[MNIST patches]{
  \includegraphics[width=0.40\textwidth]{thesis_data/mnist_patches.png}
  \label{fig:MNISTPatches}
}
\subfigure[SmallNORB patches]{
  \includegraphics[width=0.40\textwidth]{thesis_data/norbsmall_patches.png}
  \label{fig:NORBSmallPatches}
}
\subfigure[MNIST patches with ZCA applied]{
  \includegraphics[width=0.40\textwidth]{thesis_data/mnist_patches_zca.png}
  \label{fig:MNISTPatchesZCA}
}
\subfigure[SmallNORB patches widt ZCA applied]{
  \includegraphics[width=0.40\textwidth]{thesis_data/norbsmall_patches_zca.png}
  \label{fig:NORBSmallPatchesZCA}
}
\caption{Subsamples and patches from MNIST and SmallNORB. The \textbf{ZCA} transformed patches in row three correspond to the original patches in row two.}
\label{fig:ViewsOfMNISTAndNORBSmall}
\end{figure}

\clearpage

\section{Coding, Reconstruction and Sparseness}
\label{sec:CodingReconstructionSparseness}

In this section we will talk again about coding methods, and compare their reconstructive and sparseness inducing powers. A set of $1000$ patches from the SmallNORB dataset will be used for evaluating the different methods. We will work with five feature sets, which we assume are given: a set of $w=169$ random features, a set of $w=169$ \textbf{PCA} features, a set of $w=169$ sparse features, anoter set of $w=512$ random features and another set of $w=512$ sparse features. The first three sets are critically-complete while the last two are roughly three times overcomplete. No overcomplete \textbf{PCA} feature set could, of course, be built. The \textbf{PCA} and sparse $w=169$ sets can be seen in \textbf{Figure \ref{fig:RecSparseFeatureSets}}. The larger sparse feature set is similar to the one shown. The coding methods we are interested in are inner product (\textbf{Corr}), \textbf{MP}, \textbf{OMP}, \textbf{OOMP} and SparseNet with $S(x) = \log(1 + x^2)$.

\begin{figure}
\centering
\subfigure[PCA feature set with $w=169$.]{
  \includegraphics[width=0.45\textwidth]{thesis_data/recsparse/features_pca.png}
  \label{fig:RecSparseFeaturesPCA}
}
\subfigure[Sparse feature set with $w=169$.]{
  \includegraphics[width=0.45\textwidth]{thesis_data/recsparse/features_sparse.png}
  \label{fig:RecSparseFeaturesSparse}
}
\caption{Two of the three non-random feature sets employed in this section's experiments.}
\label{fig:RecSparseFeatureSets}
\end{figure}

In order to test the reconstruction power of the five coding methods we will measure the average squared sum of differences between an original patch $\textbf{x}$ and a reconstructed patch $\textbf{C}\textbf{a}$. That is, the average Euclidean norm of the error patch $\textbf{x} - \textbf{C}\textbf{a}$. We use the conventions estabilished in the chapter about coding methods. More precisely, $\textbf{C}$ is one of the five feature sets while $\textbf{a}$ is the code word obtained by applying one of the five coding methods we're testing to $\textbf{x}$ and $\textbf{C}$.

For this experiment we will constrain the reconstructive powers of the methods by imposing at most $k$ non-null coefficients. For \textbf{Corr} and SparseNet, the method will be allowed to run as usual, then the $k$ greatest coefficients in absolute value will be selected. \textbf{MP}, \textbf{OMP} and \textbf{OOMP} have $k$ builtin, therefore they suffer no modification. We will have five different values for $k$, namely, $1$, $3$, $16$, $64$ and $w$. When $k=1$ we will have a very strict coding setup. Basically, for a given $\textbf{x}$, $\textbf{a}$ will be zero everywhere except at the index $\omega$ corresponding to the most similar feature from $\textbf{C}$ to $\textbf{x}$. The similarity will be measured by inner-product / projection size of $\textbf{x}$ onto $\textbf{C}^\omega$. We expect \textbf{Corr}, \textbf{MP}, \textbf{OMP} and \textbf{OOMP} to produce the same results, as, for $k=1$, algorithmically speaking, there are no differences between the methods - even the least squares problems \textbf{OMP} and \textbf{OOMP} solve are reduced to inner product comparisons in such a limit case. When $k=3$ we will again have a very strict coding setup, though not as harsh as for $k=1$. We expect large errors in these two situations. For $k=16$ and $k=64$, differences between the methods will start to become apparent and we expect small errors. For $k=w$ we should see in a lot of methods and under a lot of feature sets the error reduced to $0$. For \textbf{OMP} and \textbf{OOMP}, in this case we'll have $k=169$, because, by their very construction, these methods guarantee a $0$ norm residual after $k=d$ steps, therefore every coefficient of $a$ for $t \ge d$ will necessarily be $0$. There is no point in wasting computational resources computing them.

\renewcommand{\arraystretch}{1.2}
\begin{table}
  \caption{Average reconstruction error for different coding methods and different feature sets.}
  \label{table:RecSparseErrors}
  \newcolumntype{R}{>{\centering}X}
  \begin{tabularx}{\textwidth}{|R|R|c|c|c|c|c|}
    \hline
    Features & Method & $k=1$ & $k=3$ & $k=16$ & $k=64$ & $k=w$ \tabularnewline \hline\hline
    \multirow{5}{*}{Random $w=169$} & \textbf{Corr} & $1.95$  & $1.88$ & $1.72$ & $1.81$ & $1.93$ \tabularnewline
    & \textbf{MP} & $1.95$ & $1.87$ & $1.57$ & $1.14$ & $0.87$ \tabularnewline
    & \textbf{OMP} & $1.95$ & $1.87$ & $1.55$ & $0.93$ & $0.00$ \tabularnewline
    & \textbf{OOMP} & $1.95$ & $1.87$ & $1.55$ & $0.93$ & $0.00$ \tabularnewline
    & SparseNet & $1.97$ & $1.93$ & $1.79$ & $1.58$ & $1.50$ \tabularnewline \hline
    \multirow{5}{*}{\textbf{PCA} $w=169$} & \textbf{Corr} & $1.47$ & $1.07$ & $0.55$ & $0.20$ & $0.00$ \tabularnewline
    & \textbf{MP} & $1.47$ & $1.07$ & $0.55$ & $0.20$ & $0.00$ \tabularnewline
    & \textbf{OMP} & $1.47$ & $1.07$ & $0.55$ & $0.20$ & $0.00$ \tabularnewline
    & \textbf{OOMP} & $1.47$ & $1.07$ & $0.55$ & $0.20$ & $0.00$ \tabularnewline
    & SparseNet & $1.58$ & $1.30$ & $1.01$ & $0.91$ & $0.90$ \tabularnewline \hline
    \multirow{5}{*}{Sparse $w=169$} & \textbf{Corr} & $1.76$ & $1.62$ & $1.25$ & $1.03$ & $1.04$ \tabularnewline
    & \textbf{MP} & $1.76$ & $1.61$ & $1.17$ & $0.57$ & $0.16$ \tabularnewline
    & \textbf{OMP} & $1.76$ & $1.61$ & $1.16$ & $0.50$ & $0.00$ \tabularnewline
    & \textbf{OOMP} & $1.76$ & $1.61$ & $1.16$ & $0.50$ & $0.00$ \tabularnewline
    & SparseNet & $1.83$ & $1.74$ & $1.51$ & $1.29$ & $1.24$ \tabularnewline \hline
    \multirow{5}{*}{Random $w=512$} & \textbf{Corr} & $1.93$ & $1.84$ & $1.71$ & $2.59$ & $5.20$ \tabularnewline
    & \textbf{MP} & $1.93$ & $1.83$ & $1.40$ & $0.68$ & $0.01$ \tabularnewline
    & \textbf{OMP} & $1.93$ & $1.83$ & $1.37$ & $0.44$ & $0.00$ \tabularnewline
    & \textbf{OOMP} & $1.93$ & $1.83$ & $1.37$ & $0.43$ & $0.00$ \tabularnewline
    & SparseNet & $1.96$ & $1.93$ & $1.78$ & $1.47$ & $0.89$ \tabularnewline \hline
    \multirow{5}{*}{Sparse $w=512$} & \textbf{Corr} & $1.75$ & $1.62$ & $1.98$ & $3.84$ & $7.02$ \tabularnewline
    & \textbf{MP} & $1.76$ & $1.53$ & $0.99$ & $0.40$ & $0.00$ \tabularnewline
    & \textbf{OMP} & $1.76$ & $1.53$ & $0.95$ & $0.26$ & $0.00$ \tabularnewline
    & \textbf{OOMP} & $1.76$ & $1.53$ & $0.95$ & $0.25$ & $0.00$ \tabularnewline
    & SparseNet & $1.88$ & $1.78$ & $1.56$ & $1.23$ & $0.80$ \tabularnewline \hline
  \end{tabularx}
\end{table}
\renewcommand{\arraystretch}{1.0}

\clearpage

A summary  of the results can be seen in \textbf{Table \ref{table:RecSparseErrors}}. Only the means are presented, however. The variances tended to be the same across methods and feature sets, and were roughly $0.40$ for $k=1$, $k=3$ and $k=16$, $0.10$ for $k=64$ and $0$ for $k=w$, except for SparseNet, which had a $0.10$ variance for this case as well. Several observations need to be made. First, regardless of feature set, reconstruction error goes down from \textbf{Corr} to \textbf{MP} to \textbf{OMP} to \textbf{OOMP}. Furthermore, for \textbf{Corr}, the error increases when  using random features. This breakdown is explained by the nature of the feature set - there is no simple structure to exploit - and the nature of the method - it cannot find ``hidden'' structure. When employing the sparse feature set with $w=169$, which does have some structure to it, the error drops to about half for $k=w$. For the overcomplete sparse set, however, the error radically increases. This is a sign of the severe limitations of the method - even with some structure it cannot produce a suitable approximation when there are more features than dimensions in the signal space. The \textbf{PCA} feature set presents itself in a form which \textbf{Corr} can handle. Therefore, for $k=w$ we have a zero error. More importantly, all methods which are somehow based on inner-products have the same behaviour for this feature set. With regards to inner-product based methods, \textbf{MP} handles itself well. For smaller $k$s it is roughly equivalent to the more complex methods. Better results are obtained in the overcomplete feature sets (both random and sparse) than the critically-complete one. For the overcomplete ones, at $k=512$ a zero error is actually reached. This means the bigger feature set allowed the method a way to find the certain features which allow a perfect reconstruction. The other two methods, \textbf{OMP} and \textbf{OOMP} behave roughly the same. \textbf{OOMP} seems to be a little bit better (by $0.01$ in some cases), but such a small difference would not recommend the method, from a purely approximation power point of view, given its increased compuational demands. As expected, when $k=169$ (the maximum useful for these methods, regardless of the actual $w$), the reconstruction is perfect, regardless of the feature set used. SparseNet does not produce very good approximations. Visual inspection of the result usually hints that there is a scale problem, rather than a shape problem. Also, the overcomplete sparse feature sete is the one which induces best results for this method, although the \textbf{PCA} set is not that far off.

\textbf{Figure \ref{fig:RecSparsePictures}} gives an ideea of the quality of a reconstructed patch, relative to the original. Notice that the patches corresponding to the random bases are more noisy than the other ones and that the patches corresponding to \textbf{PCA} are very similar to the original, regardeless of coding method. Also, the features of the SparseNet patches are barely percievable, even though a closer inspection will reveal the same general shape. This is caused by the previously mentioned amplitude problems.

\begin{figure}
\centering
\includegraphics[width=0.95\textwidth,height=0.8\textheight]{thesis_data/recsparse/rec_pictures.png}
\caption{Reconstructions of the patch in the top-left corner using different coding methods and feature sets, but with fixed $k=64$. Each column presents one coding method. They are in order: \textbf{Corr}, \textbf{MP}, \textbf{OMP}, \textbf{OOMP} and SparseNet. Each row, after the first one, presents one feature set. They are in order: random features with $w=169$, \textbf{PCA} features with $w=169$, sparse features with $w=169$, random features with $w=512$ and sparse features with $w=512$.}
\label{fig:RecSparsePictures}
\end{figure}

Reconstruction power is just one measure of the quality of a coding method, however. Another is, of course, sparseness, as measured by the sparseness coefficient $\rho(\textbf{a})$. As in the last experiment, where we were interested in the average reconstruction error, we are now interested in the average sparseness. We continue with the same experimental setup as before. However, we will not impose a small $k$, but rather use $k=w$ or $k=d$, depending on the particular method. Imposing a $k \ll w$ would have artificially restricted the spareness and we would have gotten useful results only for larger values of $k$, for which the methods would be able to induce their own spareness. We use a $\delta = 0.01$ as the threshold value for a coefficient being close to zero in the computation of the sparseness coefficient.

\renewcommand{\arraystretch}{1.2}
\begin{table}
  \caption{Average sparseness for different coding methods and different feature sets.}
  \label{table:RecSparseSparseness}
  \newcolumntype{R}{>{\centering}X}
  \begin{tabularx}{\textwidth}{|R|c|c|c|c|c|}
    \hline
    Features & \textbf{Corr} & \textbf{MP} & \textbf{OMP} & \textbf{OOMP} & SparseNet \tabularnewline \hline\hline
    Random $w=169$ & $0.48$ & $0.50$ & $0.50$ & $0.50$ & $0.41$ \tabularnewline \hline
    \textbf{PCA} $w=169$ & $0.36$ & $0.36$ & $0.36$ & $0.36$ & $0.38$ \tabularnewline \hline
    Sparse $w=169$ & $0.46$ & $0.50$ & $0.45$ & $0.45$ & $0.42$ \tabularnewline \hline
    Random $w=512$ & $0.47$ & $0.28$ & $0.13$ & $0.13$ & $0.37$ \tabularnewline \hline
    Sparse $w=512$ & $0.47$ & $0.19$ & $0.12$ & $0.12$ & $0.35$ \tabularnewline
    \hline
  \end{tabularx}
\end{table}
\renewcommand{\arraystretch}{1.0}

A summary of the results can be seen in \textbf{Table \ref{table:RecSparseSparseness}}. For the critically-complete random feature set every inner-product bases method performs roughly the same. SparseNet is significantly better, though, in general, all methods perform rather poorly, requiring about half of the coefficients to be greater than $\delta$. With \textbf{PCA} features, we again see the identical behaviour for all inner-product based methods. All methods have roughly the same behaviour, which is visibly better than with the random features, but not that great in general. Similarly, with the critically-complete sparse feature set, about half of the code word coefficients are active, regardless of method. However, \textbf{MP} is a little bit worse than \textbf{Corr} and the other pursuits. It probably can't find good matches in this limited feature set. SparseNet again leads the pack with performance similar to the random feature set case. When we move to the larger feature sets, however, the situation changes drastically. \textbf{Corr} remains at roughly the same level as with the $w=169$ feature sets, and SparseNet is slighly better, but the pursuits come into their own and produce very sparse results. Again, \textbf{OMP} and \textbf{OOMP} produce ``identical'' results, furthering our belief that the second has no practical advantage over the first. \textbf{MP} is roughly half as good as \textbf{OMP} and \textbf{OOMP}, especially in the random feature set, but is much better than simple correlation. Notice that, in absolute terms, in the best cases, roughly $60$ coefficients are needed for a good reconstruction, as $\hctimes{0.50}{128} \approx \hctimes{0.12}{512} \approx 60$. This is useful information for later experiments. Using a $k > 60$ with a $d$ similar to the ones we used in these experiments, will, most likely, not produce better results.

\begin{figure}
\centering
\includegraphics[width=0.95\textwidth,height=0.8\textheight]{thesis_data/recsparse/rec_hists.png}
\caption{Histograms of the code words for the patch in the top-left corner of \textbf{Figure \ref{fig:RecSparsePictures}} produced using different coding methods and feature sets. Each columns presents one coding method. They are in order: \textbf{Corr}, \textbf{MP}, \textbf{OMP}, \textbf{OOMP} and SparseNet. Each row presents one feature set. They are in order: random features with $w=169$, \textbf{PCA} features with $w=169$, sparse features with $w=169$, random features with $w=512$ and sparse features with $w=512$. All horizontal axes are between $[-1.5,1.5]$ and all vertical ones are between $[0,250]$.}
\label{fig:RecSparseHists}
\end{figure}

\textbf{Figure \ref{fig:RecSparseHists}} gives an ideea of the distribution of the patches in the different cases. We used $61$ bins spread evenly in the interval $[-1.5,1.5]$. Notice how spiked the distributions for the pursuits are, expecially for \textbf{OMP} and \textbf{OOMP}. Again, these two produce almost identical results. The distribution for \textbf{Corr} is however very flat. \textbf{MP} is situated somewhat between these two extremes, but with a clear peak around $0$. Similarly, SparseNet has a peak around $0$, but the distribution has a skwed form, resulting from the regularization term we used. The effect is good, although the sparsity is clearly more strinking in the \textbf{OMP} and \textbf{OOMP} methods. The \textbf{PCA} features do not produce very ``sparse'' looking histograms, regardless of the methods employed, despite good reconstruction performance and acceptable average sparseness. Real sparseness is to be found in the way the values are distributed - there must be just a few code word coefficients with large values, besides being a lot of them equal to or near $0$.

\section{Learning Sparse Feature Sets}
\label{sec:LearningSparseFeatureSet}

[[[We have to see about what we'll talk about in this section.]]]

\section{Coder Architecture Evaluation}
\label{sec:CoderArchitectureEvaluation}

In this section we will seek the best coder architectures for the image classification tasks posed by the MNIST and SmallNORB datasets, described in the start of this chapter. We will use the feature extractor model described in \textbf{Chapter \ref{chap:OverviewFeatureExtraction}}, and, as mentioned there, we will employ just one recoder module. Given a certain problem, the task is to pick the recoder parameters in such a way as to maximize the classification accuracy on the test version of the dataset. The recoder parameters are the parameters for each of the four stages. For the coding stage we have the feature size $d = \hctimes{p}{p}$, the feature set $\textbf{C}$ and $w$, the number of allowed non-small coefficients $k$ and the actual coding method $\mathcal{C}_{\textbf{C},k}$. For the nonlinear step we have the nonlinearity type $\mathcal{N}$. For the polarity split stage we have the split type $\mathcal{P}$. Finally, for the reduce step we have the reducer procedure $\mathcal{R}$ and the size of the spread $r$. Instead of $k$ we will use $\hat{\rho} = k/w$. This makes it somewhat easier to compare parameter sets, independent of $w$. Thus, a full parameterization of the recoder is $(p,w,\textbf{C},\hat{\rho},\mathcal{C}_{\textbf{C},k},\mathcal{N},\mathcal{P},\mathcal{R},r)$.

\begin{wrapfigure}{R}{0.25\textwidth}
\begin{center}
\includegraphics[width=0.25\textwidth]{thesis_data/recoderev/original_image.png}
\end{center}
\caption{The original image which is coded in \textbf{Figure \ref{fig:RecoderEvRecodingExamplesCorr}} and \textbf{Figure \ref{fig:RecoderEvRecodingExamplesOOMP}}.}
\label{fig:RecoderEvOriginalImage}
\end{wrapfigure}

Before we describe the experiments performed and the results obtained, \textbf{Figure \ref{fig:RecoderEvRecodingExamplesCorr}} and \textbf{Figure \ref{fig:RecoderEvRecodingExamplesOOMP}} show how the image in \textbf{Figure \ref{fig:RecoderEvOriginalImage}} looks after recoding. For both image sets, a very small feature set of $w=9$ patches of size $\hctimes{9}{9}$ were used, for illustrative purposes. The coded output is a vector of size $\lfloor m / r \rfloor \lfloor n / r \rfloor w$ which can be interpreted as a series of $w$ $\hctimes{\lfloor m / r \rfloor}{\lfloor n / r \rfloor}$ images - a three dimensional tensor. Each image is associated with a single feature and represents the original image coded with regards to that feature. As we mentioned earlier, in the most general case, the coefficients in one image depend on the corresponding coefficients in the other images. In any case, this form is good for visualizing how the different architecture choices influence the coded output. The images are displayed in the odd rows, and as a vector in the even rows. Notice that, in order to go from the tensor representation to the linear one, no transformations need be applied, as, in our implementation, the data is stored in the proper order. Therefore, the first coefficients of the linear form correspond to the first image, the next coefficients correspond to the second image etc. When polarity splitting is used, the positive coefficients appear in the first half of the liner form, while the negative coefficients appear in the second half. The order in these two regions is the same as in the case of no polarity splitting.

\begin{figure}
\centering
\includegraphics[width=0.95\textwidth,height=0.8\textheight]{thesis_data/recoderev/coded_partial_corr.png}
\caption{Different views of the coded image in \textbf{Figure \ref{fig:RecoderEvOriginalImage}}. The coding was done with the \textbf{Corr} coding method. The odd rows present the code word coefficients for each one of th e $w=9$ features, arranged as an image. The even rows present the actual ``linear'' form of the recoded image, which is fed to a classifier, for example. The row pairs are, in order: linear nonlinearity, no polarity split, subsample reduce; linear nonlinearity, no polarity split, maximum with sign kept reduce; linear nonlinearity, no polarity split, sum of squares reduce. See the text for more details.}
\label{fig:RecoderEvRecodingExamplesCorr}
\end{figure}

\begin{figure}
\centering
\includegraphics[width=0.95\textwidth,height=0.8\textheight]{thesis_data/recoderev/coded_partial_oomp.png}
\caption{Different views of the coded image in \textbf{Figure \ref{fig:RecoderEvOriginalImage}}. The coding was done with the \textbf{OOMP} coding method. The odd rows present the code word coefficients for each one of th e $w=9$ features, arranged as an image. The even rows present the actual ``linear'' form of the recoded image, which is fed to a classifier, for example. The row pairs are, in order: logistic nonlinearity, no polarity split, maximum with no sign kept reduce; global order nonlinearity, no polarity split, maximum with sign reduce; linear nonlinearity, polarity split with kept sign, maximum with kept sign reduce. See the text for more details.}
\label{fig:RecoderEvRecodingExamplesOOMP}
\end{figure}

\renewcommand{\arraystretch}{1.5}
\begin{table}
  \caption{Allowed values for the recoder parameters.}
  \label{table:RecoderEvAllowedParams}
  \newcolumntype{R}{>{\centering}X}
  \begin{tabularx}{\textwidth}{|c|R|}
    \hline
    Parameter & Allowed values \tabularnewline \hline\hline
    $p$ & $5,7,9,11,13,15,17$ \tabularnewline \hline
    $w$ & $p^2,3p^2$ and $1024,1536,2048$ \tabularnewline \hline
    $\textbf{C}$ & Random, \textbf{PCA}, Sparse \tabularnewline \hline
    $\hat{\rho} = k / w$ & $0.05,0.10,\dots,0.70,0.75$ \tabularnewline \hline
    $\mathcal{C}_{\textbf{C},k}$ & \textbf{Corr}, \textbf{MP}, \textbf{OMP}, \textbf{OOMP} \tabularnewline \hline
    $\mathcal{N}$ & Linear, Logistic, GlobalOrder \tabularnewline \hline
    $\mathcal{P}$ & None, NoSign, KeepSign \tabularnewline \hline
    $\mathcal{R}$ & Subsample, MaxNoSign, MaxKeepSign, SumAbs, SumSqr \tabularnewline \hline
    $r$ & $2,3,4,6,8,10,14,20$ \tabularnewline
    \hline
  \end{tabularx}
\end{table}
\renewcommand{\arraystretch}{1.0}

Returning to our experiment, we can see in \textbf{Table \ref{table:RecoderEvAllowedParams}} a list of possible values for all the different parameters associated with a recoder, usable by both datasets. An exhaustive search of the full parameter space would require evaluating almost $5$ million different parameter combinations - clearly a very hard task! The only way forward is to manage this complexity. One approach for doing this, is to split the problem into several subproblems which are ``independent'' enough. The best configuration is then formed by combining the answers to the subproblems. One thing we'll need is an initial ``good-enough'' parameter set. \textbf{Table \ref{table:RecoderEvInitialConfigs}} presents not one, but two such configurations, dubbed $\Theta_1^0$,$\Theta_2^0$ and $\Theta_3^0$, which have been obtained through trial and error and eariler undocumented experiments. The subproblems will try to improve upon these three initial configurations.

\renewcommand{\arraystretch}{1.2}
\begin{table}
  \caption{Initial ``good'' configurations.}
  \label{table:RecoderEvInitialConfigs}
  \newcolumntype{R}{>{\centering}X}
  \begin{tabularx}{\textwidth}{|R|c|c|c|c|c|c|c|c|c|c|c|}
    \hline
     & $p$ & $w$ & $\textbf{C}$ & $\hat{\rho}$ & $\mathcal{C}_{\textbf{C},k}$ & $\mathcal{N}$ & $\mathcal{P}$ & $\mathcal{R}$ & $r$ & Score & $\mathbb{E}[\rho]$ \tabularnewline\hline\hline
    $\Theta_1^0$ & $11$ & $363$ & Sparse & $0.10$ & \textbf{MP} & Logistic & None & SumSqr & $4$ & 98.80 & 0.34 \tabularnewline\hline
    $\Theta_2^0$ & $11$ & $363$ & Random & $0.10$ & \textbf{Corr} & GlobalOrder & None & SumSqr & $4$ & 99.06 & 0.41 \tabularnewline\hline
    $\Theta_3^0$ & $11$ & $363$ & Sparse & $0.10$ & \textbf{OMP} & Logistic & KeepSign & SumSqr & $4$ & 99.01 & 0.23 \tabularnewline
    \hline
  \end{tabularx}
\end{table}
\renewcommand{\arraystretch}{1.0}

We split the parameter set into three groups: feature set parameters ($p$,$w$ and $\textbf{C}$), coding method parameters ($\hat{\rho}$ and $\mathcal{C}_{\textbf{C},k}$) and aftercoding stage parameters ($\mathcal{N}$, $\mathcal{P}$, $\mathcal{R}$ and $r$). The subproblems will then handle each of these three parameter sets sperately. More precisely, the following procedure will be followed. Every combination of feature set parameters from \textbf{Table \ref{table:RecoderEvAllowedParams}} is used, except the $w$s which are over $1000$. More precisely, for every $p$ tested we use a critically-complete feature set and a double overcomplete feature set. We do not use feature sets of a higher count because of computational concerns. There are nine distinct values for $p$, two values for $w$ and three values for \textbf{C}. Therefore we can form $90$ different parameter combinations. For each test, the other parameters come from $\Theta_1^0$, $\Theta_2^0$ and $\Theta_3^0$ respectively. Therefore, we will test a total of $270$ full parameter combinations. After this first experiment, we select the three best full parameter configurations and name them $\Theta_1^1$, $\Theta_2^1$ and $\Theta_3^1$. We then continue with the second experiment, which looks at every combination of coding method parameters from \textbf{Table \ref{table:RecoderEvAllowedParams}}. There are twenty distinct values for $\hat{\rho}$ and three distinct values for $\mathcal{C}_{\textbf{C},k}$. Therefore we can form $60$ different parameter combinations. For each test, the other parameters come from $\Theta_1^1$, $\Theta_2^1$ and $\Theta_3^1$ respectively. Therefore, we will test a total of $180$ full parameter combinations. After this second experiment, we select the three best full parameter configurations and name them $\Theta_1^2$, $\Theta_2^2$ and $\Theta_3^2$. We then continue with the third experiment, which looks at every combination of parameters for all the stages after the coding stage. There are three distinct values for $\mathcal{N}$, three distinct values for $\mathcal{R}$, five distinct values for $\mathcal{R}$ and eight distinct values for $r$. Therefore, we can form $360$ different parameter combinations. For each test, the other parameters come from $\Theta_1^2$, $\Theta_2^2$ and $\Theta_3^2$ respectively. Therefore we will test a total of $1080$ full parameter combinations. After this third experiment, we select the three best full parameter configurations and name them $\Theta_1^3$, $\Theta_2^3$ and $\Theta_3^3$. As a final experiment we will look at how these three configurations perform with very high $w$ and varying $\hat{\rho}$.

\renewcommand{\arraystretch}{1.2}
\begin{table}
  \caption{Summarized experiment results for MNIST.}
  \label{table:RecoderEvMNISTResultsSummarized}
  \newcolumntype{R}{>{\centering}X}
  \begin{tabularx}{\textwidth}{|R|c|c|c|c|c|c|c|c|c|c|c|}
    \hline
    & $p$ & $w$ & $\textbf{C}$ & $\hat{\rho}$ & $\mathcal{C}_{\textbf{C},k}$ & $\mathcal{N}$ & $\mathcal{P}$ & $\mathcal{R}$ & $r$ & Score & $\mathbb{E}[\rho]$ \tabularnewline\hline\hline
    $\Theta_1^1$ & $11$ & $256$ & Sparse & $0.10$ & \textbf{MP} & Logistic & None & SumSqr & $4$ & 98.50 & 0.11 \tabularnewline\hline
    $\Theta_2^1$ & $11$ & $256$ & Random & $0.10$ & \textbf{Corr} & GlobalOrder & None & SumSqr & $4$ & 98.50 & 0.11 \tabularnewline\hline
    $\Theta_3^1$ & $11$ & $256$ & Random & $0.10$ & \textbf{Corr} & GlobalOrder & None & SumSqr & $4$ & 98.50 & 0.11 \tabularnewline\hline\hline
    $\Theta_1^2$ & $11$ & $256$ & Sparse & $0.10$ & \textbf{MP} & Logistic & None & SumSqr & $4$ & 98.50 & 0.11 \tabularnewline\hline
    $\Theta_2^2$ & $11$ & $256$ & Random & $0.10$ & \textbf{Corr} & GlobalOrder & None & SumSqr & $4$ & 98.50 & 0.11 \tabularnewline\hline
    $\Theta_3^2$ & $11$ & $256$ & Random & $0.10$ & \textbf{Corr} & GlobalOrder & None & SumSqr & $4$ & 98.50 & 0.11 \tabularnewline\hline\hline
    $\Theta_1^3$ & $11$ & $256$ & Sparse & $0.10$ & \textbf{MP} & Logistic & None & SumSqr & $4$ & 98.50 & 0.11 \tabularnewline\hline
    $\Theta_2^3$ & $11$ & $256$ & Random & $0.10$ & \textbf{Corr} & GlobalOrder & None & SumSqr & $4$ & 98.50 & 0.11 \tabularnewline\hline
    $\Theta_3^3$ & $11$ & $256$ & Random & $0.10$ & \textbf{Corr} & GlobalOrder & None & SumSqr & $4$ & 98.50 & 0.11 \tabularnewline
    \hline
  \end{tabularx}
\end{table}
\renewcommand{\arraystretch}{1.0}

\renewcommand{\arraystretch}{1.2}
\begin{table}
  \caption{Summarized experiment results for SmallNORB.}
  \label{table:RecoderEvNORBSmallResultsSummarized}
  \newcolumntype{R}{>{\centering}X}
  \begin{tabularx}{\textwidth}{|R|c|c|c|c|c|c|c|c|c|c|c|}
    \hline
    & $p$ & $w$ & $\textbf{C}$ & $\hat{\rho}$ & $\mathcal{C}_{\textbf{C},k}$ & $\mathcal{N}$ & $\mathcal{P}$ & $\mathcal{R}$ & $r$ & Score & $\mathbb{E}[\rho]$ \tabularnewline\hline\hline
    $\Theta_1^1$ & $11$ & $256$ & Sparse & $0.10$ & \textbf{MP} & Logistic & None & SumSqr & $4$ & 98.50 & 0.11 \tabularnewline\hline
    $\Theta_2^1$ & $11$ & $256$ & Random & $0.10$ & \textbf{Corr} & GlobalOrder & None & SumSqr & $4$ & 98.50 & 0.11 \tabularnewline\hline
    $\Theta_3^1$ & $11$ & $256$ & Random & $0.10$ & \textbf{Corr} & GlobalOrder & None & SumSqr & $4$ & 98.50 & 0.11 \tabularnewline\hline\hline
    $\Theta_1^2$ & $11$ & $256$ & Sparse & $0.10$ & \textbf{MP} & Logistic & None & SumSqr & $4$ & 98.50 & 0.11 \tabularnewline\hline
    $\Theta_2^2$ & $11$ & $256$ & Random & $0.10$ & \textbf{Corr} & GlobalOrder & None & SumSqr & $4$ & 98.50 & 0.11 \tabularnewline\hline
    $\Theta_3^2$ & $11$ & $256$ & Random & $0.10$ & \textbf{Corr} & GlobalOrder & None & SumSqr & $4$ & 98.50 & 0.11 \tabularnewline\hline\hline
    $\Theta_1^3$ & $11$ & $256$ & Sparse & $0.10$ & \textbf{MP} & Logistic & None & SumSqr & $4$ & 98.50 & 0.11 \tabularnewline\hline
    $\Theta_2^3$ & $11$ & $256$ & Random & $0.10$ & \textbf{Corr} & GlobalOrder & None & SumSqr & $4$ & 98.50 & 0.11 \tabularnewline\hline
    $\Theta_3^3$ & $11$ & $256$ & Random & $0.10$ & \textbf{Corr} & GlobalOrder & None & SumSqr & $4$ & 98.50 & 0.11 \tabularnewline
    \hline
  \end{tabularx}
\end{table}
\renewcommand{\arraystretch}{1.0}

\renewcommand{\arraystretch}{1.2}
\begin{table}
  \caption{Classification scores for first experiments on MNIST.}
  \label{table:RecoderEvMNISTResultsPWCScore}
  \newcolumntype{R}{>{\centering}X}
  \begin{tabularx}{\textwidth}{cRR|c|c|c|c|c|c|c|}
    \cline{4-10}
    & & & \multicolumn{7}{c|}{$p$} \tabularnewline \cline{4-10}
    & & & $5$ & $7$ & $9$ & $11$ & $13$ & $15$ & $17$ \tabularnewline \hline
    \multicolumn{1}{|c}{\multirow{5}{*}{$\textbf{C}$}} & \multicolumn{1}{|R}{\multirow{2}{*}{Random}} & \multicolumn{1}{|R|}{$w=p^2$} & $98.30^{(1)}$ & $98.47^{(1)}$ & $98.62^{(1)}$ & $98.77^{(1)}$ & $98.68^{(1)}$ & $98.76^{(1)}$ & $98.56^{(1)}$ \tabularnewline
    \multicolumn{1}{|c}{} & \multicolumn{1}{|R}{} & \multicolumn{1}{|R|}{$w=3p^2$} & 98.50 & 98.50 & 98.50 & 98.50 & 98.50 & 98.50 & 98.50 \tabularnewline
    \multicolumn{1}{|c}{} & \multicolumn{1}{|R}{\textbf{PCA}} & \multicolumn{1}{|R|}{$w=p^2$} & 98.50 & 98.50 & 98.50 & 98.50 & 98.50 & 98.50 & 98.50 \tabularnewline
    \multicolumn{1}{|c}{} & \multicolumn{1}{|R}{\multirow{2}{*}{Sparse}} & \multicolumn{1}{|R|}{$w=p^2$} & 98.50 & 98.50 & 98.50 & 98.50 & 98.50 & 98.50 & 98.50 \tabularnewline
    \multicolumn{1}{|c}{} & \multicolumn{1}{|R}{} & \multicolumn{1}{|R|}{$w=3p^2$} & 98.50 & 98.50 & 98.50 & 98.50 & 98.50 & 98.50 & 98.50 \tabularnewline\hline
  \end{tabularx}
\end{table}
\renewcommand{\arraystretch}{1.0}

\renewcommand{\arraystretch}{1.2}
\begin{table}
  \caption{Classification scores for second experiment on MNIST.}
  \label{table:RecoderEvMNISTResultsRhoCCkScore}
  \newcolumntype{R}{>{\centering}X}
  \begin{tabularx}{\textwidth}{ccc|R|R|R|R|R|}
    \cline{4-8}
    & & & \multicolumn{5}{c|}{$\hat{\delta}$} \tabularnewline\cline{4-8}
    & & & $0.00$ & $0.05$ & $0.10$ & $0.15$ & $0.20$ \tabularnewline\hline
    \multicolumn{1}{|c|}{\multirow{12}{*}{$\mathcal{C}_{\textbf{C},k}$}} & \multirow{3}{*}{\textbf{Corr}} & \multicolumn{1}{|c|}{$\hat{\rho}=0.05+\hat{\delta}$} & $98.50^1$ & $98.50^1$ & $98.50^1$ & $98.50^1$ & $98.50^1$ \tabularnewline
    \multicolumn{1}{|c|}{} & & \multicolumn{1}{|c|}{$\hat{\rho} = 0.25 + \hat{\delta}$} & $98.50^1$ & $98.50^1$ & $98.50^1$ & $98.50^1$ & $98.50^1$ \tabularnewline
    \multicolumn{1}{|c|}{} & & \multicolumn{1}{|c|}{$\hat{\rho} = 0.50 + \hat{\delta}$} & $98.50^1$ & $98.50^1$ & $98.50^1$ & $98.50^1$ & $98.50^1$ \tabularnewline
    \multicolumn{1}{|c|}{} & \multirow{3}{*}{\textbf{MP}} & \multicolumn{1}{|c|}{$\hat{\rho}=0.05+\hat{\delta}$} & $98.50^1$ & $98.50^1$ & $98.50^1$ & $98.50^1$ & $98.50^1$ \tabularnewline
    \multicolumn{1}{|c|}{} & & \multicolumn{1}{|c|}{$\hat{\rho} = 0.25 + \hat{\delta}$} & $98.50^1$ & $98.50^1$ & $98.50^1$ & $98.50^1$ & $98.50^1$ \tabularnewline
    \multicolumn{1}{|c|}{} & & \multicolumn{1}{|c|}{$\hat{\rho} = 0.50 + \hat{\delta}$} & $98.50^1$ & $98.50^1$ & $98.50^1$ & $98.50^1$ & $98.50^1$ \tabularnewline
    \multicolumn{1}{|c|}{} & \multirow{3}{*}{\textbf{OMP}} & \multicolumn{1}{|c|}{$\hat{\rho}=0.05+\hat{\delta}$} & $98.50^1$ & $98.50^1$ & $98.50^1$ & $98.50^1$ & $98.50^1$ \tabularnewline
    \multicolumn{1}{|c|}{} & & \multicolumn{1}{|c|}{$\hat{\rho} = 0.25 + \hat{\delta}$} & $98.50^1$ & $98.50^1$ & $98.50^1$ & $98.50^1$ & $98.50^1$ \tabularnewline
    \multicolumn{1}{|c|}{} & & \multicolumn{1}{|c|}{$\hat{\rho} = 0.50 + \hat{\delta}$} & $98.50^1$ & $98.50^1$ & $98.50^1$ & $98.50^1$ & $98.50^1$ \tabularnewline
    \multicolumn{1}{|c|}{} & \multirow{3}{*}{\textbf{OOMP}} & \multicolumn{1}{|c|}{$\hat{\rho}=0.05+\hat{\delta}$} & $98.50^1$ & $98.50^1$ & $98.50^1$ & $98.50^1$ & $98.50^1$ \tabularnewline
    \multicolumn{1}{|c|}{} & & \multicolumn{1}{|c|}{$\hat{\rho} = 0.25 + \hat{\delta}$} & $98.50^1$ & $98.50^1$ & $98.50^1$ & $98.50^1$ & $98.50^1$ \tabularnewline
    \multicolumn{1}{|c|}{} & & \multicolumn{1}{|c|}{$\hat{\rho} = 0.50 + \hat{\delta}$} & $98.50^1$ & $98.50^1$ & $98.50^1$ & $98.50^1$ & $98.50^1$ \tabularnewline\hline
  \end{tabularx}
\end{table}
\renewcommand{\arraystretch}{1.0}

\renewcommand{\arraystretch}{1.2}
\begin{table}
  \caption{Classification scores for third experiment on MNIST.}
  \label{table:RecoderEvMNISTSmallResultsNP}
  \newcolumntype{R}{>{\centering}X}
  \begin{tabularx}{\textwidth}{cR|R|R|R|}
    \cline{3-5}
    & & \multicolumn{3}{c|}{$\mathcal{N}$} \tabularnewline
    \cline{3-5}
    & & Linear & Logistic & GlobalOrder \tabularnewline\hline
    \multicolumn{1}{|c|}{\multirow{3}{*}{$\mathcal{P}$}} & None & $98.50^1$ & $98.50^1$ & $98.50^1$ \tabularnewline
    \multicolumn{1}{|c|}{} & NoSign & $98.50^1$ & $98.50^1$ & $98.50^1$ \tabularnewline
    \multicolumn{1}{|c|}{} & KeepSign & $98.50^1$ & $98.50^1$ & $98.50^1$ \tabularnewline\hline
  \end{tabularx}
\end{table}
\renewcommand{\arraystretch}{1.0}

\renewcommand{\arraystretch}{1.2}
\begin{table}
  \caption{Classification scores for fourth experiment on MNIST.}
  \label{table:RecoderEvMNISTSmallResultsRr}
  \newcolumntype{R}{>{\centering}X}
  \begin{tabularx}{\textwidth}{cR|c|c|c|c|c|c|c|c|}
    \cline{3-10}
    & & \multicolumn{8}{c|}{$r$} \tabularnewline
    \cline{3-10}
    &            & 2 & 3 & 4 & 6 & 8 & 10 & 14 & 20 \tabularnewline\hline
    \multicolumn{1}{|c|}{\multirow{5}{*}{$\mathcal{R}$}} & Subsample   & $98.50^1$ & $98.50^1$ & $98.50^1$ & $98.50^1$ & $98.50^1$ & $98.50^1$ & $98.50^1$ & $98.50^1$ \tabularnewline
    \multicolumn{1}{|c|}{} & MaxNoSign   & $98.50^1$ & $98.50^1$ & $98.50^1$ & $98.50^1$ & $98.50^1$ & $98.50^1$ & $98.50^1$ & $98.50^1$ \tabularnewline
    \multicolumn{1}{|c|}{} & MaxKeepSign & $98.50^1$ & $98.50^1$ & $98.50^1$ & $98.50^1$ & $98.50^1$ & $98.50^1$ & $98.50^1$ & $98.50^1$ \tabularnewline
    \multicolumn{1}{|c|}{} & SumAbs      & $98.50^1$ & $98.50^1$ & $98.50^1$ & $98.50^1$ & $98.50^1$ & $98.50^1$ & $98.50^1$ & $98.50^1$ \tabularnewline
    \multicolumn{1}{|c|}{} & SumSqr      & $98.50^1$ & $98.50^1$ & $98.50^1$ & $98.50^1$ & $98.50^1$ & $98.50^1$ & $98.50^1$ & $98.50^1$ \tabularnewline\hline
  \end{tabularx}
\end{table}
\renewcommand{\arraystretch}{1.0}

\renewcommand{\arraystretch}{1.2}
\begin{table}
  \caption{Classification scores for first experiments on SmallNORB.}
  \label{table:RecoderEvNORBSmallResultsPWCScore}
  \newcolumntype{R}{>{\centering}X}
  \begin{tabularx}{\textwidth}{cRR|c|c|c|c|c|c|c|}
    \cline{4-10}
    & & & \multicolumn{7}{c|}{$p$} \tabularnewline \cline{4-10}
    & & & $5$ & $7$ & $9$ & $11$ & $13$ & $15$ & $17$ \tabularnewline \hline
    \multicolumn{1}{|c}{\multirow{5}{*}{$\textbf{C}$}} & \multicolumn{1}{|R}{\multirow{2}{*}{Random}} & \multicolumn{1}{|R|}{$w=p^2$} & $98.30^{(1)}$ & $98.47^{(1)}$ & $98.62^{(1)}$ & $98.77^{(1)}$ & $98.68^{(1)}$ & $98.76^{(1)}$ & $98.56^{(1)}$ \tabularnewline
    \multicolumn{1}{|c}{} & \multicolumn{1}{|R}{} & \multicolumn{1}{|R|}{$w=3p^2$} & 98.50 & 98.50 & 98.50 & 98.50 & 98.50 & 98.50 & 98.50 \tabularnewline
    \multicolumn{1}{|c}{} & \multicolumn{1}{|R}{\textbf{PCA}} & \multicolumn{1}{|R|}{$w=p^2$} & 98.50 & 98.50 & 98.50 & 98.50 & 98.50 & 98.50 & 98.50 \tabularnewline
    \multicolumn{1}{|c}{} & \multicolumn{1}{|R}{\multirow{2}{*}{Sparse}} & \multicolumn{1}{|R|}{$w=p^2$} & 98.50 & 98.50 & 98.50 & 98.50 & 98.50 & 98.50 & 98.50 \tabularnewline
    \multicolumn{1}{|c}{} & \multicolumn{1}{|R}{} & \multicolumn{1}{|R|}{$w=3p^2$} & 98.50 & 98.50 & 98.50 & 98.50 & 98.50 & 98.50 & 98.50 \tabularnewline\hline
  \end{tabularx}
\end{table}
\renewcommand{\arraystretch}{1.0}

\renewcommand{\arraystretch}{1.2}
\begin{table}
  \caption{Classification scores for second experiment on SmallNORB.}
  \label{table:RecoderEvNORBSmallResultsRhoCCkScore}
  \newcolumntype{R}{>{\centering}X}
  \begin{tabularx}{\textwidth}{ccc|R|R|R|R|R|}
    \cline{4-8}
    & & & \multicolumn{5}{c|}{$\hat{\delta}$} \tabularnewline\cline{4-8}
    & & & $0.00$ & $0.05$ & $0.10$ & $0.15$ & $0.20$ \tabularnewline\hline
    \multicolumn{1}{|c|}{\multirow{12}{*}{$\mathcal{C}_{\textbf{C},k}$}} & \multirow{3}{*}{\textbf{Corr}} & \multicolumn{1}{|c|}{$\hat{\rho}=0.05+\hat{\delta}$} & $98.50^1$ & $98.50^1$ & $98.50^1$ & $98.50^1$ & $98.50^1$ \tabularnewline
    \multicolumn{1}{|c|}{} & & \multicolumn{1}{|c|}{$\hat{\rho} = 0.25 + \hat{\delta}$} & $98.50^1$ & $98.50^1$ & $98.50^1$ & $98.50^1$ & $98.50^1$ \tabularnewline
    \multicolumn{1}{|c|}{} & & \multicolumn{1}{|c|}{$\hat{\rho} = 0.50 + \hat{\delta}$} & $98.50^1$ & $98.50^1$ & $98.50^1$ & $98.50^1$ & $98.50^1$ \tabularnewline
    \multicolumn{1}{|c|}{} & \multirow{3}{*}{\textbf{MP}} & \multicolumn{1}{|c|}{$\hat{\rho}=0.05+\hat{\delta}$} & $98.50^1$ & $98.50^1$ & $98.50^1$ & $98.50^1$ & $98.50^1$ \tabularnewline
    \multicolumn{1}{|c|}{} & & \multicolumn{1}{|c|}{$\hat{\rho} = 0.25 + \hat{\delta}$} & $98.50^1$ & $98.50^1$ & $98.50^1$ & $98.50^1$ & $98.50^1$ \tabularnewline
    \multicolumn{1}{|c|}{} & & \multicolumn{1}{|c|}{$\hat{\rho} = 0.50 + \hat{\delta}$} & $98.50^1$ & $98.50^1$ & $98.50^1$ & $98.50^1$ & $98.50^1$ \tabularnewline
    \multicolumn{1}{|c|}{} & \multirow{3}{*}{\textbf{OMP}} & \multicolumn{1}{|c|}{$\hat{\rho}=0.05+\hat{\delta}$} & $98.50^1$ & $98.50^1$ & $98.50^1$ & $98.50^1$ & $98.50^1$ \tabularnewline
    \multicolumn{1}{|c|}{} & & \multicolumn{1}{|c|}{$\hat{\rho} = 0.25 + \hat{\delta}$} & $98.50^1$ & $98.50^1$ & $98.50^1$ & $98.50^1$ & $98.50^1$ \tabularnewline
    \multicolumn{1}{|c|}{} & & \multicolumn{1}{|c|}{$\hat{\rho} = 0.50 + \hat{\delta}$} & $98.50^1$ & $98.50^1$ & $98.50^1$ & $98.50^1$ & $98.50^1$ \tabularnewline
    \multicolumn{1}{|c|}{} & \multirow{3}{*}{\textbf{OOMP}} & \multicolumn{1}{|c|}{$\hat{\rho}=0.05+\hat{\delta}$} & $98.50^1$ & $98.50^1$ & $98.50^1$ & $98.50^1$ & $98.50^1$ \tabularnewline
    \multicolumn{1}{|c|}{} & & \multicolumn{1}{|c|}{$\hat{\rho} = 0.25 + \hat{\delta}$} & $98.50^1$ & $98.50^1$ & $98.50^1$ & $98.50^1$ & $98.50^1$ \tabularnewline
    \multicolumn{1}{|c|}{} & & \multicolumn{1}{|c|}{$\hat{\rho} = 0.50 + \hat{\delta}$} & $98.50^1$ & $98.50^1$ & $98.50^1$ & $98.50^1$ & $98.50^1$ \tabularnewline\hline
  \end{tabularx}
\end{table}
\renewcommand{\arraystretch}{1.0}

\renewcommand{\arraystretch}{1.2}
\begin{table}
  \caption{Classification scores for third experiment on SmallNORB.}
  \label{table:RecoderEvNORBSmallSmallResultsNP}
  \newcolumntype{R}{>{\centering}X}
  \begin{tabularx}{\textwidth}{cR|R|R|R|}
    \cline{3-5}
    & & \multicolumn{3}{c|}{$\mathcal{N}$} \tabularnewline
    \cline{3-5}
    & & Linear & Logistic & GlobalOrder \tabularnewline\hline
    \multicolumn{1}{|c|}{\multirow{3}{*}{$\mathcal{P}$}} & None & $98.50^1$ & $98.50^1$ & $98.50^1$ \tabularnewline
    \multicolumn{1}{|c|}{} & NoSign & $98.50^1$ & $98.50^1$ & $98.50^1$ \tabularnewline
    \multicolumn{1}{|c|}{} & KeepSign & $98.50^1$ & $98.50^1$ & $98.50^1$ \tabularnewline\hline
  \end{tabularx}
\end{table}
\renewcommand{\arraystretch}{1.0}

\renewcommand{\arraystretch}{1.2}
\begin{table}
  \caption{Classification scores for fourth experiment on SmallNORB.}
  \label{table:RecoderEvNORBSmallSmallResultsRr}
  \newcolumntype{R}{>{\centering}X}
  \begin{tabularx}{\textwidth}{cR|c|c|c|c|c|c|c|c|}
    \cline{3-10}
    & & \multicolumn{8}{c|}{$r$} \tabularnewline
    \cline{3-10}
    &            & 2 & 3 & 4 & 6 & 8 & 10 & 14 & 20 \tabularnewline\hline
    \multicolumn{1}{|c|}{\multirow{5}{*}{$\mathcal{R}$}} & Subsample   & $98.50^1$ & $98.50^1$ & $98.50^1$ & $98.50^1$ & $98.50^1$ & $98.50^1$ & $98.50^1$ & $98.50^1$ \tabularnewline
    \multicolumn{1}{|c|}{} & MaxNoSign   & $98.50^1$ & $98.50^1$ & $98.50^1$ & $98.50^1$ & $98.50^1$ & $98.50^1$ & $98.50^1$ & $98.50^1$ \tabularnewline
    \multicolumn{1}{|c|}{} & MaxKeepSign & $98.50^1$ & $98.50^1$ & $98.50^1$ & $98.50^1$ & $98.50^1$ & $98.50^1$ & $98.50^1$ & $98.50^1$ \tabularnewline
    \multicolumn{1}{|c|}{} & SumAbs      & $98.50^1$ & $98.50^1$ & $98.50^1$ & $98.50^1$ & $98.50^1$ & $98.50^1$ & $98.50^1$ & $98.50^1$ \tabularnewline
    \multicolumn{1}{|c|}{} & SumSqr      & $98.50^1$ & $98.50^1$ & $98.50^1$ & $98.50^1$ & $98.50^1$ & $98.50^1$ & $98.50^1$ & $98.50^1$ \tabularnewline\hline
  \end{tabularx}
\end{table}
\renewcommand{\arraystretch}{1.0}

In order to evaluate the performance of a recoder with a certain configuration $\Theta$ on a dataset, we transform both the training and the testing parts of the dataset with the recoder and then we train a Linear SVM in a One-vs-All fashion using the training part of the dataset. We then classify the testing part and measure the classification accuracy as well as compute the average sparsity. The quality of one configuration relative to another is determined using only the classification accuracy though. For speed reasons, roughly one third of the training portion of the dataset is actually used for training, the so-called \emph{model selection} subsample. This will make the classification accuracy slightly lower, but it will do so in a systematic way, enabling us to still select the proper configuration without fear of having a rank inversion when using on the full training dataset.

Selecting the classifier parameters (just $C$) for the Linear SVM is done through cross-validation. We wish to select one of $20$ possible values for $C$ logarithmically distributed between $10^{-2}$ and $1$. In order to evaluate the performance of the SVM with each choice of regularization parameter, we select half of the model selection subsample, which we call the \emph{crossvalidation} subsample. We then perform five splits of this subsample into five pairs of \emph{training} and \emph{validation} subsamples. For a certain value $C_i$ we train a One-vs-All classifier on each of the five training subsamples and compute the classification accuracy on the validation subsamples. We use the average of these five classification accuracies when choosing a best $C$. Naturally, the $C^\star$ with the greatest average accuracy is selected.

A new classifier is then trained on the whole transformed model selection subsample, using the $C^\star$ found by cross-validation. This is then used to classify the testing sample from the dataset and asses the classification accuracy. This whole process should be repeated several times in order to smooth out fluctuations in the accuracy cause by the choice of model selection and crossvalidation subsamples as well as ``built-in'' randomness, such as the choice of features in a random feature set. We only do this three times, although, as a matter of principle, we should do it a lot more. Computational constraints do not allow us to do this however. The average classification accuracy and sparseness over all the trials is reported in the tables.

We also considered a baseline setup. This consisted of simply feeding the linearized images to a linear SVM. The same cross-validation setup was used. The results can be seen in .... We also did two alternative pre-processing steps, namely standardization and normalization. The results using these can as well be seen. In any case, the results are not that great and we can safely say that using the proposed methods is at least better than an of-the-shelf immediate solution.

\chapter{Conclusions}
\label{chap:Conclusions}

\bibliographystyle{unsrt}
\bibliography{Bibliography}

\appendix

\end{document}
