\documentclass[runningheads,a4paper]{llncs}

\usepackage{amssymb}
\usepackage{amsmath}
\usepackage{algpseudocode}
\usepackage{algorithm}
\usepackage{graphicx}
\usepackage{url}
\usepackage{float}
\usepackage{caption}
\usepackage{subcaption}
\usepackage{tabularx}
\usepackage{array}
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}

\setcounter{tocdepth}{3}
\floatstyle{boxed}
\restylefloat{figure}
\algrenewcommand\algorithmicrequire{\textbf{input}}
\algrenewcommand\algorithmicensure{\textbf{output}}

\newcommand{\hctimes}[2]{{#1}\!\times\!{#2}}
\newcommand{\hcrange}[2]{\overline{{#1}\colon\!\!{#2}}}
\newcommand{\hcsignalspace}{\mathbb{R}^d}
\newcommand{\hcweightspace}{\mathbb{R}^w}
\newcommand{\hcdictspace}{\mathbb{R}^{\hctimes{w}{d}}}

\urldef{\mailall}\path|{coman,barth,martinetz}@inb.uni-luebeck.de|
\newcommand{\keywords}[1]{\par\addvspace\baselineskip
\noindent\keywordname\enspace\ignorespaces#1}

\begin{document}

\mainmatter

\title{Sparse Coding Neural Gas Applied to Image Recognition}
\titlerunning{Sparse Coding Neural Gas Applied to Image Recognition}

\author{Horia Coman$^{1,2}$ \and Erhardt Barth$^{1}$ \and Thomas Martinetz$^{1}$}
\authorrunning{Sparse Coding Neural Gas Applied to Image Recognition}

\institute{$^1$Institute for Neuro-and Bioinformatics, University of L\"{u}beck\\
Ratzeburger Alee 160, 23538 L\"{u}beck, Germany\\
\url{http://www.inb.uni-luebeck.de}\\
\mailall\\
$^2$LAPI, The ``POLITEHNICA'' University of Bucure\c{s}ti\\
Splaiul Independen\c{t}ei nr. 313, 060042 Bucure\c{s}ti, Romania\\
\url{http://imag.pub.ro}}


\toctitle{Sparse Coding Neural Gas Applied to Image Recognition}
\tocauthor{Sparse Coding Neural Gas Applied to Image Recognition}
\maketitle

\begin{abstract} A generalization of the Sparse Coding Neural Gas algorithm for feature learning is proposed and is then discussed in the context of modern classifier techniques for signals. The algorithm is then used as part of a larger image classification system, which is tested on the MNIST handwritten digit dataset and the CIFAR10 object dataset, obtaining similar results to other state-of-the-art methods. The algorithm facilitates learning of very large feature sets, which are helpful for classification.
\keywords{Neural Gas, Sparse Coding, Sparse Coding Neural Gas, Image Recognition, Matching Pursuit}
\end{abstract}

\section{Introduction}

The task of image recognition is a complex one. Simply training a classifier on raw image data will yield poor performance. A common strategy is to select a group of important properties of an image, called \emph{features}. Then, given an image, $I$, these features are used to compute a feature descriptor, $F$. This contains, for each feature, a measure of the inclusion of that feature into the image. In order to obtain an estimated class, only $F$ is considered. The construction of features is thus a big part of most machine learning applications. However, for best performance, specific domain knowledge must be used. This makes both the design and comparision of learning systems harder.

In the last decade, automatic feature construction, also known as unsupervised feature learning \cite{best-architecture-object-recognition,emergence-sparse-coding,sparse-coding-strategy-V1,tiny-images}, has become mainstream, surpassing hand-crafted methods on diverse problems \cite{best-architecture-object-recognition,learning-convolutional-feature-hierarchies,gradient-based-learning,convolutional-networks-vision,best-practices-cnn,simple-method-sparse-coding,sparse-features-audio-classification}. These techinques aim to build features by looking at the statistical properties of a dataset. In addition to this, a full framework for classification has been refined, based on the model of the V1 area of the mammalian brain.

This paper studies the results of using a particular type of feature learning method, called the Sparse Coding Neural Gas \cite{sparse-coding-neural-gas-1,sparse-coding-neural-gas-2,sparse-coding-neural-gas-3,sparse-coding-neural-gas-4}, on two tasks of image classification. The algorithm itself is an adaptation of the Neural Gas vector quantization algorithm \cite{neural-gas-1,neural-gas-2}, which has roots in the self-organizing map subfield of artificial intelligence.

\section{Overview of Feature Extraction}

Given an image $I \in \mathbb{R}^{\hctimes{m}{n}}$, the conceptual recognition pipeline can be summarized as:

\begin{equation}
I \Rightarrow F \rightarrow \omega
\end{equation}

The $\rightarrow$ corresponds to actual classification. A simple classifier, usually a Logistic/SoftMax Regression or a Linear SVM is used here. The onus falls on the feature extraction phase, denoted by $\Rightarrow$, to produce feature descriptors of such a nature, that the simple classifiers can properly discriminate the classes.

What is then, a feature? And, furthermore, once we gather a set of them, how can they be ``extracted'' from the image $I$. For our purposes, a feature is a small filter of some sort. More precisely, the full feature set consists of $w$ normalized square images of size $d = \hctimes{p}{p}$ with $p < \min(m,n)$. This set is denoted by $C = \left[ C^1 \left|\right. C^2 \left|\right. \dots \left|\right. C^w \right] \in \mathbb{R}^{\hctimes{d}{w}}$. A feature set can be obtained from several sources. First, it can be generated randomly \cite{random-weights-feature-learning}. Second, a well-known set can be employed, like DCT bases or Gabor Wavelet bases \cite{simple-method-sparse-coding}. Thirdly, the set can be learned from a sample of patches extracted from the training set of the classification system \cite{emergence-sparse-coding,sparse-coding-strategy-V1,tiny-images}.

Actual extraction consists of three steps, called \emph{coding}, \emph{nonlinear} and \emph{reduce}. The coding step accepts as input the original image $I$ and produces a set of $w$ images, of the same size as $I$. In order to obtain the $(i,j)^{th}$ pixel of the $l^{th}$ image, the patch of size $\hctimes{p}{p}$ from $I$, centered at position $(i,j)$, is ``coded'', with regards to filter $C^l$. The simplest form of coding is an inner product between the two images. Considering the whole image, this corresponds to doing a convolution with the filter $C^l$. In fact, this is the strategy employed by the Convolutional Neural Network (CNN) of \cite{gradient-based-learning}. In general, the response for the $(i,j)^{th}$ pixel need not depend just on $C^l$, but on the whole of $C$. Therefore, in the next section, which is dedicated to coding methods, we'll consider the problem of finding the values of the pixels at position $(i,j)$ in all the $w$ images at once.

The nonlinear step accepts as input the set of $w$ images produced by the coding step and produces another set of $w$ images, of the same size as $I$, but with elements mapped to a restricted interval, such as $[-1,+1]$. Each pixel in each image is transformed independently by passing its value through a sigmoid-like nonlinearity, such as the logistic function. The first and second steps, together, can be viewed as a feed-forward neural network, with $mn$ input units and $wmn$ output units and a very specific weight setup.

Lastly, the reduce step accepts as input the previous set of $w$ images and produces a final set of $w$ images, this time of smaller sizes than the original. More precisely, each image is divided into non-overlapping blocks of size $\hctimes{k}{k}$ and all the values from each block are combined to form one value, according to some function. Common choices are the $\max(\left|\star\right|,\dots,\left|\star\right|)$ and $\sum_{i,j}{(\star)^2}$ functions. The output of this stage is a set of $w$ images of size $\hctimes{m / k}{n / k}$. In general, CNNs can have a more flexible reduce step, but we've found this limited form, which considers each image individually, to be useful as well. The reasons given for the reduce step are that it introduces a certain kind of resistance to small translations. Basically, anywhere in a $\hctimes{k}{k}$ block a feature is detected, the corresponding output of the reduce step should be large.

The final feature vector, $F$, is a linearized version of these images, that is, a $\frac{m}{k}\frac{n}{k}w$-dimensional vector. Also notice, that the whole system can be viewed as one heterogenous neural network consisting of two different stages. In general, several of these modules can be linked, each with its own set of features, tailored to the type of images it receives from the previous layer. In principle, very deep networks can thus be built, depending on the complexity of the dataset being studied. In our experiments, only feature extractors with one layer were used. Also, if a perceptron or a two layer MLP are used as the classifier, the whole system becomes one great neural network. Classical back-propagation can then be used to \emph{fine-tune} the weights, starting from initial values assigned according to $C$. In our experiments, this procedure was not used, but situations such as transfer-learning or self-taught learning \cite{self-taught-learning} can make use of this advantage.

\section{Coding}

This section describes in more detail how to do coding. For simplicity, we will assume we work with $d$-dimensional signals. Thus, the $\hctimes{p}{p}$ patches previously discussed must be linearized such that $d = p^2$. Assume, also, that we are given a set of features, $C$, like in the previous section. Most of the times we will have $w > d$, that is, the set of features is \emph{overcomplete}. Our wish is to approximate a signal $x \in \hcsignalspace$ in terms of $C$. The most common approach is to use a linear combination of the features. The \emph{code} is then the signal $a \in \hcweightspace$ and the \emph{approximation} is:

\begin{equation*}
\hat{x} = \sum_{i=1}^w {a_i C_i} = Ca
\end{equation*}

The quality of the code is determined by how well the reconstruction $\hat{x}$ matches the original signal. For audio and image processing, it has been shown that a sparse code for $x$, that is, one with numerous zero or close to zero components, has many desirable properties \cite{emergence-sparse-coding,sparse-coding-strategy-V1}. Many, methods for sparse coding have been proposed \cite{undetermined-minimal-L1,sparse-coding-strategy-V1}. We will focus on a group of iterative methods for computing $a$, known as pursuits, which originate in the signal processing community. All assume $C$ and $x$ are given and run for a number of $k \leq d$ iterations. The general problem they try to solve is $\arg\min_a \|x - Ca\|_2^2$ subject to $\|a\|_0 \leq k$. This an NP-complete problem and the pursuits are greedy approximations for it. Let the initial residual $R^0x = x$. At iteration $t$, let $C^\omega$ be the most similar code word in $C$, relative to $R^tx$. The updated code and residual, $a^{t+1}$ and $R^{t+1}x$, are produced by decomposing $R^tx$ in terms of $C^\omega$. After $k$ iterations, $a^k$ is returned as the code associated to $x$ and $R^kx$ is returned as a measure of the ability of the algorithm to reconstruct the signal in terms of $C$. The difference between the several methods consist in how they find $C^\omega$ and how they update $a^{t+1}$. The general procedure is illustrated in \textbf{Algorithm \ref{algo:Pursuit}}. At the end of this algorithm we have that $x = Ca^k + \mathcal{R}^kx$. Also, the norm of the final residual $\mathcal{R}^kx$ tends to $0$ as $k \rightarrow +\infty$, for sensible choices of $\text{\textbf{sim}}$ and $\text{\textbf{next}}$ functions, and, in the limit, the equality becomes just $x = Ca^{+\infty}$. 

\begin{algorithm}
\caption{The General Pursuit Method}
\label{algo:Pursuit}
\begin{algorithmic}
\Require $C,x,k$
\Ensure $a^k,\mathcal{R}^kx$
\State $\Lambda^0 \gets \phi$
\State $a^0 \gets 0$
\State $\mathcal{R}^0x \gets x$
\State $t \gets 0$
\While {$t < k \text{~or~} \|\mathcal{R}^tx\|_2 \geq \delta$}
\State $\omega \gets \arg \max_{i \in \textbf{dom}} \text{\textbf{sim}}(R^tx,C,\Lambda^t,i)$
\State $\Lambda^{t+1} \gets \Lambda^t \cup \omega$
\State $a^{t+1} \gets \text{\textbf{next}}(a^t,R^tx,C,\Lambda^{t+1},\omega)$
\State $\mathcal{R}^{t+1}x \gets \mathcal{R}^tx - a^{t+1}_\omega C^\omega$
\State $t \gets t + 1$
\EndWhile
\end{algorithmic}
\end{algorithm}

The simplest pursuit method, introduced in \cite{matchingpursuit1}, is Matching Pursuit (\textbf{MP}). \textbf{Table \ref{table:PursuitParametrization}} shows what form the $\text{\textbf{sim}}$ and $\text{\textbf{next}}$ functions take in this case. An important property of this algorithm is that, for every $t$, $\|\mathcal{R}^tx\|_2^2 \geq \|\mathcal{R}^{t+1}x\|_2^2$ and, furthermore, this decay is exponential. The two major flaws of this method are that the approximation at time $t$, $Ca^t$ is not optimal with respect to the selection of code words $\Lambda^t$ and that for the residual norm to actually reach small enough values, a $k > w$ could be necessary. The flaws are not critical for classification purposes, and, because of its simplicity and speed, we use it in our experiments.

An improvement to \textbf{MP} is Orthogonal Matching Pursuit (\textbf{OMP}) \cite{matchingpursuit2,orthopursuit,pursuitdifferences}, which addresses the two issues discusses above. Again, \textbf{Table \ref{table:PursuitParametrization}} shows the forms the $\text{\textbf{sim}}$ and $\text{\textbf{next}}$ functions take. Also, notice that at each iteration, only the code words not considered before are processed. All the properties of \textbf{MP} hold here as well. At iteration $t$, the approximation computed is the closest point in $\overline{\text{span}(C_{\Lambda^t})}$ to $x$, according to the Euclidean norm. As a consequence, if we allow $k = d$ then the reconstruction error will be minimal, although maybe not the sparsest possible (as the problem is NP-complete). Also, a useful result is that the residual at time $t$ is orthogonal to the approximation at that time, that is $\langle \mathcal{R}^tx , Ca^t \rangle = 0$. The version presented here is suboptimal from an algorithmic point of view. More sophisticated methods, based on QR decomposition have been developed \cite{matchingpursuit2,pursuitdifferences}.

\renewcommand{\arraystretch}{1.5}
\begin{table}
  \caption{The different parametrization for pursuit methods}
  \label{table:PursuitParametrization}
  \begin{tabularx}{\textwidth}{|l|>{\centering}X|>{\centering}X|c|}
       \hline
        Method & $\text{\textbf{sim}}$ function & $\text{\textbf{next}}$ function & $\text{\textbf{dom}}$ domain\\ \hline \hline
        \textbf{MP} & $\left| \langle R^tx , Ci \rangle \right|$ & $a^t + \langle R^x , C^\omega \rangle \delta_\omega$ & $\hcrange{1}{w}$ \\  \hline
        \textbf{OMP} & $\left| \langle R^tx , Ci \rangle \right|$ & $\arg\min_{a} {\| x - C_{\Lambda^{t+1}}a \|_2^2}$ & $\hcrange{1}{w} \setminus \Lambda^t$ \\
       \hline
    \end{tabularx}
\end{table}
\renewcommand{\arraystretch}{1.0}

\section{Learning a Feature Set}

We now turn to the problem of learning the feature set, $C$, given a coding method $\hat{\mathcal{C}}_C$, and a sample $X = \left[ X^1 \left|\right. X^2 \left|\right. \dots \left|\right. X^N \right] \in \mathbb{R}^{\hctimes{d}{N}}$ of linearized image patches of size $\hctimes{p}{p}$, usually extracted from either the whole training set or from a larger, ``natural scenes'' dataset \cite{self-taught-learning}. Also, let $\mathcal{L}$ be a coding loss function. The problem is then one of finding an optimal code $C^{\star}$ which minimizes the average loss, considering the input distribution. We have:

\begin{eqnarray}
\nonumber
C^* & = & \arg\min_{C} \mathbb{E}\mathcal{L}(x,\mathcal{\hat{C}}_C\{x\}) \\
\label{eq:MinEquation}
    & = & \arg\min_{C} \frac{1}{N} \sum_{i=1}^N \mathcal{L}(X^i,\mathcal{\hat{C}}_C\{X^i\})
\end{eqnarray}

There are several methods for solving this problem. The simplest is gradient descent or stochastic gradient descent in feature set space as in \cite{emergence-sparse-coding,sparse-coding-strategy-V1}. The Method of Optimal Directions \cite{mod-frame-design} and K-SVD \cite{ksvd-sparse-representation} are alternatives to this simple approach. The method we employed here is the Sparse Coding Neural Gas approach, which is an adaptation of the Neural Gas algorithm, introduced in the context of vector quantization. This task can be considered as a stricter version of feature learning, where the codes are $1$-sparse and only a boolean ``indictator'' of the code word most similar to the input, $x$, as measured by the Euclidean distance, is stored.

The Neural Gas algorithm is an iterative one. It begins by initializing $C$ to $w$ random observations from the training sample $X$. Then, for a number of $T_{max}$ iterations, an adaptation process takes place, which slowly moves $C$ in order to best represent the distribution over the input space. More precisely, at each iteration, $t$, an observation is randomly selected from $X$ and distances to each element of $C$ are computed. Each word is then modified in a manner proportional to the distorsion between it and the signal $x$, on the one hand, and the ranking of this distorsion in the list of all distorsions, on the other hand. Therefore, the update process includes a local and a global component. \textbf{Algorithm \ref{algo:NeuralGas}} gives the whole picture. Note that both a time decreasing learning factor is used as well as a time decreasing neighbourhood control. 

\begin{algorithm}
\caption{Neural Gas}
\label{algo:NeuralGas}
\begin{algorithmic}
\Require $X,w,T_{max},\mu^0,\mu^{T_{max}},\lambda^0,\lambda^{T_{max}}$
\Ensure $C$
\State $C \gets \mbox{randomly select $w$ observations from $X$}$
\State $t \gets 0$
\For {$t = \hcrange{1}{T_{max}}$}
\State $\mu^t \gets \mu^0 (\mu^{T_max} / \mu^0)^{t / T_{max}}$ \Comment {Current learning rate}
\State $\lambda^t \gets \lambda^0 (\lambda^{T_{max}} / \lambda^0)^{t / T_{max}}$ \Comment {Current neighbourhood control}
\State $x \gets \text{an observation from $X$}$
\State $a \gets [ ~ \|x - C_i\|_2^2 ~ \mbox{for $i \in \hcrange{1}{w}$} ~ ]$
\State $C \gets C + [ ~ \mu^t e^{-rank_a(a_i) / \lambda^t} (x - C_i) ~ \mbox{for $i \in \hcrange{1}{w}$} ~ ]$
\EndFor
\end{algorithmic}
\end{algorithm}

The Neural Gas algorithm works in input space rather than code word space. It is also dependent on a particularly strict coding method. Adapting the algorithm to work with features and accept any coding method gives rise to a first version of the Sparse Coding Neural Gas. The major modification is the fact that each update is done according to Oja's Rule \cite{oja-rule} instead of the simple error term of the Neural Gas. The full algorithm is described in \textbf{Algorithm \ref{algo:SparseCodingNeuralGasV1}}. Notice that the $rank_a$ function considers absolute values, so that features are updated proportional to the magnitude of the associated response.

\begin{algorithm}
\caption{Sparse Coding Neural Gas V1}
\label{algo:SparseCodingNeuralGasV1}
\begin{algorithmic}
\Require $X,w,\mathcal{C},T_{max},\lambda^0,\lambda^{T_{max}},\mu^0,\mu^{T_{max}}$
\Ensure $C$
\State $C \gets \mbox{randomly initialize $w$ normalized code words}$
\State $t \gets 0$
\For {$t = \hcrange{1}{T_{max}}$}
\State $\mu^t \gets \mu^0 (\mu^{T_max} / \mu^0)^{t / T_{max}}$  \Comment {Current learning rate}
\State $\lambda^t \gets \lambda^0 (\lambda^{T_{max}} / \lambda^0)^{t / T_{max}}$ \Comment {Current neighbourhood control}
\State $x \gets \text{an observation from $X$}$
\State $a \gets \mathcal{C}_C\{ x \}$
\State $C \gets C + [ ~ \mu^t e^{-rank_a(a_i) / \lambda^t} a_i (x - a_i C_i) ~ \mbox{for $i \in \hcrange{1}{w}$} ~ ]$
\State $C \gets \mbox{normalize each word in C}$
\EndFor
\end{algorithmic}
\end{algorithm}

A further improvement is possible, considering the fact that many coding methods are iterative and produce orderings of a subset $\hcrange{1}{w} \setminus \Lambda^t$ of the feature elements at each iteration. A second version of the Sparse Coding Neural Gas is presented as \textbf{Algorithm \ref{algo:SparseCodingNeuralGasV2}}. Notice that at each iteration only the subset of previously unselected features is updated, instead of the whole set. Also, the variable $S^i$, which is a substitute for all the abstracted coding method specific information, must contain a copy of the original feature set $C$, at iteration $t$, before the inner-loop coding procedure. The reason for this is that $C$ is updated in the inner-loop and it can cause problems for the coder to change the features as time progresses.

\begin{algorithm}
\caption{Sparse Coding Neural Gas V2}
\label{algo:SparseCodingNeuralGasV2}
\begin{algorithmic}
\Require $X,w,\mathcal{C},T_{max},\lambda^0,\lambda^{T_{max}},\mu^0,\mu^{T_{max}}$
\Ensure $C$
\State $C \gets \mbox{randomly initialize $w$ normalized code words}$
\State $t \gets 0$
\For {$t = \hcrange{1}{T_{max}}$}
\State $\mu^t \gets \mu^0 (\mu^{T_max} / \mu^0)^{t / T_{max}}$  \Comment {Current learning rate}
\State $\lambda^t \gets \lambda^0 (\lambda^{T_{max}} / \lambda^0)^{t / T_{max}}$ \Comment {Current neighbourhood control}
\State $x \gets \text{an observation from $X$}$
\State $S^0 \gets \text{initialize coding method specific state}$
\For {$t = \hcrange{0}{k}$}
\State $[\alpha^t~\Lambda^t~S^{t+1}] \gets \mathcal{C}_C \{S^t,x\}$ \Comment {$\alpha^t$ stores similarities for features in $\hcrange{1}{w} \setminus \Lambda^t$}
\State $C \gets C + [ ~ \mu^t e^{-rank_{\alpha^t}(\alpha_i^t) / \lambda^t} \alpha_i^t (x - \alpha_i^t C_i) ~ \mbox{for $i \in \hcrange{1}{w}\setminus\Lambda^t$} ~ ]$
\State $C \gets \mbox{normalize each word in C}$
\EndFor
\EndFor
\end{algorithmic}
\end{algorithm}

\section{Experiments}

In order to test the classifier system, two datasets were employed: the well known MNIST handwritten digit dataset \cite{gradient-based-learning} and the CIFAR10 object dataset \cite{tiny-images}. Both of these are well known and widely used for benchmarking classifiers. MNIST has $10$ classes corresponding to the $10$ arabic numerals. It consists of $60000$ training images and $10000$ test images. CIFAR10 has $10$ classes as well, corresponding to different categories of objects (airplane,dog,etc.). It consists of $50000$ training images and $10000$ test images. Examples from each class of each dataset can be seen in \textbf{Figure \ref{fig:Classes}}.

Classification scores for the methods we used as well as other details are presented in \textbf{Table \ref{table:ResultsMNIST}} and \textbf{Table \ref{table:ResultsCIFAR10}}. Using random patches as features and using features learned by gradient descent in feature set space are also presented. Several other state of the art methods are included for comparison purposes. For both datasets, only methods which dealt with the unmodified dataset are considered. Extending the dataset through elastic distorsions \cite{best-practices-cnn} has been shown to improve performance. However, such methods are not always applicable.

\renewcommand{\arraystretch}{1.5}
\begin{table}
  \caption{Classification results for MNIST}
  \label{table:ResultsMNIST}
  \begin{tabular*}{\textwidth}{|c|c|c|}
    \hline
    Method & Score & Notes \\ \hline\hline
    CNN (unsupervised pretraining) & 99.47 & See \cite{best-architecture-object-recognition} for details \\ \hline
    CNN (unsupervised pretraining) & 99.40 & See \cite{efficient-learning-sparse-energy-model} for details \\ \hline
    CNN (feature learning) & 99.38 & See \cite{} for details \\ \hline
    Our Method & 99.29 & MP-$11$ with $p=11$ and $w=1024$ \\ \hline
    CNN (random features) & 99.11 & See \cite{} for details \\ \hline
  \end{tabular*}
\end{table}
\renewcommand{\arraystretch}{1.0}

\renewcommand{\arraystretch}{1.5}
\begin{table}
  \caption{Classification results for CIFAR10}
  \label{table:ResultsCIFAR10}
  \begin{tabularx}{\textwidth}{|X|X|X|}
    \hline
    Method & Score & Notes \\
    \hline
  \end{tabularx}
\end{table}
\renewcommand{\arraystretch}{1.0}

After features were extracted from a dataset, a classifier was trained on the features training dataset. We employed a simple Linear SVM, as implemented by LIBLINEAR \cite{liblinear}. The regularization parameter $C$ was fine-tuned through cross-validation. A subset of $20\%$ of the training set instances was put aside for this purpose. We tested $C$ with $10$ possible values, logarithmically distributed between $10^{-3}$ and $10^{-1}$. For each value, $5$ random $50-50$ splits of the dataset were performed. The classifier was trained on one half of the data and was tested on the other half. Average scores were then computed for selection purposes. For each value of $C$, the same split was used. After a good $C$ was found, a classifier was built on the whole training dataset and evaluated on the testing dataset.

\section{Conclusion}

This paper has shown a way to build a complex image classifier, starting from biological principles. Such a classifier has several components (feature learning, coding system, proper classifier), and this article investigated wether the Sparse Coding Neural Gas algorithm is applicable as a feature learning method for it. It indeed is, and can learn useful dictionaries even with inferior coding methods. The resulting classifier was tested on the MNIST and CIFAR10 datasets and found to perform at expected levels.

\subsubsection*{Acknowledgments.} Horia Coman would like to thank, for supporting this work, The German Academic Exchange Service Programme ``Ostpartnerschaften'', the University of L\"{u}beck  and The Sectoral Operational Programme Human Resources Development 2007-2013 of the Romanian Ministry of Labour, Family and Social Protection through the Financial Agreement POSDRU/86/1.2/S/61756.

\bibliographystyle{ieeetr}
\bibliography{Bibliography}

\end{document}
